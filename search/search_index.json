{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Notes and code / command lines snippets This repo contains technical notes.","title":"Home"},{"location":"#notes-and-code-command-lines-snippets","text":"This repo contains technical notes.","title":"Notes and code / command lines snippets"},{"location":"Desktop/Manage_Dotfiles_with_Git/","text":"Git comes to rescue In this example, we will use $HOME/Dotfiles as the Git repository, but feel free to change it to your needs. First of all, we will initialize this repository git init --bare $HOME/Dotfiles Then, as all the git commands that we will use will refer to this repository, it is advised to create an alias, such as: alias dotfiles='/usr/bin/git --git-dir=$HOME/Dotfiles --work-tree=$HOME' You can add this line to your $SHELL configuration file ( $HOME/.bashrc if you use Bash or $HOME/.zshrc if you use zsh ). Next, we will configure Git so it will not show all the untracked files. This is required as we use the entire $HOME as work tree. dotfiles config --local status.showUntrackedFiles no At that point, you should be able to check the state of this repository: dotfiles status Then you can add your configuration files and commit as you wish. For example, let\u2019s add our .bashrc : dotfiles add .bashrc dotfiles commit -m \"Added .bashrc\" Now just add a remote repository (your self-hosted Git or a public one) and push your changes to it: dotfiles remote add origin git@gitlab.domain.tld:sogal/dotfiles.git dotfiles push Setup a new machine Now that you have it all set, let\u2019s configure a new system with the dotfiles you have in your repository. First, clone locally your online repository: git clone --bare git@gitlab.domain.tld:sogal/dotfiles.git $HOME/Dotfiles Again, you have to defined the same alias as before: alias dotfiles='/usr/bin/git --git-dir=$HOME/Dotfiles --work-tree=$HOME' Remember to put it in your $SHELL configuration file. Now, just apply the changes from the repository you have just cloned to your system: dotfiles checkout If some of the files already exist, you will get an error. This will probably happen with files created by default during the openSUSE installation and user account creation, such as the $HOME/.bashrc file, no worries, just rename or delete them. Now, each time you change your configuration files tracked by Git, remember to commit and push your changes.","title":"Manage Dotfiles with Git"},{"location":"Desktop/Manage_Dotfiles_with_Git/#git-comes-to-rescue","text":"In this example, we will use $HOME/Dotfiles as the Git repository, but feel free to change it to your needs. First of all, we will initialize this repository git init --bare $HOME/Dotfiles Then, as all the git commands that we will use will refer to this repository, it is advised to create an alias, such as: alias dotfiles='/usr/bin/git --git-dir=$HOME/Dotfiles --work-tree=$HOME' You can add this line to your $SHELL configuration file ( $HOME/.bashrc if you use Bash or $HOME/.zshrc if you use zsh ). Next, we will configure Git so it will not show all the untracked files. This is required as we use the entire $HOME as work tree. dotfiles config --local status.showUntrackedFiles no At that point, you should be able to check the state of this repository: dotfiles status Then you can add your configuration files and commit as you wish. For example, let\u2019s add our .bashrc : dotfiles add .bashrc dotfiles commit -m \"Added .bashrc\" Now just add a remote repository (your self-hosted Git or a public one) and push your changes to it: dotfiles remote add origin git@gitlab.domain.tld:sogal/dotfiles.git dotfiles push","title":"Git comes to rescue"},{"location":"Desktop/Manage_Dotfiles_with_Git/#setup-a-new-machine","text":"Now that you have it all set, let\u2019s configure a new system with the dotfiles you have in your repository. First, clone locally your online repository: git clone --bare git@gitlab.domain.tld:sogal/dotfiles.git $HOME/Dotfiles Again, you have to defined the same alias as before: alias dotfiles='/usr/bin/git --git-dir=$HOME/Dotfiles --work-tree=$HOME' Remember to put it in your $SHELL configuration file. Now, just apply the changes from the repository you have just cloned to your system: dotfiles checkout If some of the files already exist, you will get an error. This will probably happen with files created by default during the openSUSE installation and user account creation, such as the $HOME/.bashrc file, no worries, just rename or delete them. Now, each time you change your configuration files tracked by Git, remember to commit and push your changes.","title":"Setup a new machine"},{"location":"Desktop/Pulseaudio%26Bluetooth/","text":"In case of trouble with bluetooth devices and PA sound output Edit the file: /etc/pulse/default.pa and comment out (with an # at the beginning of the line) the following line: load-module module-bluetooth-discover now edit the file: /usr/bin/start-pulseaudio-x11 and after the lines: if [ x\u201d$SESSION_MANAGER\u201d != x ] ; then /usr/bin/pactl load-module module-x11-xsmp \u201cdisplay=$DISPLAY session_manager=$SESSION_MANAGER\u201d > /dev/null fi add the following line: /usr/bin/pactl load-module module-bluetooth-discover Dans le fichier : /etc/bluetooth/main.conf Changer ControllerMode = dual en : ControllerMode = bredr Si probl\u00e8me avec gDM: /var/lib/gdm/.pulse/client.conf autospawn = no daemon-binary = /bin/true","title":"Pulseaudio&Bluetooth"},{"location":"Desktop/Sync_Github_fork/","text":"Syncing a Github fork Before you can sync your fork with an upstream repository, you must configure a remote that points to the upstream repository in Git. Open Terminal. Change the current working directory to your local project. Fetch the branches and their respective commits from the upstream repository. Commits to master will be stored in a local branch, upstream/master. git fetch upstream > remote : Counting objects : 75 , done . > remote : Compressing objects : 100 % ( 53 / 53 ), done . > remote : Total 62 ( delta 27 ), reused 44 ( delta 9 ) > Unpacking objects : 100 % ( 62 / 62 ), done . > From https : //github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY > * [ new branch ] master -> upstream / master Check out your fork\u2019s local master branch. git checkout master > Switched to branch 'master' Merge the changes from upstream/master into your local master branch. This brings your fork\u2019s master branch into sync with the upstream repository, without losing your local changes. git merge upstream / master > Updating a422352 .. 5 fdff0f > Fast - forward > README | 9 ------- > README . md | 7 ++++++ > 2 files changed , 7 insertions ( + ), 9 deletions ( - ) > delete mode 100644 README > create mode 100644 README . md If your local bran ch didn\u2019t have any unique commits, Git will instead perform a \u201cfast-forward\u201d: git merge upstream / master > Updating 34 e91da .. 16 c56ad > Fast - forward > README . md | 5 +++ -- > 1 file changed , 3 insertions ( + ), 2 deletions ( - ) Tip Syncing your fork only updates your local copy of the repository. To update your fork on GitHub, you must push your changes.","title":"Syncing a Github fork"},{"location":"Desktop/Sync_Github_fork/#syncing-a-github-fork","text":"Before you can sync your fork with an upstream repository, you must configure a remote that points to the upstream repository in Git. Open Terminal. Change the current working directory to your local project. Fetch the branches and their respective commits from the upstream repository. Commits to master will be stored in a local branch, upstream/master. git fetch upstream > remote : Counting objects : 75 , done . > remote : Compressing objects : 100 % ( 53 / 53 ), done . > remote : Total 62 ( delta 27 ), reused 44 ( delta 9 ) > Unpacking objects : 100 % ( 62 / 62 ), done . > From https : //github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY > * [ new branch ] master -> upstream / master Check out your fork\u2019s local master branch. git checkout master > Switched to branch 'master' Merge the changes from upstream/master into your local master branch. This brings your fork\u2019s master branch into sync with the upstream repository, without losing your local changes. git merge upstream / master > Updating a422352 .. 5 fdff0f > Fast - forward > README | 9 ------- > README . md | 7 ++++++ > 2 files changed , 7 insertions ( + ), 9 deletions ( - ) > delete mode 100644 README > create mode 100644 README . md If your local bran ch didn\u2019t have any unique commits, Git will instead perform a \u201cfast-forward\u201d: git merge upstream / master > Updating 34 e91da .. 16 c56ad > Fast - forward > README . md | 5 +++ -- > 1 file changed , 3 insertions ( + ), 2 deletions ( - ) Tip Syncing your fork only updates your local copy of the repository. To update your fork on GitHub, you must push your changes.","title":"Syncing a Github fork"},{"location":"Divers/mkdocs/","text":"MkDocs Installer MkDocs avec les plugins pip3 install --upgrade \\ mkdocs \\ mkdocs-material \\ pymdown-extensions Afficher le site mkdocs serve","title":"MkDocs"},{"location":"Divers/mkdocs/#mkdocs","text":"","title":"MkDocs"},{"location":"Divers/mkdocs/#installer-mkdocs-avec-les-plugins","text":"pip3 install --upgrade \\ mkdocs \\ mkdocs-material \\ pymdown-extensions","title":"Installer MkDocs avec les plugins"},{"location":"Divers/mkdocs/#afficher-le-site","text":"mkdocs serve","title":"Afficher le site"},{"location":"Docker/Container_backup/","text":"Docker container backup This config will describe a procedure of how to back up a Docker container as well as it will also show how to recover a Docker container from backup. To understand the Docker container backup and recovery process we first need to understand the difference between docker image and docker container. A docker image contains an operating system with possibly one or more prefigured applications. Whereas, a docker container is a running instance created from an image. When we need make a backup of a docker container we commit its current state and save it as a docker image. docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 78727078 a04b debian : 8 \"/bin/bash\" 13 seconds ago Up 11 seconds container1 From the above output we see a running docker container named container1 with an ID 78727078a04b. We now use commit command to take a snapshot of its current running state: docker commit - p 78727078 a04b container1 e09f9ac65c8b3095927c14ca0594868f73831bde0800ce66415afeb91aea93cf With do above command we have first paused a running container with -p option, made a commit to save the entire snapshot as a docker image with a name container1: docker images REPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE container1 latest e09f9ac65c8b 39 seconds ago 125.1 MB Now we have a container backup saved as an image waiting to be redeployed again. If we wish to redeploy our container1 image on another docker host system we may push the image to some private docker repository: docker login docker push container1 or we can save it as a tar file and move it freely to any desired docker host system for a deployment: docker save - o ~/ container1 . tar container1 ls - l ~/ container1 . tar - rw - r --r--. 1 root root 131017216 Jun 14 20:31 /root/container1.tar Docker container recovery The above paragraphs explained how to backup a docker container. In this section we will discuss how recover from a docker backup. In case that we have pushed our backed up docker container image to a private repository we can simply use docker run command to start a new instance from the container1 image. If we have transferred our container1.tar backup file to another docker host system we first need to load backed up tar file into a docker\u2019s local image repository: docker load -i /root/container1.tar Confirm that the image was loaded with: docker images Now we can use docker run command to start a new instance from the above loaded container1 image.","title":"Container backup"},{"location":"Docker/Container_backup/#docker-container-backup","text":"This config will describe a procedure of how to back up a Docker container as well as it will also show how to recover a Docker container from backup. To understand the Docker container backup and recovery process we first need to understand the difference between docker image and docker container. A docker image contains an operating system with possibly one or more prefigured applications. Whereas, a docker container is a running instance created from an image. When we need make a backup of a docker container we commit its current state and save it as a docker image. docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 78727078 a04b debian : 8 \"/bin/bash\" 13 seconds ago Up 11 seconds container1 From the above output we see a running docker container named container1 with an ID 78727078a04b. We now use commit command to take a snapshot of its current running state: docker commit - p 78727078 a04b container1 e09f9ac65c8b3095927c14ca0594868f73831bde0800ce66415afeb91aea93cf With do above command we have first paused a running container with -p option, made a commit to save the entire snapshot as a docker image with a name container1: docker images REPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE container1 latest e09f9ac65c8b 39 seconds ago 125.1 MB Now we have a container backup saved as an image waiting to be redeployed again. If we wish to redeploy our container1 image on another docker host system we may push the image to some private docker repository: docker login docker push container1 or we can save it as a tar file and move it freely to any desired docker host system for a deployment: docker save - o ~/ container1 . tar container1 ls - l ~/ container1 . tar - rw - r --r--. 1 root root 131017216 Jun 14 20:31 /root/container1.tar","title":"Docker container backup"},{"location":"Docker/Container_backup/#docker-container-recovery","text":"The above paragraphs explained how to backup a docker container. In this section we will discuss how recover from a docker backup. In case that we have pushed our backed up docker container image to a private repository we can simply use docker run command to start a new instance from the container1 image. If we have transferred our container1.tar backup file to another docker host system we first need to load backed up tar file into a docker\u2019s local image repository: docker load -i /root/container1.tar Confirm that the image was loaded with: docker images Now we can use docker run command to start a new instance from the above loaded container1 image.","title":"Docker container recovery"},{"location":"Docker/Databases_backup/","text":"Postgres DB docker exec -t your-db-container pg_dumpall -c -U postgres | gzip > dump_`date +%d-%m-%Y\"_\"%H_%M_%S`.sql Restore your databases cat your_dump.sql | docker exec -i your-db-container psql -U postgres With mySQL cat dump.sql | docker-compose exec -T <mysql_container> mysql -u <db-username> -p<db-password> <db-name>","title":"Databases backup"},{"location":"Docker/Databases_backup/#postgres-db","text":"docker exec -t your-db-container pg_dumpall -c -U postgres | gzip > dump_`date +%d-%m-%Y\"_\"%H_%M_%S`.sql","title":"Postgres DB"},{"location":"Docker/Databases_backup/#restore-your-databases","text":"cat your_dump.sql | docker exec -i your-db-container psql -U postgres","title":"Restore your databases"},{"location":"Docker/Databases_backup/#with-mysql","text":"cat dump.sql | docker-compose exec -T <mysql_container> mysql -u <db-username> -p<db-password> <db-name>","title":"With mySQL"},{"location":"Docker/General_use/","text":"Remove image docker rmi $image_id Get img size docker image inspect img/name:label --format=' {{ .Size }} ' Build image docker build --label myLabel --tag myTag:latest path/to/Dockerfile Tag for remote repo docker tag myTag:version git.server.tld:5000/myTag:version docker push git.server.tld:5000/myTag:version","title":"General use"},{"location":"Docker/General_use/#remove-image","text":"docker rmi $image_id","title":"Remove image"},{"location":"Docker/General_use/#get-img-size","text":"docker image inspect img/name:label --format=' {{ .Size }} '","title":"Get img size"},{"location":"Docker/General_use/#build-image","text":"docker build --label myLabel --tag myTag:latest path/to/Dockerfile","title":"Build image"},{"location":"Docker/General_use/#tag-for-remote-repo","text":"docker tag myTag:version git.server.tld:5000/myTag:version docker push git.server.tld:5000/myTag:version","title":"Tag for remote repo"},{"location":"Docker/Tips/","text":"General Docker Tips and issues solutions this fixes the input device is not a TTY .. see https://github.com/docker/compose/issues/5696 export COMPOSE_INTERACTIVE_NO_CLI=1 Address a container in swarm mode SERVICE_NAME= ${ DEPLOY_STACK } # Get deployed service information as json output SERVICE_JSON=$(docker service ps $SERVICE_NAME --no-trunc --format '{{ json . }}' -f desired-state=running) # Parse the output, get swarm node on which service has been deployed SWARM_NODE=$(echo \" $SERVICE_JSON \" | jq -r '.Node') # Wait just to give time for service to be deployed sleep 10 # Get container ID and run command via SSH on the swarm node ssh -t $SWARM_NODE \"docker exec -it $(echo $SERVICE_JSON | jq -r '.Name').$(echo $SERVICE_JSON | jq -r '.ID') make permissions\" Have .env vars exported when deploying a stack docker-compose config | docker stack deploy --compose-file - <STACK_NAME> If there is a .env entry in the compose file, it will be read and vars replaced in the compose file.","title":"General Docker Tips and issues solutions"},{"location":"Docker/Tips/#general-docker-tips-and-issues-solutions","text":"","title":"General Docker Tips and issues solutions"},{"location":"Docker/Tips/#this-fixes-the-input-device-is-not-a-tty-see-httpsgithubcomdockercomposeissues5696","text":"export COMPOSE_INTERACTIVE_NO_CLI=1","title":"this fixes the input device is not a TTY .. see https://github.com/docker/compose/issues/5696"},{"location":"Docker/Tips/#address-a-container-in-swarm-mode","text":"SERVICE_NAME= ${ DEPLOY_STACK } # Get deployed service information as json output SERVICE_JSON=$(docker service ps $SERVICE_NAME --no-trunc --format '{{ json . }}' -f desired-state=running) # Parse the output, get swarm node on which service has been deployed SWARM_NODE=$(echo \" $SERVICE_JSON \" | jq -r '.Node') # Wait just to give time for service to be deployed sleep 10 # Get container ID and run command via SSH on the swarm node ssh -t $SWARM_NODE \"docker exec -it $(echo $SERVICE_JSON | jq -r '.Name').$(echo $SERVICE_JSON | jq -r '.ID') make permissions\"","title":"Address a container in swarm mode"},{"location":"Docker/Tips/#have-env-vars-exported-when-deploying-a-stack","text":"docker-compose config | docker stack deploy --compose-file - <STACK_NAME> If there is a .env entry in the compose file, it will be read and vars replaced in the compose file.","title":"Have .env vars exported when deploying a stack"},{"location":"Filesystems/Filesystems_tricks/","text":"Step 1: Find inode number of any file using following command on terminal. ls -i /var/log/messages 13377 /var/log/messages Step 2: Find File Creation Time (crtime) debugfs -R 'stat <inode_number>' /dev/sda1 Nettoyage un NTFS inconsistent : ntfsfix /dev/sdX1","title":"Step 1: Find inode number of any file using following command on terminal."},{"location":"Filesystems/Filesystems_tricks/#step-1-find-inode-number-of-any-file-using-following-command-on-terminal","text":"ls -i /var/log/messages 13377 /var/log/messages","title":"Step 1: Find inode number of any file using following command on terminal."},{"location":"Filesystems/Filesystems_tricks/#step-2-find-file-creation-time-crtime","text":"debugfs -R 'stat <inode_number>' /dev/sda1","title":"Step 2: Find File Creation Time (crtime)"},{"location":"Filesystems/Filesystems_tricks/#nettoyage-un-ntfs-inconsistent","text":"ntfsfix /dev/sdX1","title":"Nettoyage un NTFS inconsistent :"},{"location":"Filesystems/GlusterFS/","text":"source GlusterFS GlusterFS est un syst\u00e8me de fichiers r\u00e9seau client/serveur permettant d\u2019agr\u00e9ger diff\u00e9rents n\u0153uds de stockage afin de fournir un environnement NAS hautement disponible. Pour quoi faire ? Admettons que j\u2019ai une application Web lambda, je vais pouvoir d\u00e9ployer plusieurs instances Apache ou Nginx qui se trouveront derri\u00e8re un \u00e9quilibreur de charge, lui-m\u00eame hautement disponible. Sur chaque instance de serveur Web, il me sera facile de d\u00e9ployer l\u2019application. Toutefois chaque instance aura besoin d\u2019acc\u00e9der \u00e0 des fichiers communs, g\u00e9n\u00e9r\u00e9s ou non par l\u2019application. Bien souvent, je vais rencontrer dans ce cas un serveur NFS qui va donc lui-m\u00eame constituer un point de faiblesse dans l\u2019architecture. Gluster permet de mettre en cluster plusieurs n\u0153uds de stockage (\u00e0 minima deux), ce qui permet de r\u00e9pondre \u00e0 deux probl\u00e9matiques majeures d\u00e8s qu\u2019une application a besoin de pouvoir monter en charge : la parall\u00e9lisation et la r\u00e9plication du stockage. Pour fournir ces fonctionnalit\u00e9s sur un volume, une \u00ab brick \u00bb en langage Gluster, le syst\u00e8me s\u2019appuie sur des syst\u00e8mes de fichiers traditionnels, XFS ou EXT4 au-dessus d\u2019un p\u00e9riph\u00e9rique en mode bloc (partition, LVM, RAID, etc..). Gluster travaille donc principalement au niveau fichier. Contrairement \u00e0 un certain nombre d\u2019autres syst\u00e8mes de fichiers de ce type, Gluster offre l\u2019immense avantage de ne pas n\u00e9cessiter de serveur de m\u00e9ta donn\u00e9es pour fonctionner. De fait, cette absence ne constitue pas un point de faiblesse ou un \u00e9l\u00e9ment suppl\u00e9mentaire \u00e0 maintenir dans l\u2019infrastructure de stockage. De plus, chaque fois que l\u2019on ajoute un n\u0153ud au cluster, le syst\u00e8me devient plus performant et l\u2019augmentation de la performance est lin\u00e9aire avec l\u2019extension de l\u2019infrastructure. Dernier point pour mettre en \u00e9vidence cette simplicit\u00e9 de conception, il n\u2019existe pas de notion de ma\u00eetre ou d\u2019esclave avec GlusterFS. Les volumes GlusterFS Un volume est une agr\u00e9gation de plusieurs bricks r\u00e9partis sur diff\u00e9rents n\u0153uds de stockage. Le choix du type de volume se fait en fonction des attentes de performances, de s\u00e9curit\u00e9 ou de la combinaison des deux. Voyons les types de volumes standards, sachant qu\u2019il existe des modes g\u00e9o-r\u00e9pliqu\u00e9s, stripp\u00e9s ou bas\u00e9s sur l\u2019erasure coding pour des workflows sp\u00e9cifiques. Volume distribu\u00e9 Ce mode est le mode par d\u00e9faut de GlusterFS. Les fichiers sont r\u00e9partis sur l\u2019ensemble des bricks du volume sans redondance aucune. Par cons\u00e9quent, lors de la perte d\u2019un n\u0153ud, les donn\u00e9es de celui-ci sont perdues et il faudra se baser sur des m\u00e9canismes compl\u00e9mentaires pour assurer la reprise apr\u00e8s incident. La volum\u00e9trie utile est celle de l\u2019ensemble des n\u0153uds du cluster Ce mode permet une croissance ais\u00e9e de la volum\u00e9trie en ajoutant simplement des n\u0153uds au volume. Il faut donc au minimum deux n\u0153uds, et la distribution peut se faire sur autant de n\u0153uds du cluster (voir figure 1). Volume r\u00e9pliqu\u00e9 Ce mode permet de r\u00e9pondre au probl\u00e8me de la s\u00e9curit\u00e9 de la donn\u00e9e pos\u00e9 par le mode distribu\u00e9. Dans ce mode op\u00e9ratoire, le syst\u00e8me maintient n copies de chaque fichier au sein des \u00ab bricks \u00bb sp\u00e9cifi\u00e9es. Il faut donc autant de n\u0153uds au cluster que de r\u00e9plicas d\u00e9sir\u00e9s. De la m\u00eame fa\u00e7on que sur du RAID1, la volum\u00e9trie utile est la moiti\u00e9 de la volum\u00e9trie allou\u00e9e (voir figure 2). Volume distribu\u00e9 r\u00e9pliqu\u00e9 Vous l\u2019aurez compris, ce mode est une combinaison des deux modes pr\u00e9c\u00e9dents. Cela permet de traiter des workflows n\u00e9cessitant disponibilit\u00e9 et capacit\u00e9 \u00e0 monter en charge. Le nombre de bricks n\u00e9cessaires est un multiple du niveau de r\u00e9plication attendu. De plus, la r\u00e9plication entre les bricks est d\u00e9finie par leur ordre de d\u00e9claration \u00e0 la cr\u00e9ation du volume. Pour quatre bricks avec deux r\u00e9plicas, les deux premi\u00e8res bricks r\u00e9pliquent ensemble et de m\u00eame pour les deux suivantes (voir figure 3). Si nous souhaitions quatre r\u00e9plicas, il nous faudrait donc huit bricks et les quatre premi\u00e8res r\u00e9pliqueraient entre elles. L\u2019environnement GlusterFS est assez agnostique par rapport \u00e0 l\u2019environnement et \u00e0 la distribution. Pour ma part, les d\u00e9monstrations suivantes seront toutes r\u00e9alis\u00e9es sous Ubuntu 16.04. Un point indispensable \u00e9tant que les n\u0153uds soient capables de discuter par leur nom, qu\u2019il soit r\u00e9solu par DNS ou par le fichier hosts, mais pas par adresse IP. Pour la suite, je partirai sur deux VMs stor0 et stor1 afin de monter un cluster \u00e0 deux n\u0153uds. Chaque serveur dispose d\u2019un second disque virtuel de 10 Gio pour la d\u00e9monstration. Le p\u00e9riph\u00e9rique doit par contre imp\u00e9rativement disposer d\u2019un syst\u00e8me de fichiers supportant les attributs \u00e9tendus, ext4 ou XFS sachant qu\u2019XFS est de loin le syst\u00e8me de fichiers recommand\u00e9. La convention de nommage veut, mais n\u2019impose pas, que les donn\u00e9es soient plac\u00e9es dans /data/glusterfs/volume/brick. La cr\u00e9ation des volumes peut se faire simplement comme ceci : apt-get -y install lvm2 acl attr xfsprogs $(echo o; echo n; echo p; echo 1; echo ; echo; echo t; echo 8E; echo w) | fdisk /dev/sdb pvcreate /dev/sdb1 vgcreate VG-Brick0 /dev/sdb1 lvcreate -l 100%VG -n LV-Brick0 VG-Brick0 mkfs.xfs -i size=512 -L Brick0 /dev/VG-Brick0/LV-Brick0 mkdir -p /data/glusterfs/vol0/ echo \"/dev/VG-Brick0/LV-Brick0 /data/glusterfs/vol0/ xfs defaults 1 2\" >> /etc/fstab mount /data/glusterfs/vol0/ mkdir /data/glusterfs/vol0/brick0 Le LVM n\u2019est pas obligatoire, mais s\u2019y tenir permet d\u2019avoir les bons r\u00e9flexes pour de la production. Pour le reste, l\u2019installation des packages est tr\u00e8s simple, cette simple commande suffit : apt-get -y install glusterfs-server Par souci de simplification, aucun pare-feu n\u2019est activ\u00e9 sur les diff\u00e9rentes machines. Point de vigilance, il ne faudra pas cloner les machines avec le disque de donn\u00e9es suppl\u00e9mentaire. Le trusted pool Avant d\u2019\u00eatre en mesure de g\u00e9rer des volumes de stockage, les membres d\u2019un cluster GlusterFS doivent se reconna\u00eetre entre eux et faire partie d\u2019un m\u00eame trusted pool. Tant que cette op\u00e9ration n\u2019est pas r\u00e9alis\u00e9e, il n\u2019est pas possible pour un h\u00f4te de joindre le r\u00e9seau de stockage. Pour cela, c\u2019est tr\u00e8s simple, il suffit depuis un n\u0153ud de sonder avec la commande gluster peer probe d\u2019ajouter les autres n\u0153uds : root@stor1:~# gluster peer probe stor0 peer probe: success. Pour v\u00e9rifier : root @stor1 : ~ # gluster peer status Number of Peers : 1 Hostname : stor0 Uuid : a920b020 - 9e5 a - 46 f6 - b073 - 1 cc8ec00ba0e State : Peer in Cluster ( Connected ) Un volume r\u00e9pliqu\u00e9 On va poursuivre notre itin\u00e9raire au sein de GlusterFS en cr\u00e9ant un volume r\u00e9pliqu\u00e9 \u00e0 deux n\u0153uds. J\u2019ai donn\u00e9 en introduction un exemple bas\u00e9 sur des attentes de haute disponibilit\u00e9 du stockage, il me semble pertinent de poursuivre sur cet exemple qui parlera sans doute davantage. Notre cluster ayant deux n\u0153uds, avec conservation de deux copies, cela nous fait donc un syst\u00e8me en miroir. Sur chaque serveur, on indique le dossier dans lequel se trouvent les donn\u00e9es. Par s\u00e9curit\u00e9, il est pr\u00e9conis\u00e9 de cr\u00e9er le volume dans un sous-r\u00e9pertoire du point de montage afin qu\u2019en cas d\u2019\u00e9chec de montage du volume, cela n\u2019ait pas d\u2019incidence sur la r\u00e9plication gluster. Du fait du risque d\u2019avoir un dossier vide sur un membre du cluster lors du d\u00e9marrage des services, le comportement ne serait pas forc\u00e9ment pr\u00e9visible. root @stor1 : ~ # gluster volume create repl - vol replica 2 transport tcp stor0 : / data / glusterfs / vol0 / brick0 / stor1 : / data / glusterfs / vol0 / brick0 / volume create : repl - vol : success : please start the volume to access data root @stor1 : ~ # gluster volume start repl - vol volume start : repl - vol : success On peut donc v\u00e9rifier que tout est en ordre avec la commande ci-dessous. Le volume doit \u00eatre marqu\u00e9 comme online sur l\u2019ensemble des n\u0153uds : root @stor1 : ~ # gluster volume status Status of volume : repl - vol Gluster process TCP Port RDMA Port Online Pid ------------------------------------------------------------------------------ Brick stor0 : / data / glusterfs / vol0 / brick0 49152 0 Y 2536 Brick stor1 : / data / glusterfs / vol0 / brick0 49152 0 Y 2338 NFS Server on localhost 2049 0 Y 2359 Self - heal Daemon on localhost N / A N / A Y 2364 NFS Server on stor0 2049 0 Y 2557 Self - heal Daemon on stor0 N / A N / A Y 2562 Task Status of Volume repl - vol ------------------------------------------------------------------------------ There are no active volume tasks Connexion d\u2019un client Il existe trois m\u00e9canismes d\u2019acc\u00e8s principaux c\u00f4t\u00e9 client : \u2013 le client natif acc\u00e9d\u00e9 au travers de FUSE, le syst\u00e8me permettant de cr\u00e9er des pilotes de filesystem au niveau userland. Il suffit pour cela d\u2019installer les packages n\u00e9cessaires. \u2013 via NFS, Gluster imp\u00e9mentant nativement le support NFS. Si vous avez \u00e9t\u00e9 vigilant lors de l\u2019installation du package glusterfs-server, vous avez s\u00fbrement remarqu\u00e9 certaines d\u00e9pendances. Le serveur NFS n\u2019est pas activ\u00e9 par d\u00e9faut cependant. \u2013 en CIFS, avec un serveur Samba. Dans les deux derniers cas, il est souhaitable d\u2019associer les serveurs \u00e0 un syst\u00e8me de type CTDB pour fournir de la haute disponibilit\u00e9. NFS et Samba ne savent en effet pas tirer parti de l\u2019ensemble des fonctionnalit\u00e9s contrairement au client natif. Connectons donc un premier client : apt-get -y install glusterfs-client mkdir /data mount -t glusterfs stor1:/repl-vol /data Cr\u00e9ons un fichier al\u00e9atoire avec par exemple la commande ci-dessous. dd if=/dev/urandom of=/data/toto bs=1024 count=10240 Pour confirmer que la r\u00e9plication est fonctionnelle, il suffit de v\u00e9rifier avec une simple commande ls que le fichier est pr\u00e9sent sur les bricks de chacun des deux serveurs GlusterFS : ls -l /data/glusterfs/vol0/brick0/toto -rw-r--r-- 2 root root 10485760 sept. 9 19:58 /data/glusterfs/vol0/brick0/toto Un point qui a d\u00fb vous surprendre est la commande de montage. On a en effet explicitement sp\u00e9cifi\u00e9 l\u2019un des serveurs alors que l\u2019on est cens\u00e9 avoir d\u00e9ploy\u00e9 un stockage hautement disponible. En pratique, le client natif glusterfs ne fait que r\u00e9cup\u00e9rer lors de la commande de mount les informations de configuration du cluster. Il communiquera directement avec l\u2019ensemble des serveurs d\u00e9finis dans les volfile (dans le r\u00e9pertoire /var/lib/glusterd/vols/repl-vol sur les n\u0153uds de stockage). Un bon moyen de v\u00e9rifier est d\u2019arr\u00eater le n\u0153ud vers lequel on a r\u00e9alis\u00e9 le montage (un halt -p sur stor1 dans ce cas) : le client doit continuer \u00e0 fonctionner. C\u00f4t\u00e9 client, la perte de connexion doit \u00eatre visible dans le fichier /var/log/glusterfs/data.log [2017-09-09 18:01:18.933835] W [socket.c:588:__socket_rwv] 0-glusterfs: readv on 192.168.69.61:24007 failed (Aucune donn\u00e9e disponible) [2017-09-09 18:01:37.954070] W [socket.c:588:__socket_rwv] 0-repl-vol-client-1: readv on 192.168.69.61:49152 failed (Connexion termin\u00e9e par expiration du d\u00e9lai d'attente) Un point que vous aurez not\u00e9 \u00e9galement, c\u2019est que la bascule n\u2019est pas imm\u00e9diate. En pratique, le d\u00e9lai est de 42 secondes. Pour ramener ce d\u00e9lai \u00e0 une valeur plus raisonnable de 5 secondes, modifions notre n\u0153ud comme suit : root@stor1:~# gluster volume set repl-vol network.ping-timeout 5 volume set: success Ce changement est trac\u00e9 dans le log /var/log/glusterfs/glustershd.log avec une ligne par n\u0153ud comme celle-ci : [2017-09-09 18:33:19.591108] I [rpc-clnt.c:1823:rpc_clnt_reconfig] 0-repl-vol-client-0: changing ping timeout to 5 (from 42) Un brin de s\u00e9curit\u00e9 Jusqu\u2019ici, on a pu monter le volume simplement en contactant l\u2019un des serveurs du pool GlusterFS, mais aucune s\u00e9curit\u00e9 suppl\u00e9mentaire n\u2019a \u00e9t\u00e9 impos\u00e9e. Il est possible de restreindre l\u2019acc\u00e8s \u00e0 notre volume en d\u00e9finissant une ACL similaire \u00e0 ce qui existe en NFS via le fichier /etc/exports. root@stor2:~# gluster volume set repl-vol auth.allow 192.168.69.104 volume set: success Il est \u00e9galement possible de d\u00e9finir une wildcard, par exemple 192.168.69. afin d\u2019autoriser tout un r\u00e9seau. Dans cet exemple, nous avons autoris\u00e9 explicitement une adresse IP \u00e0 se connecter au volume. Nous aurions \u00e9galement pu autoriser un nom d\u2019h\u00f4te ou plusieurs adresses IP ou noms s\u00e9par\u00e9s par des virgules. Le fait de d\u00e9finir l\u2019attribut auth.allow a comme effet imm\u00e9diat d\u2019interdire toutes les autres machines qui n\u2019ont pas \u00e9t\u00e9 explicitement autoris\u00e9es. Pour revenir au comportement par d\u00e9faut, il faut autoriser le caract\u00e8re wildcard ( ) tout simplement. A l\u2019inverse, l\u2019attribut auth.reject n\u2019interdit aucune machine par d\u00e9faut (auth.reject avec comme valeur NONE). Il sert comme vous l\u2019avez devin\u00e9 \u00e0 interdire explicitement une machine. Pour r\u00e9sumer, le contr\u00f4le d\u2019acc\u00e8s a une logique similaire avec ce qui existe c\u00f4t\u00e9 TCP Wrappers. Une corbeille sur le volume GlusterFS sait g\u00e9rer une corbeille au niveau volume pour conserver les fichiers supprim\u00e9s. Le dossier est cr\u00e9\u00e9 automatiquement par gluster et ne peut \u00eatre supprim\u00e9. Fait int\u00e9ressant, gluster sait si on le lui dit, tirer parti de cette corbeille pour ses op\u00e9rations internes. Activons donc une corbeille pour les fichiers de moins de 10 Mio : gluster volume set repl-vol features.trash on gluster volume set repl-vol features.trash-dir \"Corbeille\" gluster volume set repl-vol features.trash-max-filesize 10485760 gluster volume set repl-vol features.trash-internal-op on Node HS ? Pas de panique ! Un incident majeur sur un \u00e9quipement sensible d\u2019un syst\u00e8me d\u2019information, c\u2019est bien entendu quelque chose auquel on se doit d\u2019\u00eatre pr\u00e9par\u00e9. Dans un syst\u00e8me hautement disponible, tout \u00e9l\u00e9ment qui n\u2019est pas consid\u00e9r\u00e9 comme un point unique de d\u00e9faillance (SPOF) doit pouvoir \u00eatre indisponible sans impacter fortement le bon fonctionnement du syst\u00e8me. Nous nous retrouvons dans un \u00e9tat de fonctionnement d\u00e9grad\u00e9. Si le syst\u00e8me d\u00e9faillant ne peut \u00eatre d\u00e9pann\u00e9, un processus de reconstruction doit \u00eatre mis en \u0153uvre. Nous allons consid\u00e9rer que le n\u0153ud stor0 est irr\u00e9m\u00e9diablement d\u00e9faillant, la VM est m\u00eame supprim\u00e9e. Cela se v\u00e9rifie par la commande suivante : root @stor1 : ~ # gluster volume heal repl - vol info Brick stor0 : / data / glusterfs / vol0 / brick0 Status : Noeud final de transport n ' est pas connect\u00e9 Brick stor1 : / data / glusterfs / vol0 / brick0 Number of entries : 0 Voyons \u00e9tape par \u00e9tape comment le nouveau serveur nomm\u00e9 stor2 va prendre de relais de celui-ci. Pour cela, la premi\u00e8re \u00e9tape que je ne vais pas d\u00e9tailler consiste \u00e0 provisionner un nouveau serveur avec le disque de donn\u00e9es et les d\u00e9pendances comme indiqu\u00e9 pr\u00e9c\u00e9demment. Premi\u00e8rement, on ajoute le nouveau n\u0153ud et on va confirmer qu\u2019on a bien un nouveau n\u0153ud pr\u00e9sent, et un ancien toujours connu du cluster mais manquant : root @stor1 : ~ # gluster peer probe stor2 peer probe : success . root @stor1 : ~ # gluster peer status Number of Peers : 2 Hostname : stor0 Uuid : a920b020 - 9e5 a - 46 f6 - b073 - 1 cc8ec00ba0e State : Peer in Cluster ( Disconnected ) Hostname : stor2 Uuid : f2a03465 - 11 bb - 4 c2a - a882 - 22933 cfa2d08 State : Peer in Cluster ( Connected ) Rempla\u00e7ons maintenant la brick du stor0 par celle de notre nouveau serveur stor2 et v\u00e9rifions son \u00e9tat de sant\u00e9 : root@stor1:~# gluster volume replace-brick repl-vol stor0:/data/glusterfs/vol0/brick0 stor2:/data/glusterfs/vol0/brick0 commit force volume replace-brick: success: replace-brick commit force operation successful On r\u00e9concilie le volume : root @stor1 : ~ # gluster volume heal repl - vol full Launching heal operation to perform full self heal on volume repl - vol has been successful Use heal info commands to check status root @stor1 : ~ # gluster volume heal repl - vol info Brick stor2 : / data / glusterfs / vol0 / brick0 Number of entries : 0 Brick stor1 : / data / glusterfs / vol0 / brick0 Number of entries : 0 Et depuis le nouveau node, lan\u00e7ons une synchronisation : root@stor2:/data/glusterfs/vol0/brick0# gluster volume sync stor1 repl-vol Sync volume may make data inaccessible while the sync is in progress. Do you want to continue? (y/n) y Il nous reste une derni\u00e8re \u00e9tape : r\u00e9pliquer le volume id dans les attributs \u00e9tendus du syst\u00e8me de fichiers et le propager au second serveur. Pour le r\u00e9cup\u00e9rer, il faut lancer la commande suivante : root @stor1 : ~ # getfattr - n trusted . glusterfs . volume - id / data / glusterfs / vol0 / brick0 / getfattr : Suppression des \u00ab / \u00bb en t\u00eate des chemins absolus # file : data / glusterfs / vol0 / brick0 / trusted . glusterfs . volume - id = 0 seEhN1zXZTFOXmRGV92ibvw == Sur le nouveau serveur, on applique l\u2019ID du volume sur la brick : root@stor2:/data/glusterfs/vol0/brick0# setfattr -n trusted.glusterfs.volume-id -v '0seEhN1zXZTFOXmRGV92ibvw==' /data/glusterfs/vol0/brick0/ service glusterfs-server restart La configuration de notre volume est bien mise \u00e0 jour comme on peut le voir ci-dessous. Dans le cadre d\u2019un volume distribu\u00e9, il faudrait lancer un r\u00e9\u00e9quilibrage (rebalance) du volume : root@stor1:~# gluster volume info repl-vol Volume Name: repl-vol Type: Replicate Volume ID: 1c493043-9c2d-4be6-afcd-8512577342c9 Status: Started Number of Bricks: 1 x 2 = 2 Transport-type: tcp Bricks: Brick1: stor2:/data/glusterfs/vol0/brick0 Brick2: stor1:/data/glusterfs/vol0/brick0 Options Reconfigured: performance.readdir-ahead: on cluster.self-heal-daemon: enable network.ping-timeout: 5 Enfin, il ne reste plus qu\u2019\u00e0 retirer l\u2019ancien n\u0153ud des peers autoris\u00e9s dans le trusted pool : root @stor1 : ~ # gluster peer detach stor0 peer detach : success root @stor1 : ~ # gluster peer status Number of Peers : 1 Hostname : stor2 Uuid : f2a03465 - 11 bb - 4 c2a - a882 - 22933 cfa2d08 State : Peer in Cluster ( Connected ) Il ne doit plus appara\u00eetre dans la liste des n\u0153uds : root@stor1:~# gluster pool list UUID Hostname State 0cb0f3b6-10e5-41c4-ad7e-cb9ca794db9e stor2 Connected d5ff9617-6989-48ae-be3a-3e1286060ea1 localhost Connected Vous savez d\u00e9sormais comment remplacer un n\u0153ud d\u00e9faillant, sachant que ce processus s\u2019applique \u00e9galement en cas de migration de la brick de stor0 vers un nouveau serveur. \u00c9tendre le volume Quand l\u2019espace disque commence \u00e0 manquer, une premi\u00e8re solution peut \u00eatre d\u2019\u00e9tendre l\u2019espace libre sur les bricks, d\u2019o\u00f9 l\u2019int\u00e9r\u00eat d\u2019\u00eatre partis au d\u00e9part sur du LVM. Une autre solution est d\u2019\u00e9tendre le cluster avec de nouveaux n\u0153uds afin d\u2019am\u00e9liorer la disponibilit\u00e9 du syst\u00e8me dans son ensemble. Cette extension du cluster se fait en outre sans interruption de service. Pour \u00e9tendre un cluster r\u00e9pliqu\u00e9, il faut ajouter un nombre de bricks avec un nombre multiple du nombre de r\u00e9plicas. Nous avons mont\u00e9 un volume \u00e0 deux r\u00e9plicas, il nous faut donc ajouter deux bricks suppl\u00e9mentaires. La commande gluster volume info repl-vol nous permet de le confirmer (1\u00d72). Nous allons ajouter donc deux serveurs stor3 et stor4, avec le volume disque pr\u00e9par\u00e9 et le package glusterfs-server install\u00e9. La premi\u00e8re \u00e9tape consiste \u00e0 autoriser les deux n\u0153uds avec la commande gluster peer probe vue pr\u00e9c\u00e9demment. On peut donc ensuite ajouter des bricks au volume en sp\u00e9cifiant les bricks de nos deux nouveaux serveurs : root @stor2 : ~ # gluster volume add - brick repl - vol stor3 : / data / glusterfs / vol0 / brick0 stor4 : / data / glusterfs / vol0 / brick0 volume add - brick : success V\u00e9rifions notre volume, nous devons retrouver nos deux bricks suppl\u00e9mentaires. root@stor2:~# gluster volume info repl-vol Volume Name: repl-vol Type: Distributed-Replicate Volume ID: 78484dd7-35d9-4c53-9799-1195f7689bbf Status: Started Number of Bricks: 2 x 2 = 4 Transport-type: tcp Bricks: Brick1: stor2:/data/glusterfs/vol0/brick0 Brick2: stor1:/data/glusterfs/vol0/brick0 Brick3: stor3:/data/glusterfs/vol0/brick0 Brick4: stor4:/data/glusterfs/vol0/brick0 Options Reconfigured: performance.readdir-ahead: on Notre volume \u00e0 deux r\u00e9plicas comportant quatre n\u0153uds se comporte donc d\u00e9sormais comme un volume distribu\u00e9 r\u00e9pliqu\u00e9 par la magie de l\u2019extension du volume. Seul probl\u00e8me, il n\u2019y a aucune donn\u00e9e sur les serveurs stor3 et stor4, ce qui n\u2019a pas eu pour effet de lib\u00e9rer de l\u2019espace disque sur les deux premiers serveurs. Il est donc n\u00e9cessaire de r\u00e9partir la volum\u00e9trie sur l\u2019ensemble des bricks qui composent le volume : root@stor2:~# gluster volume rebalance repl-vol start< volume rebalance: repl-vol: success: Rebalance on repl-vol has been started successfully. Use rebalance status command to check status of the rebalance process. ID: dffbed2e-3a0c-4d7d-9f43-9d978a546b04 Pour v\u00e9rifier il suffit de lancer la m\u00eame commande avec le param\u00e8tre status : root@stor2:~# gluster volume rebalance repl-vol status Node Rebalanced-files size scanned failures skipped status run time in secs --------- ----------- ----------- ----------- ----------- ----------- ------------ -------------- localhost 5 0Bytes 10 0 0 completed 2.00 stor1 0 0Bytes 0 0 0 completed 1.00 stor3 0 0Bytes 2 0 0 completed 1.00 stor4 0 0Bytes 0 0 0 completed 1.00 volume rebalance: repl-vol: success Les quotas GlusterFS dispose d\u2019un m\u00e9canisme permettant de d\u00e9finir des quotas au niveau dossier. Ils ne sont pas activ\u00e9s par d\u00e9faut. Pour changer ce comportement : root@stor2:~# gluster volume quota repl-vol enable volume quota : success Nous allons appliquer une limite \u00e0 1Gio sur le sous-dossier subdir de notre volume. Ce dossier devra avoir \u00e9t\u00e9 imp\u00e9rativement cr\u00e9\u00e9 depuis le client glusterfs ajout\u00e9 pr\u00e9c\u00e9demment. Pour cr\u00e9er ce quota : root@stor2:~# gluster volume quota repl-vol limit-usage /subdir 1GB volume quota : success Si nous avions souhait\u00e9 cr\u00e9er un quota au niveau du volume, il suffit d\u2019indiquer / dans le chemin. Cr\u00e9ons un fichier approchant le quota depuis notre client GlusterFS : root@desktop:/data/subdir# dd if=/dev/zero of=/data/subdir/toto bs=1024 count=1024000 1024000+0 enregistrements lus 1024000+0 enregistrements \u00e9crits 1048576000 bytes (1,0 GB, 1000 MiB) copied, 223,044 s, 4,7 MB/s Et voyons l\u2019\u00e9tat du quota : root@stor1:~# gluster volume quota repl-vol list Path Hard-limit Soft-limit Used Available Soft-limit exceeded? Hard-limit exceeded? /subdir 1.0GB 80%(819.2MB) 1000.0MB 24.0MB Yes No Reprenons notre commande pr\u00e9c\u00e9dente, en cr\u00e9ant un fichier au nom diff\u00e9rent, la cr\u00e9ation est bien interrompue sur le d\u00e9passement de quota hard : root @desktop : / data # dd if =/ dev / zero of =/ data / subdir / tata bs = 1024 count = 1024000 dd : erreur d '\u00e9criture de ' / data / subdir / tata ': D\u00e9bordement du quota d' espace disque dd : fermeture du fichier de sortie '/data/subdir/tata' : D\u00e9bordement du quota d ' espace disque Il est difficile d\u2019\u00eatre exhaustif sur un sujet aussi vaste. J\u2019esp\u00e8re toutefois avoir aiguis\u00e9 votre app\u00e9tit sur GlusterFS et vous avoir donn\u00e9 l\u2019envie de tester ce qui n\u2019a pas \u00e9t\u00e9 d\u00e9taill\u00e9 ici.","title":"GlusterFS"},{"location":"Filesystems/GlusterFS/#glusterfs","text":"GlusterFS est un syst\u00e8me de fichiers r\u00e9seau client/serveur permettant d\u2019agr\u00e9ger diff\u00e9rents n\u0153uds de stockage afin de fournir un environnement NAS hautement disponible.","title":"GlusterFS"},{"location":"Filesystems/GlusterFS/#pour-quoi-faire","text":"Admettons que j\u2019ai une application Web lambda, je vais pouvoir d\u00e9ployer plusieurs instances Apache ou Nginx qui se trouveront derri\u00e8re un \u00e9quilibreur de charge, lui-m\u00eame hautement disponible. Sur chaque instance de serveur Web, il me sera facile de d\u00e9ployer l\u2019application. Toutefois chaque instance aura besoin d\u2019acc\u00e9der \u00e0 des fichiers communs, g\u00e9n\u00e9r\u00e9s ou non par l\u2019application. Bien souvent, je vais rencontrer dans ce cas un serveur NFS qui va donc lui-m\u00eame constituer un point de faiblesse dans l\u2019architecture. Gluster permet de mettre en cluster plusieurs n\u0153uds de stockage (\u00e0 minima deux), ce qui permet de r\u00e9pondre \u00e0 deux probl\u00e9matiques majeures d\u00e8s qu\u2019une application a besoin de pouvoir monter en charge : la parall\u00e9lisation et la r\u00e9plication du stockage. Pour fournir ces fonctionnalit\u00e9s sur un volume, une \u00ab brick \u00bb en langage Gluster, le syst\u00e8me s\u2019appuie sur des syst\u00e8mes de fichiers traditionnels, XFS ou EXT4 au-dessus d\u2019un p\u00e9riph\u00e9rique en mode bloc (partition, LVM, RAID, etc..). Gluster travaille donc principalement au niveau fichier. Contrairement \u00e0 un certain nombre d\u2019autres syst\u00e8mes de fichiers de ce type, Gluster offre l\u2019immense avantage de ne pas n\u00e9cessiter de serveur de m\u00e9ta donn\u00e9es pour fonctionner. De fait, cette absence ne constitue pas un point de faiblesse ou un \u00e9l\u00e9ment suppl\u00e9mentaire \u00e0 maintenir dans l\u2019infrastructure de stockage. De plus, chaque fois que l\u2019on ajoute un n\u0153ud au cluster, le syst\u00e8me devient plus performant et l\u2019augmentation de la performance est lin\u00e9aire avec l\u2019extension de l\u2019infrastructure. Dernier point pour mettre en \u00e9vidence cette simplicit\u00e9 de conception, il n\u2019existe pas de notion de ma\u00eetre ou d\u2019esclave avec GlusterFS.","title":"Pour quoi faire\u00a0?"},{"location":"Filesystems/GlusterFS/#les-volumes-glusterfs","text":"Un volume est une agr\u00e9gation de plusieurs bricks r\u00e9partis sur diff\u00e9rents n\u0153uds de stockage. Le choix du type de volume se fait en fonction des attentes de performances, de s\u00e9curit\u00e9 ou de la combinaison des deux. Voyons les types de volumes standards, sachant qu\u2019il existe des modes g\u00e9o-r\u00e9pliqu\u00e9s, stripp\u00e9s ou bas\u00e9s sur l\u2019erasure coding pour des workflows sp\u00e9cifiques.","title":"Les volumes GlusterFS"},{"location":"Filesystems/GlusterFS/#volume-distribue","text":"Ce mode est le mode par d\u00e9faut de GlusterFS. Les fichiers sont r\u00e9partis sur l\u2019ensemble des bricks du volume sans redondance aucune. Par cons\u00e9quent, lors de la perte d\u2019un n\u0153ud, les donn\u00e9es de celui-ci sont perdues et il faudra se baser sur des m\u00e9canismes compl\u00e9mentaires pour assurer la reprise apr\u00e8s incident. La volum\u00e9trie utile est celle de l\u2019ensemble des n\u0153uds du cluster Ce mode permet une croissance ais\u00e9e de la volum\u00e9trie en ajoutant simplement des n\u0153uds au volume. Il faut donc au minimum deux n\u0153uds, et la distribution peut se faire sur autant de n\u0153uds du cluster (voir figure 1).","title":"Volume distribu\u00e9"},{"location":"Filesystems/GlusterFS/#volume-replique","text":"Ce mode permet de r\u00e9pondre au probl\u00e8me de la s\u00e9curit\u00e9 de la donn\u00e9e pos\u00e9 par le mode distribu\u00e9. Dans ce mode op\u00e9ratoire, le syst\u00e8me maintient n copies de chaque fichier au sein des \u00ab bricks \u00bb sp\u00e9cifi\u00e9es. Il faut donc autant de n\u0153uds au cluster que de r\u00e9plicas d\u00e9sir\u00e9s. De la m\u00eame fa\u00e7on que sur du RAID1, la volum\u00e9trie utile est la moiti\u00e9 de la volum\u00e9trie allou\u00e9e (voir figure 2).","title":"Volume r\u00e9pliqu\u00e9"},{"location":"Filesystems/GlusterFS/#volume-distribue-replique","text":"Vous l\u2019aurez compris, ce mode est une combinaison des deux modes pr\u00e9c\u00e9dents. Cela permet de traiter des workflows n\u00e9cessitant disponibilit\u00e9 et capacit\u00e9 \u00e0 monter en charge. Le nombre de bricks n\u00e9cessaires est un multiple du niveau de r\u00e9plication attendu. De plus, la r\u00e9plication entre les bricks est d\u00e9finie par leur ordre de d\u00e9claration \u00e0 la cr\u00e9ation du volume. Pour quatre bricks avec deux r\u00e9plicas, les deux premi\u00e8res bricks r\u00e9pliquent ensemble et de m\u00eame pour les deux suivantes (voir figure 3). Si nous souhaitions quatre r\u00e9plicas, il nous faudrait donc huit bricks et les quatre premi\u00e8res r\u00e9pliqueraient entre elles.","title":"Volume distribu\u00e9 r\u00e9pliqu\u00e9"},{"location":"Filesystems/GlusterFS/#lenvironnement","text":"GlusterFS est assez agnostique par rapport \u00e0 l\u2019environnement et \u00e0 la distribution. Pour ma part, les d\u00e9monstrations suivantes seront toutes r\u00e9alis\u00e9es sous Ubuntu 16.04. Un point indispensable \u00e9tant que les n\u0153uds soient capables de discuter par leur nom, qu\u2019il soit r\u00e9solu par DNS ou par le fichier hosts, mais pas par adresse IP. Pour la suite, je partirai sur deux VMs stor0 et stor1 afin de monter un cluster \u00e0 deux n\u0153uds. Chaque serveur dispose d\u2019un second disque virtuel de 10 Gio pour la d\u00e9monstration. Le p\u00e9riph\u00e9rique doit par contre imp\u00e9rativement disposer d\u2019un syst\u00e8me de fichiers supportant les attributs \u00e9tendus, ext4 ou XFS sachant qu\u2019XFS est de loin le syst\u00e8me de fichiers recommand\u00e9. La convention de nommage veut, mais n\u2019impose pas, que les donn\u00e9es soient plac\u00e9es dans /data/glusterfs/volume/brick. La cr\u00e9ation des volumes peut se faire simplement comme ceci : apt-get -y install lvm2 acl attr xfsprogs $(echo o; echo n; echo p; echo 1; echo ; echo; echo t; echo 8E; echo w) | fdisk /dev/sdb pvcreate /dev/sdb1 vgcreate VG-Brick0 /dev/sdb1 lvcreate -l 100%VG -n LV-Brick0 VG-Brick0 mkfs.xfs -i size=512 -L Brick0 /dev/VG-Brick0/LV-Brick0 mkdir -p /data/glusterfs/vol0/ echo \"/dev/VG-Brick0/LV-Brick0 /data/glusterfs/vol0/ xfs defaults 1 2\" >> /etc/fstab mount /data/glusterfs/vol0/ mkdir /data/glusterfs/vol0/brick0 Le LVM n\u2019est pas obligatoire, mais s\u2019y tenir permet d\u2019avoir les bons r\u00e9flexes pour de la production. Pour le reste, l\u2019installation des packages est tr\u00e8s simple, cette simple commande suffit : apt-get -y install glusterfs-server Par souci de simplification, aucun pare-feu n\u2019est activ\u00e9 sur les diff\u00e9rentes machines. Point de vigilance, il ne faudra pas cloner les machines avec le disque de donn\u00e9es suppl\u00e9mentaire.","title":"L\u2019environnement"},{"location":"Filesystems/GlusterFS/#le-trusted-pool","text":"Avant d\u2019\u00eatre en mesure de g\u00e9rer des volumes de stockage, les membres d\u2019un cluster GlusterFS doivent se reconna\u00eetre entre eux et faire partie d\u2019un m\u00eame trusted pool. Tant que cette op\u00e9ration n\u2019est pas r\u00e9alis\u00e9e, il n\u2019est pas possible pour un h\u00f4te de joindre le r\u00e9seau de stockage. Pour cela, c\u2019est tr\u00e8s simple, il suffit depuis un n\u0153ud de sonder avec la commande gluster peer probe d\u2019ajouter les autres n\u0153uds : root@stor1:~# gluster peer probe stor0 peer probe: success. Pour v\u00e9rifier : root @stor1 : ~ # gluster peer status Number of Peers : 1 Hostname : stor0 Uuid : a920b020 - 9e5 a - 46 f6 - b073 - 1 cc8ec00ba0e State : Peer in Cluster ( Connected )","title":"Le trusted pool"},{"location":"Filesystems/GlusterFS/#un-volume-replique","text":"On va poursuivre notre itin\u00e9raire au sein de GlusterFS en cr\u00e9ant un volume r\u00e9pliqu\u00e9 \u00e0 deux n\u0153uds. J\u2019ai donn\u00e9 en introduction un exemple bas\u00e9 sur des attentes de haute disponibilit\u00e9 du stockage, il me semble pertinent de poursuivre sur cet exemple qui parlera sans doute davantage. Notre cluster ayant deux n\u0153uds, avec conservation de deux copies, cela nous fait donc un syst\u00e8me en miroir. Sur chaque serveur, on indique le dossier dans lequel se trouvent les donn\u00e9es. Par s\u00e9curit\u00e9, il est pr\u00e9conis\u00e9 de cr\u00e9er le volume dans un sous-r\u00e9pertoire du point de montage afin qu\u2019en cas d\u2019\u00e9chec de montage du volume, cela n\u2019ait pas d\u2019incidence sur la r\u00e9plication gluster. Du fait du risque d\u2019avoir un dossier vide sur un membre du cluster lors du d\u00e9marrage des services, le comportement ne serait pas forc\u00e9ment pr\u00e9visible. root @stor1 : ~ # gluster volume create repl - vol replica 2 transport tcp stor0 : / data / glusterfs / vol0 / brick0 / stor1 : / data / glusterfs / vol0 / brick0 / volume create : repl - vol : success : please start the volume to access data root @stor1 : ~ # gluster volume start repl - vol volume start : repl - vol : success On peut donc v\u00e9rifier que tout est en ordre avec la commande ci-dessous. Le volume doit \u00eatre marqu\u00e9 comme online sur l\u2019ensemble des n\u0153uds : root @stor1 : ~ # gluster volume status Status of volume : repl - vol Gluster process TCP Port RDMA Port Online Pid ------------------------------------------------------------------------------ Brick stor0 : / data / glusterfs / vol0 / brick0 49152 0 Y 2536 Brick stor1 : / data / glusterfs / vol0 / brick0 49152 0 Y 2338 NFS Server on localhost 2049 0 Y 2359 Self - heal Daemon on localhost N / A N / A Y 2364 NFS Server on stor0 2049 0 Y 2557 Self - heal Daemon on stor0 N / A N / A Y 2562 Task Status of Volume repl - vol ------------------------------------------------------------------------------ There are no active volume tasks","title":"Un volume r\u00e9pliqu\u00e9"},{"location":"Filesystems/GlusterFS/#connexion-dun-client","text":"Il existe trois m\u00e9canismes d\u2019acc\u00e8s principaux c\u00f4t\u00e9 client : \u2013 le client natif acc\u00e9d\u00e9 au travers de FUSE, le syst\u00e8me permettant de cr\u00e9er des pilotes de filesystem au niveau userland. Il suffit pour cela d\u2019installer les packages n\u00e9cessaires. \u2013 via NFS, Gluster imp\u00e9mentant nativement le support NFS. Si vous avez \u00e9t\u00e9 vigilant lors de l\u2019installation du package glusterfs-server, vous avez s\u00fbrement remarqu\u00e9 certaines d\u00e9pendances. Le serveur NFS n\u2019est pas activ\u00e9 par d\u00e9faut cependant. \u2013 en CIFS, avec un serveur Samba. Dans les deux derniers cas, il est souhaitable d\u2019associer les serveurs \u00e0 un syst\u00e8me de type CTDB pour fournir de la haute disponibilit\u00e9. NFS et Samba ne savent en effet pas tirer parti de l\u2019ensemble des fonctionnalit\u00e9s contrairement au client natif. Connectons donc un premier client : apt-get -y install glusterfs-client mkdir /data mount -t glusterfs stor1:/repl-vol /data Cr\u00e9ons un fichier al\u00e9atoire avec par exemple la commande ci-dessous. dd if=/dev/urandom of=/data/toto bs=1024 count=10240 Pour confirmer que la r\u00e9plication est fonctionnelle, il suffit de v\u00e9rifier avec une simple commande ls que le fichier est pr\u00e9sent sur les bricks de chacun des deux serveurs GlusterFS : ls -l /data/glusterfs/vol0/brick0/toto -rw-r--r-- 2 root root 10485760 sept. 9 19:58 /data/glusterfs/vol0/brick0/toto Un point qui a d\u00fb vous surprendre est la commande de montage. On a en effet explicitement sp\u00e9cifi\u00e9 l\u2019un des serveurs alors que l\u2019on est cens\u00e9 avoir d\u00e9ploy\u00e9 un stockage hautement disponible. En pratique, le client natif glusterfs ne fait que r\u00e9cup\u00e9rer lors de la commande de mount les informations de configuration du cluster. Il communiquera directement avec l\u2019ensemble des serveurs d\u00e9finis dans les volfile (dans le r\u00e9pertoire /var/lib/glusterd/vols/repl-vol sur les n\u0153uds de stockage). Un bon moyen de v\u00e9rifier est d\u2019arr\u00eater le n\u0153ud vers lequel on a r\u00e9alis\u00e9 le montage (un halt -p sur stor1 dans ce cas) : le client doit continuer \u00e0 fonctionner. C\u00f4t\u00e9 client, la perte de connexion doit \u00eatre visible dans le fichier /var/log/glusterfs/data.log [2017-09-09 18:01:18.933835] W [socket.c:588:__socket_rwv] 0-glusterfs: readv on 192.168.69.61:24007 failed (Aucune donn\u00e9e disponible) [2017-09-09 18:01:37.954070] W [socket.c:588:__socket_rwv] 0-repl-vol-client-1: readv on 192.168.69.61:49152 failed (Connexion termin\u00e9e par expiration du d\u00e9lai d'attente) Un point que vous aurez not\u00e9 \u00e9galement, c\u2019est que la bascule n\u2019est pas imm\u00e9diate. En pratique, le d\u00e9lai est de 42 secondes. Pour ramener ce d\u00e9lai \u00e0 une valeur plus raisonnable de 5 secondes, modifions notre n\u0153ud comme suit : root@stor1:~# gluster volume set repl-vol network.ping-timeout 5 volume set: success Ce changement est trac\u00e9 dans le log /var/log/glusterfs/glustershd.log avec une ligne par n\u0153ud comme celle-ci : [2017-09-09 18:33:19.591108] I [rpc-clnt.c:1823:rpc_clnt_reconfig] 0-repl-vol-client-0: changing ping timeout to 5 (from 42)","title":"Connexion d\u2019un client"},{"location":"Filesystems/GlusterFS/#un-brin-de-securite","text":"Jusqu\u2019ici, on a pu monter le volume simplement en contactant l\u2019un des serveurs du pool GlusterFS, mais aucune s\u00e9curit\u00e9 suppl\u00e9mentaire n\u2019a \u00e9t\u00e9 impos\u00e9e. Il est possible de restreindre l\u2019acc\u00e8s \u00e0 notre volume en d\u00e9finissant une ACL similaire \u00e0 ce qui existe en NFS via le fichier /etc/exports. root@stor2:~# gluster volume set repl-vol auth.allow 192.168.69.104 volume set: success Il est \u00e9galement possible de d\u00e9finir une wildcard, par exemple 192.168.69. afin d\u2019autoriser tout un r\u00e9seau. Dans cet exemple, nous avons autoris\u00e9 explicitement une adresse IP \u00e0 se connecter au volume. Nous aurions \u00e9galement pu autoriser un nom d\u2019h\u00f4te ou plusieurs adresses IP ou noms s\u00e9par\u00e9s par des virgules. Le fait de d\u00e9finir l\u2019attribut auth.allow a comme effet imm\u00e9diat d\u2019interdire toutes les autres machines qui n\u2019ont pas \u00e9t\u00e9 explicitement autoris\u00e9es. Pour revenir au comportement par d\u00e9faut, il faut autoriser le caract\u00e8re wildcard ( ) tout simplement. A l\u2019inverse, l\u2019attribut auth.reject n\u2019interdit aucune machine par d\u00e9faut (auth.reject avec comme valeur NONE). Il sert comme vous l\u2019avez devin\u00e9 \u00e0 interdire explicitement une machine. Pour r\u00e9sumer, le contr\u00f4le d\u2019acc\u00e8s a une logique similaire avec ce qui existe c\u00f4t\u00e9 TCP Wrappers.","title":"Un brin de s\u00e9curit\u00e9"},{"location":"Filesystems/GlusterFS/#une-corbeille-sur-le-volume","text":"GlusterFS sait g\u00e9rer une corbeille au niveau volume pour conserver les fichiers supprim\u00e9s. Le dossier est cr\u00e9\u00e9 automatiquement par gluster et ne peut \u00eatre supprim\u00e9. Fait int\u00e9ressant, gluster sait si on le lui dit, tirer parti de cette corbeille pour ses op\u00e9rations internes. Activons donc une corbeille pour les fichiers de moins de 10 Mio : gluster volume set repl-vol features.trash on gluster volume set repl-vol features.trash-dir \"Corbeille\" gluster volume set repl-vol features.trash-max-filesize 10485760 gluster volume set repl-vol features.trash-internal-op on","title":"Une corbeille sur le volume"},{"location":"Filesystems/GlusterFS/#node-hs-pas-de-panique","text":"Un incident majeur sur un \u00e9quipement sensible d\u2019un syst\u00e8me d\u2019information, c\u2019est bien entendu quelque chose auquel on se doit d\u2019\u00eatre pr\u00e9par\u00e9. Dans un syst\u00e8me hautement disponible, tout \u00e9l\u00e9ment qui n\u2019est pas consid\u00e9r\u00e9 comme un point unique de d\u00e9faillance (SPOF) doit pouvoir \u00eatre indisponible sans impacter fortement le bon fonctionnement du syst\u00e8me. Nous nous retrouvons dans un \u00e9tat de fonctionnement d\u00e9grad\u00e9. Si le syst\u00e8me d\u00e9faillant ne peut \u00eatre d\u00e9pann\u00e9, un processus de reconstruction doit \u00eatre mis en \u0153uvre. Nous allons consid\u00e9rer que le n\u0153ud stor0 est irr\u00e9m\u00e9diablement d\u00e9faillant, la VM est m\u00eame supprim\u00e9e. Cela se v\u00e9rifie par la commande suivante : root @stor1 : ~ # gluster volume heal repl - vol info Brick stor0 : / data / glusterfs / vol0 / brick0 Status : Noeud final de transport n ' est pas connect\u00e9 Brick stor1 : / data / glusterfs / vol0 / brick0 Number of entries : 0 Voyons \u00e9tape par \u00e9tape comment le nouveau serveur nomm\u00e9 stor2 va prendre de relais de celui-ci. Pour cela, la premi\u00e8re \u00e9tape que je ne vais pas d\u00e9tailler consiste \u00e0 provisionner un nouveau serveur avec le disque de donn\u00e9es et les d\u00e9pendances comme indiqu\u00e9 pr\u00e9c\u00e9demment. Premi\u00e8rement, on ajoute le nouveau n\u0153ud et on va confirmer qu\u2019on a bien un nouveau n\u0153ud pr\u00e9sent, et un ancien toujours connu du cluster mais manquant : root @stor1 : ~ # gluster peer probe stor2 peer probe : success . root @stor1 : ~ # gluster peer status Number of Peers : 2 Hostname : stor0 Uuid : a920b020 - 9e5 a - 46 f6 - b073 - 1 cc8ec00ba0e State : Peer in Cluster ( Disconnected ) Hostname : stor2 Uuid : f2a03465 - 11 bb - 4 c2a - a882 - 22933 cfa2d08 State : Peer in Cluster ( Connected ) Rempla\u00e7ons maintenant la brick du stor0 par celle de notre nouveau serveur stor2 et v\u00e9rifions son \u00e9tat de sant\u00e9 : root@stor1:~# gluster volume replace-brick repl-vol stor0:/data/glusterfs/vol0/brick0 stor2:/data/glusterfs/vol0/brick0 commit force volume replace-brick: success: replace-brick commit force operation successful On r\u00e9concilie le volume : root @stor1 : ~ # gluster volume heal repl - vol full Launching heal operation to perform full self heal on volume repl - vol has been successful Use heal info commands to check status root @stor1 : ~ # gluster volume heal repl - vol info Brick stor2 : / data / glusterfs / vol0 / brick0 Number of entries : 0 Brick stor1 : / data / glusterfs / vol0 / brick0 Number of entries : 0 Et depuis le nouveau node, lan\u00e7ons une synchronisation : root@stor2:/data/glusterfs/vol0/brick0# gluster volume sync stor1 repl-vol Sync volume may make data inaccessible while the sync is in progress. Do you want to continue? (y/n) y Il nous reste une derni\u00e8re \u00e9tape : r\u00e9pliquer le volume id dans les attributs \u00e9tendus du syst\u00e8me de fichiers et le propager au second serveur. Pour le r\u00e9cup\u00e9rer, il faut lancer la commande suivante : root @stor1 : ~ # getfattr - n trusted . glusterfs . volume - id / data / glusterfs / vol0 / brick0 / getfattr : Suppression des \u00ab / \u00bb en t\u00eate des chemins absolus # file : data / glusterfs / vol0 / brick0 / trusted . glusterfs . volume - id = 0 seEhN1zXZTFOXmRGV92ibvw == Sur le nouveau serveur, on applique l\u2019ID du volume sur la brick : root@stor2:/data/glusterfs/vol0/brick0# setfattr -n trusted.glusterfs.volume-id -v '0seEhN1zXZTFOXmRGV92ibvw==' /data/glusterfs/vol0/brick0/ service glusterfs-server restart La configuration de notre volume est bien mise \u00e0 jour comme on peut le voir ci-dessous. Dans le cadre d\u2019un volume distribu\u00e9, il faudrait lancer un r\u00e9\u00e9quilibrage (rebalance) du volume : root@stor1:~# gluster volume info repl-vol Volume Name: repl-vol Type: Replicate Volume ID: 1c493043-9c2d-4be6-afcd-8512577342c9 Status: Started Number of Bricks: 1 x 2 = 2 Transport-type: tcp Bricks: Brick1: stor2:/data/glusterfs/vol0/brick0 Brick2: stor1:/data/glusterfs/vol0/brick0 Options Reconfigured: performance.readdir-ahead: on cluster.self-heal-daemon: enable network.ping-timeout: 5 Enfin, il ne reste plus qu\u2019\u00e0 retirer l\u2019ancien n\u0153ud des peers autoris\u00e9s dans le trusted pool : root @stor1 : ~ # gluster peer detach stor0 peer detach : success root @stor1 : ~ # gluster peer status Number of Peers : 1 Hostname : stor2 Uuid : f2a03465 - 11 bb - 4 c2a - a882 - 22933 cfa2d08 State : Peer in Cluster ( Connected ) Il ne doit plus appara\u00eetre dans la liste des n\u0153uds : root@stor1:~# gluster pool list UUID Hostname State 0cb0f3b6-10e5-41c4-ad7e-cb9ca794db9e stor2 Connected d5ff9617-6989-48ae-be3a-3e1286060ea1 localhost Connected Vous savez d\u00e9sormais comment remplacer un n\u0153ud d\u00e9faillant, sachant que ce processus s\u2019applique \u00e9galement en cas de migration de la brick de stor0 vers un nouveau serveur.","title":"Node HS\u00a0? Pas de panique\u00a0!"},{"location":"Filesystems/GlusterFS/#etendre-le-volume","text":"Quand l\u2019espace disque commence \u00e0 manquer, une premi\u00e8re solution peut \u00eatre d\u2019\u00e9tendre l\u2019espace libre sur les bricks, d\u2019o\u00f9 l\u2019int\u00e9r\u00eat d\u2019\u00eatre partis au d\u00e9part sur du LVM. Une autre solution est d\u2019\u00e9tendre le cluster avec de nouveaux n\u0153uds afin d\u2019am\u00e9liorer la disponibilit\u00e9 du syst\u00e8me dans son ensemble. Cette extension du cluster se fait en outre sans interruption de service. Pour \u00e9tendre un cluster r\u00e9pliqu\u00e9, il faut ajouter un nombre de bricks avec un nombre multiple du nombre de r\u00e9plicas. Nous avons mont\u00e9 un volume \u00e0 deux r\u00e9plicas, il nous faut donc ajouter deux bricks suppl\u00e9mentaires. La commande gluster volume info repl-vol nous permet de le confirmer (1\u00d72). Nous allons ajouter donc deux serveurs stor3 et stor4, avec le volume disque pr\u00e9par\u00e9 et le package glusterfs-server install\u00e9. La premi\u00e8re \u00e9tape consiste \u00e0 autoriser les deux n\u0153uds avec la commande gluster peer probe vue pr\u00e9c\u00e9demment. On peut donc ensuite ajouter des bricks au volume en sp\u00e9cifiant les bricks de nos deux nouveaux serveurs : root @stor2 : ~ # gluster volume add - brick repl - vol stor3 : / data / glusterfs / vol0 / brick0 stor4 : / data / glusterfs / vol0 / brick0 volume add - brick : success V\u00e9rifions notre volume, nous devons retrouver nos deux bricks suppl\u00e9mentaires. root@stor2:~# gluster volume info repl-vol Volume Name: repl-vol Type: Distributed-Replicate Volume ID: 78484dd7-35d9-4c53-9799-1195f7689bbf Status: Started Number of Bricks: 2 x 2 = 4 Transport-type: tcp Bricks: Brick1: stor2:/data/glusterfs/vol0/brick0 Brick2: stor1:/data/glusterfs/vol0/brick0 Brick3: stor3:/data/glusterfs/vol0/brick0 Brick4: stor4:/data/glusterfs/vol0/brick0 Options Reconfigured: performance.readdir-ahead: on Notre volume \u00e0 deux r\u00e9plicas comportant quatre n\u0153uds se comporte donc d\u00e9sormais comme un volume distribu\u00e9 r\u00e9pliqu\u00e9 par la magie de l\u2019extension du volume. Seul probl\u00e8me, il n\u2019y a aucune donn\u00e9e sur les serveurs stor3 et stor4, ce qui n\u2019a pas eu pour effet de lib\u00e9rer de l\u2019espace disque sur les deux premiers serveurs. Il est donc n\u00e9cessaire de r\u00e9partir la volum\u00e9trie sur l\u2019ensemble des bricks qui composent le volume : root@stor2:~# gluster volume rebalance repl-vol start< volume rebalance: repl-vol: success: Rebalance on repl-vol has been started successfully. Use rebalance status command to check status of the rebalance process. ID: dffbed2e-3a0c-4d7d-9f43-9d978a546b04 Pour v\u00e9rifier il suffit de lancer la m\u00eame commande avec le param\u00e8tre status : root@stor2:~# gluster volume rebalance repl-vol status Node Rebalanced-files size scanned failures skipped status run time in secs --------- ----------- ----------- ----------- ----------- ----------- ------------ -------------- localhost 5 0Bytes 10 0 0 completed 2.00 stor1 0 0Bytes 0 0 0 completed 1.00 stor3 0 0Bytes 2 0 0 completed 1.00 stor4 0 0Bytes 0 0 0 completed 1.00 volume rebalance: repl-vol: success","title":"\u00c9tendre le volume"},{"location":"Filesystems/GlusterFS/#les-quotas","text":"GlusterFS dispose d\u2019un m\u00e9canisme permettant de d\u00e9finir des quotas au niveau dossier. Ils ne sont pas activ\u00e9s par d\u00e9faut. Pour changer ce comportement : root@stor2:~# gluster volume quota repl-vol enable volume quota : success Nous allons appliquer une limite \u00e0 1Gio sur le sous-dossier subdir de notre volume. Ce dossier devra avoir \u00e9t\u00e9 imp\u00e9rativement cr\u00e9\u00e9 depuis le client glusterfs ajout\u00e9 pr\u00e9c\u00e9demment. Pour cr\u00e9er ce quota : root@stor2:~# gluster volume quota repl-vol limit-usage /subdir 1GB volume quota : success Si nous avions souhait\u00e9 cr\u00e9er un quota au niveau du volume, il suffit d\u2019indiquer / dans le chemin. Cr\u00e9ons un fichier approchant le quota depuis notre client GlusterFS : root@desktop:/data/subdir# dd if=/dev/zero of=/data/subdir/toto bs=1024 count=1024000 1024000+0 enregistrements lus 1024000+0 enregistrements \u00e9crits 1048576000 bytes (1,0 GB, 1000 MiB) copied, 223,044 s, 4,7 MB/s Et voyons l\u2019\u00e9tat du quota : root@stor1:~# gluster volume quota repl-vol list Path Hard-limit Soft-limit Used Available Soft-limit exceeded? Hard-limit exceeded? /subdir 1.0GB 80%(819.2MB) 1000.0MB 24.0MB Yes No Reprenons notre commande pr\u00e9c\u00e9dente, en cr\u00e9ant un fichier au nom diff\u00e9rent, la cr\u00e9ation est bien interrompue sur le d\u00e9passement de quota hard : root @desktop : / data # dd if =/ dev / zero of =/ data / subdir / tata bs = 1024 count = 1024000 dd : erreur d '\u00e9criture de ' / data / subdir / tata ': D\u00e9bordement du quota d' espace disque dd : fermeture du fichier de sortie '/data/subdir/tata' : D\u00e9bordement du quota d ' espace disque Il est difficile d\u2019\u00eatre exhaustif sur un sujet aussi vaste. J\u2019esp\u00e8re toutefois avoir aiguis\u00e9 votre app\u00e9tit sur GlusterFS et vous avoir donn\u00e9 l\u2019envie de tester ce qui n\u2019a pas \u00e9t\u00e9 d\u00e9taill\u00e9 ici.","title":"Les quotas"},{"location":"Filesystems/LUKS/","text":"Comment changer le mot de passe qui permet de le d\u00e9verrouiller. Tout d\u2019abord, une fois la partition chiffr\u00e9e mont\u00e9e, vous pouvez l\u2019identifier avec cette commande : blkid | grep \"crypto_LUKS\" /dev/sda5: UUID=\"7a805bed-e309-44e0-8dfa-6994a13d29a1\" TYPE=\"crypto_LUKS\" PARTUUID=\"0001c467-05\" /dev/sde1: UUID=\"1862f1c7-b546-4109-8a2d-03ce47baca6e\" TYPE=\"crypto_LUKS\" PARTUUID=\"8f60cf56-7a68-484d-83c4-2caf8aad230f\" Ensuite pour changer le mot de passe : cryptsetup luksChangeKey /dev/sde1","title":"LUKS"},{"location":"Filesystems/LUKS/#comment-changer-le-mot-de-passe-qui-permet-de-le-deverrouiller","text":"Tout d\u2019abord, une fois la partition chiffr\u00e9e mont\u00e9e, vous pouvez l\u2019identifier avec cette commande : blkid | grep \"crypto_LUKS\" /dev/sda5: UUID=\"7a805bed-e309-44e0-8dfa-6994a13d29a1\" TYPE=\"crypto_LUKS\" PARTUUID=\"0001c467-05\" /dev/sde1: UUID=\"1862f1c7-b546-4109-8a2d-03ce47baca6e\" TYPE=\"crypto_LUKS\" PARTUUID=\"8f60cf56-7a68-484d-83c4-2caf8aad230f\" Ensuite pour changer le mot de passe : cryptsetup luksChangeKey /dev/sde1","title":"Comment changer le mot de passe qui permet de le d\u00e9verrouiller."},{"location":"Filesystems/LVM/","text":"Fix LVM errors Exemple: #Errors: /dev/sdk: read failed after 0 of 4096 at 6442442752: Input/output error /dev/sdk: read failed after 0 of 4096 at 4096: Input/output error Solution : 1) Check which Volume Group have the issue , run \u201cvgscan\u201d command . 2) Find out the Logical Volumes attached with that Volume Group . 3) Inactive the logical volumes as : lvchange -an <lv-name> 4) Inactive Volume group as : vgchange -an <vg-name> 5) Again Scan Volume group using \u201cvgscan\u201d . 6) Now activate the Volume Group : vgchange -ay <volume-group-name> 7) Run command \u201clvscan\u201d , the error should be gone now . 8) Now activate the Logical Volume Name : lvchange -ay <lv-name>","title":"LVM"},{"location":"Filesystems/LVM/#fix-lvm-errors","text":"Exemple: #Errors: /dev/sdk: read failed after 0 of 4096 at 6442442752: Input/output error /dev/sdk: read failed after 0 of 4096 at 4096: Input/output error","title":"Fix LVM errors"},{"location":"Filesystems/LVM/#solution","text":"1) Check which Volume Group have the issue , run \u201cvgscan\u201d command . 2) Find out the Logical Volumes attached with that Volume Group . 3) Inactive the logical volumes as : lvchange -an <lv-name> 4) Inactive Volume group as : vgchange -an <vg-name> 5) Again Scan Volume group using \u201cvgscan\u201d . 6) Now activate the Volume Group : vgchange -ay <volume-group-name> 7) Run command \u201clvscan\u201d , the error should be gone now . 8) Now activate the Logical Volume Name : lvchange -ay <lv-name>","title":"Solution :"},{"location":"Filesystems/RAID_Rebuild/","text":"How to rebuild an MDadm Raid Get disk serial udevadm info --query=all --name=/dev/sda | grep ID_SERIAL Replacing A Failed Hard Drive In A Software RAID1 Array This guide shows how to remove a failed hard drive from a Linux RAID1 array (software RAID), and how to add a new hard disk to the RAID1 array without losing data. NOTE: There is a new version of this tutorial available that uses gdisk instead of sfdisk to support GPT partitions. Context In this example I have two hard drives, /dev/sda and /dev/sdb, with the partitions /dev/sda1 and /dev/sda2 as well as /dev/sdb1 and /dev/sdb2. /dev/sda1 and /dev/sdb1 make up the RAID1 array /dev/md0. /dev/sda2 and /dev/sdb2 make up the RAID1 array /dev/md1. /dev/sda1 + /dev/sdb1 = /dev/md0 /dev/sda2 + /dev/sdb2 = /dev/md1 /dev/sdb has failed, and we want to replace it. How Do I Tell If A Hard Disk Has Failed? If a disk has failed, you will probably find a lot of error messages in the log files, e.g. /var/log/messages or /var/log/syslog. You can also run cat /proc/mdstat and instead of the string [UU] you will see [U_] if you have a degraded RAID1 array. Removing The Failed Disk To remove /dev/sdb, we will mark /dev/sdb1 and /dev/sdb2 as failed and remove them from their respective RAID arrays (/dev/md0 and /dev/md1). First we mark /dev/sdb1 as failed: mdadm --manage /dev/md0 --fail /dev/sdb1 The output of cat /proc/mdstat should look like this: Personalities : [ linear ] [ multipath ] [ raid0 ] [ raid1 ] [ raid5 ] [ raid4 ] [ raid6 ] [ raid10 ] md0 : active raid1 sda1 [ 0 ] sdb1 [ 2 ] ( F ) 24418688 blocks [ 2/1 ] [ U_ ] md1 : active raid1 sda2 [ 0 ] sdb2 [ 1 ] 24418688 blocks [ 2/2 ] [ UU ] unused devices : < none > Then we remove /dev/sdb1 from /dev/md0: mdadm --manage /dev/md0 --remove /dev/sdb1 The output should be like this: mdadm --manage /dev/md0 --remove /dev/sdb1 mdadm: hot removed /dev/sdb1 And cat /proc/mdstat should show this: Personalities : [ linear ] [ multipath ] [ raid0 ] [ raid1 ] [ raid5 ] [ raid4 ] [ raid6 ] [ raid10 ] md0 : active raid1 sda1 [ 0 ] 24418688 blocks [ 2/1 ] [ U_ ] md1 : active raid1 sda2 [ 0 ] sdb2 [ 1 ] 24418688 blocks [ 2/2 ] [ UU ] unused devices : < none > Now we do the same steps again for /dev/sdb2 (which is part of /dev/md1): mdadm --manage /dev/md1 --fail /dev/sdb2 cat / proc / mdstat Personalities : [ linear ] [ multipath ] [ raid0 ] [ raid1 ] [ raid5 ] [ raid4 ] [ raid6 ] [ raid10 ] md0 : active raid1 sda1 [ 0 ] 24418688 blocks [ 2/1 ] [ U_ ] md1 : active raid1 sda2 [ 0 ] sdb2 [ 2 ] ( F ) 24418688 blocks [ 2/1 ] [ U_ ] unused devices : < none > mdadm --manage /dev/md1 --remove /dev/sdb2 mdadm : hot removed / dev / sdb2 cat / proc / mdstat Personalities : [ linear ] [ multipath ] [ raid0 ] [ raid1 ] [ raid5 ] [ raid4 ] [ raid6 ] [ raid10 ] md0 : active raid1 sda1 [ 0 ] 24418688 blocks [ 2/1 ] [ U_ ] md1 : active raid1 sda2 [ 0 ] 24418688 blocks [ 2/1 ] [ U_ ] unused devices : < none > Then power down the system: shutdown -h now and replace the old /dev/sdb hard drive with a new one (it must have at least the same size as the old one - if it\u2019s only a few MB smaller than the old one then rebuilding the arrays will fail). Adding The New Hard Disk After you have changed the hard disk /dev/sdb, boot the system. The first thing we must do now is to create the exact same partitioning as on /dev/sda. We can do this with one simple command: sfdisk -d /dev/sda | sfdisk /dev/sdb You can run fdisk -l to check if both hard drives have the same partitioning now. Next we add /dev/sdb1 to /dev/md0 and /dev/sdb2 to /dev/md1: mdadm --manage /dev/md0 --add /dev/sdb1 mdadm : re - added / dev / sdb1 mdadm --manage /dev/md1 --add /dev/sdb2 mdadm : re - added / dev / sdb2 Now both arays (/dev/md0 and /dev/md1) will be synchronized. Run cat /proc/mdstat to see when it\u2019s finished. During the synchronization the output will look like this: server1 : ~ # cat / proc / mdstat Personalities : [ linear ] [ multipath ] [ raid0 ] [ raid1 ] [ raid5 ] [ raid4 ] [ raid6 ] [ raid10 ] md0 : active raid1 sda1 [ 0 ] sdb1 [ 1 ] 24418688 blocks [ 2/1 ] [ U_ ] [ =>................... ] recovery = 9.9 % ( 2423168 / 24418688 ) finish = 2.8 min speed = 127535 K / sec md1 : active raid1 sda2 [ 0 ] sdb2 [ 1 ] 24418688 blocks [ 2/1 ] [ U_ ] [ =>................... ] recovery = 6.4 % ( 1572096 / 24418688 ) finish = 1.9 min speed = 196512 K / sec unused devices : < none > When the synchronization is finished, the output will look like this: server1 : ~ # cat / proc / mdstat Personalities : [ linear ] [ multipath ] [ raid0 ] [ raid1 ] [ raid5 ] [ raid4 ] [ raid6 ] [ raid10 ] md0 : active raid1 sda1 [ 0 ] sdb1 [ 1 ] 24418688 blocks [ 2/2 ] [ UU ] md1 : active raid1 sda2 [ 0 ] sdb2 [ 1 ] 24418688 blocks [ 2/2 ] [ UU ] unused devices : < none > That\u2019s it, you have successfully replaced /dev/sdb!","title":"RAID Rebuild"},{"location":"Filesystems/RAID_Rebuild/#how-to-rebuild-an-mdadm-raid","text":"","title":"How to rebuild an MDadm Raid"},{"location":"Filesystems/RAID_Rebuild/#get-disk-serial","text":"udevadm info --query=all --name=/dev/sda | grep ID_SERIAL","title":"Get disk serial"},{"location":"Filesystems/RAID_Rebuild/#replacing-a-failed-hard-drive-in-a-software-raid1-array","text":"This guide shows how to remove a failed hard drive from a Linux RAID1 array (software RAID), and how to add a new hard disk to the RAID1 array without losing data. NOTE: There is a new version of this tutorial available that uses gdisk instead of sfdisk to support GPT partitions.","title":"Replacing A Failed Hard Drive In A Software RAID1 Array"},{"location":"Filesystems/RAID_Rebuild/#context","text":"In this example I have two hard drives, /dev/sda and /dev/sdb, with the partitions /dev/sda1 and /dev/sda2 as well as /dev/sdb1 and /dev/sdb2. /dev/sda1 and /dev/sdb1 make up the RAID1 array /dev/md0. /dev/sda2 and /dev/sdb2 make up the RAID1 array /dev/md1. /dev/sda1 + /dev/sdb1 = /dev/md0 /dev/sda2 + /dev/sdb2 = /dev/md1 /dev/sdb has failed, and we want to replace it.","title":"Context"},{"location":"Filesystems/RAID_Rebuild/#how-do-i-tell-if-a-hard-disk-has-failed","text":"If a disk has failed, you will probably find a lot of error messages in the log files, e.g. /var/log/messages or /var/log/syslog. You can also run cat /proc/mdstat and instead of the string [UU] you will see [U_] if you have a degraded RAID1 array.","title":"How Do I Tell If A Hard Disk Has Failed?"},{"location":"Filesystems/RAID_Rebuild/#removing-the-failed-disk","text":"To remove /dev/sdb, we will mark /dev/sdb1 and /dev/sdb2 as failed and remove them from their respective RAID arrays (/dev/md0 and /dev/md1). First we mark /dev/sdb1 as failed: mdadm --manage /dev/md0 --fail /dev/sdb1 The output of cat /proc/mdstat should look like this: Personalities : [ linear ] [ multipath ] [ raid0 ] [ raid1 ] [ raid5 ] [ raid4 ] [ raid6 ] [ raid10 ] md0 : active raid1 sda1 [ 0 ] sdb1 [ 2 ] ( F ) 24418688 blocks [ 2/1 ] [ U_ ] md1 : active raid1 sda2 [ 0 ] sdb2 [ 1 ] 24418688 blocks [ 2/2 ] [ UU ] unused devices : < none > Then we remove /dev/sdb1 from /dev/md0: mdadm --manage /dev/md0 --remove /dev/sdb1 The output should be like this: mdadm --manage /dev/md0 --remove /dev/sdb1 mdadm: hot removed /dev/sdb1 And cat /proc/mdstat should show this: Personalities : [ linear ] [ multipath ] [ raid0 ] [ raid1 ] [ raid5 ] [ raid4 ] [ raid6 ] [ raid10 ] md0 : active raid1 sda1 [ 0 ] 24418688 blocks [ 2/1 ] [ U_ ] md1 : active raid1 sda2 [ 0 ] sdb2 [ 1 ] 24418688 blocks [ 2/2 ] [ UU ] unused devices : < none > Now we do the same steps again for /dev/sdb2 (which is part of /dev/md1): mdadm --manage /dev/md1 --fail /dev/sdb2 cat / proc / mdstat Personalities : [ linear ] [ multipath ] [ raid0 ] [ raid1 ] [ raid5 ] [ raid4 ] [ raid6 ] [ raid10 ] md0 : active raid1 sda1 [ 0 ] 24418688 blocks [ 2/1 ] [ U_ ] md1 : active raid1 sda2 [ 0 ] sdb2 [ 2 ] ( F ) 24418688 blocks [ 2/1 ] [ U_ ] unused devices : < none > mdadm --manage /dev/md1 --remove /dev/sdb2 mdadm : hot removed / dev / sdb2 cat / proc / mdstat Personalities : [ linear ] [ multipath ] [ raid0 ] [ raid1 ] [ raid5 ] [ raid4 ] [ raid6 ] [ raid10 ] md0 : active raid1 sda1 [ 0 ] 24418688 blocks [ 2/1 ] [ U_ ] md1 : active raid1 sda2 [ 0 ] 24418688 blocks [ 2/1 ] [ U_ ] unused devices : < none > Then power down the system: shutdown -h now and replace the old /dev/sdb hard drive with a new one (it must have at least the same size as the old one - if it\u2019s only a few MB smaller than the old one then rebuilding the arrays will fail).","title":"Removing The Failed Disk"},{"location":"Filesystems/RAID_Rebuild/#adding-the-new-hard-disk","text":"After you have changed the hard disk /dev/sdb, boot the system. The first thing we must do now is to create the exact same partitioning as on /dev/sda. We can do this with one simple command: sfdisk -d /dev/sda | sfdisk /dev/sdb You can run fdisk -l to check if both hard drives have the same partitioning now. Next we add /dev/sdb1 to /dev/md0 and /dev/sdb2 to /dev/md1: mdadm --manage /dev/md0 --add /dev/sdb1 mdadm : re - added / dev / sdb1 mdadm --manage /dev/md1 --add /dev/sdb2 mdadm : re - added / dev / sdb2 Now both arays (/dev/md0 and /dev/md1) will be synchronized. Run cat /proc/mdstat to see when it\u2019s finished. During the synchronization the output will look like this: server1 : ~ # cat / proc / mdstat Personalities : [ linear ] [ multipath ] [ raid0 ] [ raid1 ] [ raid5 ] [ raid4 ] [ raid6 ] [ raid10 ] md0 : active raid1 sda1 [ 0 ] sdb1 [ 1 ] 24418688 blocks [ 2/1 ] [ U_ ] [ =>................... ] recovery = 9.9 % ( 2423168 / 24418688 ) finish = 2.8 min speed = 127535 K / sec md1 : active raid1 sda2 [ 0 ] sdb2 [ 1 ] 24418688 blocks [ 2/1 ] [ U_ ] [ =>................... ] recovery = 6.4 % ( 1572096 / 24418688 ) finish = 1.9 min speed = 196512 K / sec unused devices : < none > When the synchronization is finished, the output will look like this: server1 : ~ # cat / proc / mdstat Personalities : [ linear ] [ multipath ] [ raid0 ] [ raid1 ] [ raid5 ] [ raid4 ] [ raid6 ] [ raid10 ] md0 : active raid1 sda1 [ 0 ] sdb1 [ 1 ] 24418688 blocks [ 2/2 ] [ UU ] md1 : active raid1 sda2 [ 0 ] sdb2 [ 1 ] 24418688 blocks [ 2/2 ] [ UU ] unused devices : < none > That\u2019s it, you have successfully replaced /dev/sdb!","title":"Adding The New Hard Disk"},{"location":"Filesystems/RAID_Resize/","text":"How To Resize RAID Partitions (Shrink & Grow) (Software RAID) Version 1.0 | Author: Falko Timme This article describes how you can shrink and grow existing software RAID partitions. I have tested this with non-LVM RAID1 partitions that use ext3 as the file system. I will describe this procedure for an intact RAID array and also a degraded RAID array. If you use LVM on your RAID partitions, the procedure will be different, so do not use this tutorial in this case! Preliminary Note A few days ago I found out that one of my servers had a degraded RAID1 array (/dev/md2, made up of /dev/sda3 and /dev/sdb3; /dev/sda3 had failed, /dev/sdb3 was still active): server1 : ~ # cat / proc / mdstat Personalities : [ raid1 ] md2 : active raid1 sdb3 [ 1 ] 4594496 blocks [ 2/1 ] [ _U ] md1 : active raid1 sda2 [ 0 ] sdb2 [ 1 ] 497920 blocks [ 2/2 ] [ UU ] md0 : active raid1 sda1 [ 0 ] sdb1 [ 1 ] 144448 blocks [ 2/2 ] [ UU ] unused devices : < none > I tried to fix it (using this tutorial), but unfortunately at the end of the sync process (with 99.9% complete), the sync stopped and started over again. As I found out, this happened because there were some defect sectors at the end of the (working) partition /dev/sdb3 - this was in /var/log/kern.log: Nov 22 18:51:06 server1 kernel: sdb: Current: sense key: Aborted Command Nov 22 18:51:06 server1 kernel: end_request: I/O error, dev sdb, sector 1465142856 So this was the worst case that could happen - /dev/sda dead and /dev/sdb about to die. To fix this, I imagined I could shrink /dev/md2 so that it leaves out the broken sectors at the end of /dev/sdb3, then add the new /dev/sda3 (from the replaced hard drive) to /dev/md2, let the sync finish, remove /dev/sdb3 from the array and replace /dev/sdb with a new hard drive, add the new /dev/sdb3 to /dev/md2, and grow /dev/md2 again. This is one of the use cases for the following procedures (I will describe the process for an intact array and a degraded array). Please note that /dev/md2 is my system partition (mount point /), so I had to use a rescue system (e.g. Knoppix Live-CD) to resize the array. If the array you want to resize is not your system partition, you probably don\u2019t need to boot into a rescue system; but in either case, make sure that the array is unmounted! Intact Array I will describe how to resize the array /dev/md2, made up of /dev/sda3 and /dev/sdb3. Shrinking An Intact Array Boot into your rescue system and activate all needed modules: modprobe md modprobe linear modprobe multipath modprobe raid0 modprobe raid1 modprobe raid5 modprobe raid6 modprobe raid10 Then activate your RAID arrays: cp / etc / mdadm / mdadm . conf / etc / mdadm / mdadm . conf_orig mdadm --examine --scan >> /etc/mdadm/mdadm.conf mdadm - A --scan Run e2fsck -f /dev/md2 to check the file system. /dev/md2 has a size of 40GB; I want to shrink it to 30GB. First we have to shrink the file system with resize2fs; to make sure that the file system fits into the 30GB, we make it a little bit smaller (25GB) so we have a little security margin, shrink /dev/md2 to 30GB, and the resize the file system (again with resize2fs) to the max. possible value: resize2fs /dev/md2 25G Now we shrink /dev/md2 to 30GB. The \u2013size value must be in KiBytes (30 x 1024 x 1024 = 31457280); make sure it can be divided by 64: mdadm --grow /dev/md2 --size=31457280 Next we grow the file system to the largest possible value (if you don\u2019t specify a size, resize2fs will use the largest possible value)\u2026 resize2fs /dev/md2 \u2026 and run a file system check again: e2fsck -f /dev/md2 That\u2019s it - you can now boot into the normal system again. Growing An Intact Array Boot into your rescue system and activate all needed modules: modprobe md modprobe linear modprobe multipath modprobe raid0 modprobe raid1 modprobe raid5 modprobe raid6 modprobe raid10 Then activate your RAID arrays: cp / etc / mdadm / mdadm . conf / etc / mdadm / mdadm . conf_orig mdadm --examine --scan >> /etc/mdadm/mdadm.conf mdadm - A --scan Now we can grow /dev/md2 as follows: mdadm --grow /dev/md2 --size=max \u2013size=max means the largest possible value. You can as well specify a size in KiBytes (see previous chapter). Then we run a file system check\u2026 e2fsck -f /dev/md2 \u2026, resize the file system\u2026 resize2fs /dev/md2 \u2026 and check the file system again: e2fsck -f /dev/md2 Afterwards you can boot back into your normal system. Proc\u00e9dure courte pour resize un RAID1 Booter sur un live CD, style Linux Mint que j\u2019ai utilis\u00e9 installer mdadm : apt-get install mdadm charger les modules n\u00e9cessaires : modprobe {mdio, md, raid0, raid1, raid5, raid6, raid10, linear, mulipath} examiner l\u2019\u00e9tat des RAID : mdadm \u2013examine \u2013scan activer les volumes RAID trouv\u00e9s : mdadm -A \u2013scan monter la partition : mount /dev/mdX /mnt redimensionner le syst\u00e8me de fichiers : btrfs filesystem resize -10G /mnt umount /mnt mettre un disque en faulty : mdadm /dev/md/RAID \u2013fail /dev/sdaX le retirer du RAID : mdadm /dev/md/RAID \u2013remove /dev/sdaX redimensionner la partition un poil plus grand que le fs : parted /dev/sda \u2192 resizepart X \u2192 end \u2192 Yes \u2192 quit v\u00e9rifier le syst\u00e8me de fichier : btrfs check /dev/sdaX \u2192 echo $? pour \u00eatre s^ur qu\u2019il sort bien \u00e0 0 malgr\u00e9 les messages r\u00e9duire le RAID : mdadm \u2013grow /dev/md/RAID \u2013size=xxxx (xxGB * 1024 * 1024) mais environ 500 Mo de moins que les partitions qui le composent ! rajouter le disque au RAID : mdadm /dev/md/RAID \u2013add /dev/sdaX attendre la reconstruction de la grappe : cat /proc/mdstat faire pareil avec l\u2019autre disque (\u00e9tapes 8 \u00e0 14 mais pas la 13 !) agrandir le RAID au max : mdadm \u2013grow /dev/mdXXX \u2013size max reboot","title":"RAID Resize"},{"location":"Filesystems/RAID_Resize/#how-to-resize-raid-partitions-shrink-grow-software-raid","text":"Version 1.0 | Author: Falko Timme This article describes how you can shrink and grow existing software RAID partitions. I have tested this with non-LVM RAID1 partitions that use ext3 as the file system. I will describe this procedure for an intact RAID array and also a degraded RAID array. If you use LVM on your RAID partitions, the procedure will be different, so do not use this tutorial in this case!","title":"How To Resize RAID Partitions (Shrink &amp; Grow) (Software RAID)"},{"location":"Filesystems/RAID_Resize/#preliminary-note","text":"A few days ago I found out that one of my servers had a degraded RAID1 array (/dev/md2, made up of /dev/sda3 and /dev/sdb3; /dev/sda3 had failed, /dev/sdb3 was still active): server1 : ~ # cat / proc / mdstat Personalities : [ raid1 ] md2 : active raid1 sdb3 [ 1 ] 4594496 blocks [ 2/1 ] [ _U ] md1 : active raid1 sda2 [ 0 ] sdb2 [ 1 ] 497920 blocks [ 2/2 ] [ UU ] md0 : active raid1 sda1 [ 0 ] sdb1 [ 1 ] 144448 blocks [ 2/2 ] [ UU ] unused devices : < none > I tried to fix it (using this tutorial), but unfortunately at the end of the sync process (with 99.9% complete), the sync stopped and started over again. As I found out, this happened because there were some defect sectors at the end of the (working) partition /dev/sdb3 - this was in /var/log/kern.log: Nov 22 18:51:06 server1 kernel: sdb: Current: sense key: Aborted Command Nov 22 18:51:06 server1 kernel: end_request: I/O error, dev sdb, sector 1465142856 So this was the worst case that could happen - /dev/sda dead and /dev/sdb about to die. To fix this, I imagined I could shrink /dev/md2 so that it leaves out the broken sectors at the end of /dev/sdb3, then add the new /dev/sda3 (from the replaced hard drive) to /dev/md2, let the sync finish, remove /dev/sdb3 from the array and replace /dev/sdb with a new hard drive, add the new /dev/sdb3 to /dev/md2, and grow /dev/md2 again. This is one of the use cases for the following procedures (I will describe the process for an intact array and a degraded array). Please note that /dev/md2 is my system partition (mount point /), so I had to use a rescue system (e.g. Knoppix Live-CD) to resize the array. If the array you want to resize is not your system partition, you probably don\u2019t need to boot into a rescue system; but in either case, make sure that the array is unmounted!","title":"Preliminary Note"},{"location":"Filesystems/RAID_Resize/#intact-array","text":"I will describe how to resize the array /dev/md2, made up of /dev/sda3 and /dev/sdb3.","title":"Intact Array"},{"location":"Filesystems/RAID_Resize/#shrinking-an-intact-array","text":"Boot into your rescue system and activate all needed modules: modprobe md modprobe linear modprobe multipath modprobe raid0 modprobe raid1 modprobe raid5 modprobe raid6 modprobe raid10 Then activate your RAID arrays: cp / etc / mdadm / mdadm . conf / etc / mdadm / mdadm . conf_orig mdadm --examine --scan >> /etc/mdadm/mdadm.conf mdadm - A --scan Run e2fsck -f /dev/md2 to check the file system. /dev/md2 has a size of 40GB; I want to shrink it to 30GB. First we have to shrink the file system with resize2fs; to make sure that the file system fits into the 30GB, we make it a little bit smaller (25GB) so we have a little security margin, shrink /dev/md2 to 30GB, and the resize the file system (again with resize2fs) to the max. possible value: resize2fs /dev/md2 25G Now we shrink /dev/md2 to 30GB. The \u2013size value must be in KiBytes (30 x 1024 x 1024 = 31457280); make sure it can be divided by 64: mdadm --grow /dev/md2 --size=31457280 Next we grow the file system to the largest possible value (if you don\u2019t specify a size, resize2fs will use the largest possible value)\u2026 resize2fs /dev/md2 \u2026 and run a file system check again: e2fsck -f /dev/md2 That\u2019s it - you can now boot into the normal system again.","title":"Shrinking An Intact Array"},{"location":"Filesystems/RAID_Resize/#growing-an-intact-array","text":"Boot into your rescue system and activate all needed modules: modprobe md modprobe linear modprobe multipath modprobe raid0 modprobe raid1 modprobe raid5 modprobe raid6 modprobe raid10 Then activate your RAID arrays: cp / etc / mdadm / mdadm . conf / etc / mdadm / mdadm . conf_orig mdadm --examine --scan >> /etc/mdadm/mdadm.conf mdadm - A --scan Now we can grow /dev/md2 as follows: mdadm --grow /dev/md2 --size=max \u2013size=max means the largest possible value. You can as well specify a size in KiBytes (see previous chapter). Then we run a file system check\u2026 e2fsck -f /dev/md2 \u2026, resize the file system\u2026 resize2fs /dev/md2 \u2026 and check the file system again: e2fsck -f /dev/md2 Afterwards you can boot back into your normal system.","title":"Growing An Intact Array"},{"location":"Filesystems/RAID_Resize/#procedure-courte-pour-resize-un-raid1","text":"Booter sur un live CD, style Linux Mint que j\u2019ai utilis\u00e9 installer mdadm : apt-get install mdadm charger les modules n\u00e9cessaires : modprobe {mdio, md, raid0, raid1, raid5, raid6, raid10, linear, mulipath} examiner l\u2019\u00e9tat des RAID : mdadm \u2013examine \u2013scan activer les volumes RAID trouv\u00e9s : mdadm -A \u2013scan monter la partition : mount /dev/mdX /mnt redimensionner le syst\u00e8me de fichiers : btrfs filesystem resize -10G /mnt umount /mnt mettre un disque en faulty : mdadm /dev/md/RAID \u2013fail /dev/sdaX le retirer du RAID : mdadm /dev/md/RAID \u2013remove /dev/sdaX redimensionner la partition un poil plus grand que le fs : parted /dev/sda \u2192 resizepart X \u2192 end \u2192 Yes \u2192 quit v\u00e9rifier le syst\u00e8me de fichier : btrfs check /dev/sdaX \u2192 echo $? pour \u00eatre s^ur qu\u2019il sort bien \u00e0 0 malgr\u00e9 les messages r\u00e9duire le RAID : mdadm \u2013grow /dev/md/RAID \u2013size=xxxx (xxGB * 1024 * 1024) mais environ 500 Mo de moins que les partitions qui le composent ! rajouter le disque au RAID : mdadm /dev/md/RAID \u2013add /dev/sdaX attendre la reconstruction de la grappe : cat /proc/mdstat faire pareil avec l\u2019autre disque (\u00e9tapes 8 \u00e0 14 mais pas la 13 !) agrandir le RAID au max : mdadm \u2013grow /dev/mdXXX \u2013size max reboot","title":"Proc\u00e9dure courte pour resize un RAID1"},{"location":"Filesystems/ZFS/","text":"ZFS tips and tricks Create pool with raid zpool create raidz2 <pool name> <disk1> ... <diskN> Create pool with RAID 5 double parity ZFS caches ARC: RAM cache L2ARC: Level 2 ARC , on SSD, no need for redundancy ZIL (ZFS Intent Log) SLOG (Separate intent Log): persistent write cache, redundancy needed Add L2ARC cache to existing pool zpool add <pool name> cache <disk> Add SLOG disk cache to existing pool zpool add <pool name> log mirror <disk ssd1> <disk ssd2> Stats View iostat Since boot zpool iostat Dynamic view with 1sec interval zpool iostat 1 including virtual drives zpool iostat 1 -v Quotas zfs set quota=XXG <pool name> Manage cache file Re-generate the zpool.cache configuration file zpool set cachefile=/etc/zfs/zpool.cache <pool name>","title":"ZFS"},{"location":"Filesystems/ZFS/#zfs-tips-and-tricks","text":"","title":"ZFS tips and tricks"},{"location":"Filesystems/ZFS/#create-pool-with-raid","text":"zpool create raidz2 <pool name> <disk1> ... <diskN> Create pool with RAID 5 double parity","title":"Create pool with raid"},{"location":"Filesystems/ZFS/#zfs-caches","text":"ARC: RAM cache L2ARC: Level 2 ARC , on SSD, no need for redundancy ZIL (ZFS Intent Log) SLOG (Separate intent Log): persistent write cache, redundancy needed","title":"ZFS caches"},{"location":"Filesystems/ZFS/#add-l2arc-cache-to-existing-pool","text":"zpool add <pool name> cache <disk>","title":"Add L2ARC cache to existing pool"},{"location":"Filesystems/ZFS/#add-slog-disk-cache-to-existing-pool","text":"zpool add <pool name> log mirror <disk ssd1> <disk ssd2>","title":"Add SLOG disk cache to existing pool"},{"location":"Filesystems/ZFS/#stats","text":"","title":"Stats"},{"location":"Filesystems/ZFS/#view-iostat","text":"Since boot zpool iostat Dynamic view with 1sec interval zpool iostat 1 including virtual drives zpool iostat 1 -v","title":"View iostat"},{"location":"Filesystems/ZFS/#quotas","text":"zfs set quota=XXG <pool name>","title":"Quotas"},{"location":"Filesystems/ZFS/#manage-cache-file","text":"Re-generate the zpool.cache configuration file zpool set cachefile=/etc/zfs/zpool.cache <pool name>","title":"Manage cache file"},{"location":"Filesystems/sfdisk_usage/","text":"SFDISK Background: Each drive has 1 partition table. A partition table can have a maximum of 4 primary partitions. If the drive is called sdc, the the primary partitions are called sdc1, sdc2, sdc3, sdc4. A partition table can have at most 1 extended partition. The extended partition must also have a name whose numerical part is between 1 and 4: that is, the extended partition must be named sdc1 or sdc2 or sdc3 or sdc4. Logical partitions always have device names whose numerical part is greater than or equal to 5. (e.g. sdc5, sdc6, etc.) The partition table is located at sectors 447\u2013512 on the drive. A sector = 512 bytes. You can save the partition table in its native binary format with the command sudo dd if=/dev/sdc of=PT_sdc.img bs=1 count=66 skip=446 and you can restore the partition table with the command sudo dd of=/dev/sdc if=PT_sdc.img bs=1 count=66 skip=446 I mention this so you can have a picture in your mind about where the partition table is located. We won\u2019t be using dd to manipulate the partition table, however. We\u2019ll use sfdisk instead. The sfdisk commands You can save the partition table in an ascii format with the command sudo sfdisk -d /dev/sdc > PT.txt This saves the partition table on /dev/sdc to a file called PT.txt. What\u2019s particularly lovely is that is file is in ASCII format. You can edit it in a normal text editor, then tell sfdisk to write a new partition table based on our edited PT.txt: sudo sfdisk --no-reread -f /dev/sdc -O PT.save < PT.txt \u201c\u2013no-reread\u201d means don\u2019t check if disk is unmounted -f force \u201c-O PT.save\u201d means save a backup of original partition table in PT.save. PT.save is in binary format. To restore the partition table using PT.save sudo sfdisk --force -I PT.save /dev/sdc Transfer part table sfdisk -d /dev/sda | sfdisk --force /dev/sdb GPT part table apt-get install gdisk clone GPT table from /dev/sda to /dev/sdb sgdisk -R=/dev/sdb /dev/sda make unique its GUID as it was cloned and is identical with /dev/sda sgdisk -G /dev/sdb","title":"Sfdisk usage"},{"location":"Filesystems/sfdisk_usage/#sfdisk","text":"","title":"SFDISK"},{"location":"Filesystems/sfdisk_usage/#background","text":"Each drive has 1 partition table. A partition table can have a maximum of 4 primary partitions. If the drive is called sdc, the the primary partitions are called sdc1, sdc2, sdc3, sdc4. A partition table can have at most 1 extended partition. The extended partition must also have a name whose numerical part is between 1 and 4: that is, the extended partition must be named sdc1 or sdc2 or sdc3 or sdc4. Logical partitions always have device names whose numerical part is greater than or equal to 5. (e.g. sdc5, sdc6, etc.) The partition table is located at sectors 447\u2013512 on the drive. A sector = 512 bytes. You can save the partition table in its native binary format with the command sudo dd if=/dev/sdc of=PT_sdc.img bs=1 count=66 skip=446 and you can restore the partition table with the command sudo dd of=/dev/sdc if=PT_sdc.img bs=1 count=66 skip=446 I mention this so you can have a picture in your mind about where the partition table is located. We won\u2019t be using dd to manipulate the partition table, however. We\u2019ll use sfdisk instead.","title":"Background:"},{"location":"Filesystems/sfdisk_usage/#the-sfdisk-commands","text":"You can save the partition table in an ascii format with the command sudo sfdisk -d /dev/sdc > PT.txt This saves the partition table on /dev/sdc to a file called PT.txt. What\u2019s particularly lovely is that is file is in ASCII format. You can edit it in a normal text editor, then tell sfdisk to write a new partition table based on our edited PT.txt: sudo sfdisk --no-reread -f /dev/sdc -O PT.save < PT.txt \u201c\u2013no-reread\u201d means don\u2019t check if disk is unmounted -f force \u201c-O PT.save\u201d means save a backup of original partition table in PT.save. PT.save is in binary format.","title":"The sfdisk commands"},{"location":"Filesystems/sfdisk_usage/#to-restore-the-partition-table-using-ptsave","text":"sudo sfdisk --force -I PT.save /dev/sdc","title":"To restore the partition table using PT.save"},{"location":"Filesystems/sfdisk_usage/#transfer-part-table","text":"sfdisk -d /dev/sda | sfdisk --force /dev/sdb","title":"Transfer part table"},{"location":"Filesystems/sfdisk_usage/#gpt-part-table","text":"apt-get install gdisk","title":"GPT part table"},{"location":"Filesystems/sfdisk_usage/#clone-gpt-table-from-devsda-to-devsdb","text":"sgdisk -R=/dev/sdb /dev/sda","title":"clone GPT table from /dev/sda to /dev/sdb"},{"location":"Filesystems/sfdisk_usage/#make-unique-its-guid-as-it-was-cloned-and-is-identical-with-devsda","text":"sgdisk -G /dev/sdb","title":"make unique its GUID as it was cloned and is identical with /dev/sda"},{"location":"GitLab/registry_clean/","text":"Registry Management Clean old tags Get registry ID curl -sNH \"Private-Token: TOKEN_WITH_FULL_API_SCOPE\" \"https://gitlab.blabla.net/api/v4/projects/PROJET_ID/registry/repositories\" Remove all but the last 5 tags curl --request DELETE --data 'name_regex_delete=.*' --data 'keep_n=5' --header \"Private-Token: TOKEN_WITH_FULL_API_SCOPE\" \"https://gitlab.blabla.net/api/v4/projects/PROJET_ID/registry/repositories/REPO_ID/tags\"","title":"Registry Management"},{"location":"GitLab/registry_clean/#registry-management","text":"","title":"Registry Management"},{"location":"GitLab/registry_clean/#clean-old-tags","text":"Get registry ID curl -sNH \"Private-Token: TOKEN_WITH_FULL_API_SCOPE\" \"https://gitlab.blabla.net/api/v4/projects/PROJET_ID/registry/repositories\" Remove all but the last 5 tags curl --request DELETE --data 'name_regex_delete=.*' --data 'keep_n=5' --header \"Private-Token: TOKEN_WITH_FULL_API_SCOPE\" \"https://gitlab.blabla.net/api/v4/projects/PROJET_ID/registry/repositories/REPO_ID/tags\"","title":"Clean old tags"},{"location":"LDAP/LDAP_tunning/","text":"Threads G\u00e9n\u00e9ralement fonction du nombre de c\u0153ur r\u00e9el. Contre-intuitivement, un nombre de threads sup\u00e9rieur \u00e0 16 entra\u00eene une baisse de performance des les op\u00e9rations de lecture. En revanche les op\u00e9rations intensives d\u2019\u00e9criture sont plus rapides.","title":"LDAP tunning"},{"location":"LDAP/LDAP_tunning/#threads","text":"G\u00e9n\u00e9ralement fonction du nombre de c\u0153ur r\u00e9el. Contre-intuitivement, un nombre de threads sup\u00e9rieur \u00e0 16 entra\u00eene une baisse de performance des les op\u00e9rations de lecture. En revanche les op\u00e9rations intensives d\u2019\u00e9criture sont plus rapides.","title":"Threads"},{"location":"LDAP/Monitoring_LDAP/","text":"Nombre de connexions en court perl check_ldap_monitor.pl -vvv -H localhost -D cn=monitor,dc=server,dc=tld -P password -T currentconnections -w 250 -c 300 -m greater -f V\u00e9rifier la pr\u00e9sence d\u2019un enregistrement perl check_ldap_dn.pl -H localhost -p 389 -D cn=monitor,dc=server,dc=tld -W password -b cn=Active,cn=Threads,cn=monitor Compter le nombre d\u2019entr\u00e9es retourn\u00e9es perl check_ldap_query.pl -H localhost -p 389 -D cn=monitor,dc=server,dc=tld -P password -b cn=Open,cn=Threads,cn=monitor -w 200 -c 250 -m greater -v Monitorer certaines m\u00e9triques https://www.openldap.org/doc/admin24/monitoringslapd.html Exemple ldapsearch -h localhost -D cn=monitor,dc=server,dc=tld -b cn=Tasklist,cn=Threads,cn=monitor -w password -s sub '(objectClass=\\*)' '*' '+' Reporter seulement la m\u00e9trique souhait\u00e9e: ldapsearch -h localhost -D cn=monitor,dc=server,dc=tld -b cn=Read,cn=Waiters,cn=monitor -w password -LL -s sub '(objectClass=*)' monitorCounter | grep monitorCounter | cut -d\" \" -f2","title":"Monitoring LDAP"},{"location":"LDAP/Monitoring_LDAP/#nombre-de-connexions-en-court","text":"perl check_ldap_monitor.pl -vvv -H localhost -D cn=monitor,dc=server,dc=tld -P password -T currentconnections -w 250 -c 300 -m greater -f","title":"Nombre de connexions en court"},{"location":"LDAP/Monitoring_LDAP/#verifier-la-presence-dun-enregistrement","text":"perl check_ldap_dn.pl -H localhost -p 389 -D cn=monitor,dc=server,dc=tld -W password -b cn=Active,cn=Threads,cn=monitor","title":"V\u00e9rifier la pr\u00e9sence d'un enregistrement"},{"location":"LDAP/Monitoring_LDAP/#compter-le-nombre-dentrees-retournees","text":"perl check_ldap_query.pl -H localhost -p 389 -D cn=monitor,dc=server,dc=tld -P password -b cn=Open,cn=Threads,cn=monitor -w 200 -c 250 -m greater -v","title":"Compter le nombre d'entr\u00e9es retourn\u00e9es"},{"location":"LDAP/Monitoring_LDAP/#monitorer-certaines-metriques","text":"https://www.openldap.org/doc/admin24/monitoringslapd.html Exemple ldapsearch -h localhost -D cn=monitor,dc=server,dc=tld -b cn=Tasklist,cn=Threads,cn=monitor -w password -s sub '(objectClass=\\*)' '*' '+' Reporter seulement la m\u00e9trique souhait\u00e9e: ldapsearch -h localhost -D cn=monitor,dc=server,dc=tld -b cn=Read,cn=Waiters,cn=monitor -w password -LL -s sub '(objectClass=*)' monitorCounter | grep monitorCounter | cut -d\" \" -f2","title":"Monitorer certaines m\u00e9triques"},{"location":"LDAP/Recover_admin_password/","text":"Recover admin password lost Stop service service slapd stop Save ldap base slapcat -n 2 > /root/slapcat.txt Remove ldap base mkdir /tmp/ldap.bak mv /var/lib/ldap/* /tmp/ldap.bak/ Edit file vim slapcat.txt ##=> change admin password Import base slapadd -l /root/slapcat.txt -n 2 Set rights chown -R openldap:openldap /var/lib/ldap/ Restart service service slapd start","title":"Recover admin password lost"},{"location":"LDAP/Recover_admin_password/#recover-admin-password-lost","text":"","title":"Recover admin password lost"},{"location":"LDAP/Recover_admin_password/#stop-service","text":"service slapd stop","title":"Stop service"},{"location":"LDAP/Recover_admin_password/#save-ldap-base","text":"slapcat -n 2 > /root/slapcat.txt","title":"Save ldap base"},{"location":"LDAP/Recover_admin_password/#remove-ldap-base","text":"mkdir /tmp/ldap.bak mv /var/lib/ldap/* /tmp/ldap.bak/","title":"Remove ldap base"},{"location":"LDAP/Recover_admin_password/#edit-file","text":"vim slapcat.txt ##=> change admin password","title":"Edit file"},{"location":"LDAP/Recover_admin_password/#import-base","text":"slapadd -l /root/slapcat.txt -n 2","title":"Import base"},{"location":"LDAP/Recover_admin_password/#set-rights","text":"chown -R openldap:openldap /var/lib/ldap/","title":"Set rights"},{"location":"LDAP/Recover_admin_password/#restart-service","text":"service slapd start","title":"Restart service"},{"location":"LDAP/Reset_slave_state/","text":"R\u00e9initialiser l\u2019\u00e9tat d\u2019un consumer LDAP Cette proc\u00e9dure est utile dans le cas d\u2019un cluster provider-consumer LDAP o\u00f9, pour X raison, le consumer est mal synchronis\u00e9 avec le provider et o\u00f9 on souhaite r\u00e9initialiser compl\u00e8tement la synchronisation. Remettre le consumer \u00e0 z\u00e9ro Pr\u00e9parer un .ldif pour supprimer la synchro vim reset_syncrepl . ldif dn : olcDatabase = { 1 } mdb , cn = config changetype : modify delete : olcSyncrepl olcSyncrepl : { 0 } rid = 001 provider = ldap : // ldap . example . com type = refreshAndPersist retry = \"5 5 300 +\" searchbase = \"dc=example,dc=com\" attrs = \"*,+\" bindmethod = simple binddn = \"uid=user,ou=my_ou,dc=example,dc=com\" credentials = my_password int\u00e9grer le .ldif ldapadd -vv -Y EXTERNAL -H ldapi:/// -f reset_syncrepl.ldif arr\u00eater le service systemctl stop slapd d\u00e9placer la base mkdir save_ldap mv /var/lib/ldap/* save_ldap/ Relancer la synchro pr\u00e9parer le .ldif de configuration de la synchro vim syncrepl . ldif dn : olcDatabase = { 1 } mdb , cn = config changetype : modify add : olcSyncrepl olcSyncrepl : { 0 } rid = 001 provider = ldap : // ldap . example . com type = refreshAndPersist retry = \"5 5 300 +\" searchbase = \"dc=example,dc=com\" attrs = \"*,+\" bindmethod = simple binddn = \"uid=user,ou=my_ou,dc=example,dc=com\" credentials = my_password d\u00e9marrer le service systemctl start slapd int\u00e9grer le .ldif ldapadd -vv -Y EXTERNAL -H ldapi:/// -f syncrepl.ldif une fois fait, arr\u00eater le service \u00e0 nouveau systemctl stop slapd lancer le d\u00e9mon slapd \u00e0 la main en for\u00e7ant la remise \u00e0 z\u00e9ro du cookie de synchro slapd -c rid=001,csn=0 -F /etc/ldap/slapd.d (Note: le rid correspond au num\u00e9ro pass\u00e9 dans le .ldif . Le csn doit \u00eatre 0.) surveiller que la synchro se fait bien en lan\u00e7ant par exemple un tcpdump sur le provider tcpdump host <ip du consumer> and port 389 une fois la re-synchro finie, arr\u00eater le processus slapd en cours et d\u00e9marrer le service killall slapd pgrep slapd # ne doit rien renvoyer systemctl start slapd","title":"Reset slave state"},{"location":"LDAP/Reset_slave_state/#reinitialiser-letat-dun-consumer-ldap","text":"Cette proc\u00e9dure est utile dans le cas d\u2019un cluster provider-consumer LDAP o\u00f9, pour X raison, le consumer est mal synchronis\u00e9 avec le provider et o\u00f9 on souhaite r\u00e9initialiser compl\u00e8tement la synchronisation.","title":"R\u00e9initialiser l'\u00e9tat d'un consumer LDAP"},{"location":"LDAP/Reset_slave_state/#remettre-le-consumer-a-zero","text":"Pr\u00e9parer un .ldif pour supprimer la synchro vim reset_syncrepl . ldif dn : olcDatabase = { 1 } mdb , cn = config changetype : modify delete : olcSyncrepl olcSyncrepl : { 0 } rid = 001 provider = ldap : // ldap . example . com type = refreshAndPersist retry = \"5 5 300 +\" searchbase = \"dc=example,dc=com\" attrs = \"*,+\" bindmethod = simple binddn = \"uid=user,ou=my_ou,dc=example,dc=com\" credentials = my_password int\u00e9grer le .ldif ldapadd -vv -Y EXTERNAL -H ldapi:/// -f reset_syncrepl.ldif arr\u00eater le service systemctl stop slapd d\u00e9placer la base mkdir save_ldap mv /var/lib/ldap/* save_ldap/","title":"Remettre le consumer \u00e0 z\u00e9ro"},{"location":"LDAP/Reset_slave_state/#relancer-la-synchro","text":"pr\u00e9parer le .ldif de configuration de la synchro vim syncrepl . ldif dn : olcDatabase = { 1 } mdb , cn = config changetype : modify add : olcSyncrepl olcSyncrepl : { 0 } rid = 001 provider = ldap : // ldap . example . com type = refreshAndPersist retry = \"5 5 300 +\" searchbase = \"dc=example,dc=com\" attrs = \"*,+\" bindmethod = simple binddn = \"uid=user,ou=my_ou,dc=example,dc=com\" credentials = my_password d\u00e9marrer le service systemctl start slapd int\u00e9grer le .ldif ldapadd -vv -Y EXTERNAL -H ldapi:/// -f syncrepl.ldif une fois fait, arr\u00eater le service \u00e0 nouveau systemctl stop slapd lancer le d\u00e9mon slapd \u00e0 la main en for\u00e7ant la remise \u00e0 z\u00e9ro du cookie de synchro slapd -c rid=001,csn=0 -F /etc/ldap/slapd.d (Note: le rid correspond au num\u00e9ro pass\u00e9 dans le .ldif . Le csn doit \u00eatre 0.) surveiller que la synchro se fait bien en lan\u00e7ant par exemple un tcpdump sur le provider tcpdump host <ip du consumer> and port 389 une fois la re-synchro finie, arr\u00eater le processus slapd en cours et d\u00e9marrer le service killall slapd pgrep slapd # ne doit rien renvoyer systemctl start slapd","title":"Relancer la synchro"},{"location":"Linux/Bluetooth/","text":"Edit the file: /etc/pulse/default.pa and comment out (with an # at the beginning of the line) the following line: load-module module-bluetooth-discover now edit the file: /usr/bin/start-pulseaudio-x11 and after the lines: if [ x\u201d$SESSION_MANAGER\u201d != x ] ; then /usr/bin/pactl load-module module-x11-xsmp \u201cdisplay=$DISPLAY session_manager=$SESSION_MANAGER\u201d > /dev/null fi add the following line: /usr/bin/pactl load-module module-bluetooth-discover Dans le fichier : /etc/bluetooth/main.conf Changer: ControllerMode = dual en : ControllerMode = bredr Si probl\u00e8me avec gDM: cd /var/lib/gdm/.pulse/client.conf autospawn = no daemon-binary = /bin/true","title":"Bluetooth"},{"location":"Linux/Bluetooth/#edit-the-file","text":"/etc/pulse/default.pa","title":"Edit the file:"},{"location":"Linux/Bluetooth/#and-comment-out-with-an-at-the-beginning-of-the-line-the-following-line","text":"load-module module-bluetooth-discover","title":"and comment out (with an # at the beginning of the line) the following line:"},{"location":"Linux/Bluetooth/#now-edit-the-file","text":"/usr/bin/start-pulseaudio-x11","title":"now edit the file:"},{"location":"Linux/Bluetooth/#and-after-the-lines","text":"if [ x\u201d$SESSION_MANAGER\u201d != x ] ; then /usr/bin/pactl load-module module-x11-xsmp \u201cdisplay=$DISPLAY session_manager=$SESSION_MANAGER\u201d > /dev/null fi","title":"and after the lines:"},{"location":"Linux/Bluetooth/#add-the-following-line","text":"/usr/bin/pactl load-module module-bluetooth-discover","title":"add the following line:"},{"location":"Linux/Bluetooth/#dans-le-fichier","text":"/etc/bluetooth/main.conf","title":"Dans le fichier :"},{"location":"Linux/Bluetooth/#changer","text":"ControllerMode = dual","title":"Changer:"},{"location":"Linux/Bluetooth/#en","text":"ControllerMode = bredr","title":"en :"},{"location":"Linux/Bluetooth/#si-probleme-avec-gdm","text":"cd /var/lib/gdm/.pulse/client.conf autospawn = no daemon-binary = /bin/true","title":"Si probl\u00e8me avec gDM:"},{"location":"Linux/Busybox/","text":"D\u00e9marrer avec busybox, dans Grub init=/bin/busybox L\u2019utiliser busybox-static ls # ou dmesg ou autre commande Like it states in \u2013help output: create some symlinks to it: mkdir xxx cd xxx for f in $(/usr/bin/busybox-static --list); do \\ ln -s /usr/bin/busybox-static $f; done ./uname Linux FWIW: one could also build the \u2018coreutils\u2019 package to have an all-in-one program which acts the same: ./configure --help ... --enable-single-binary=shebangs|symlinks Compile all the tools in a single binary, reducing the overall size. When compiled this way, shebangs (default when enabled) or symlinks are installed for each tool that points to the single binary.","title":"Busybox"},{"location":"Linux/Copy_sparse_files/","text":"Copier un fichier sparse sur le r\u00e9seau par Paul Chevalier | 24 Mai 2018 Les fichiers dits \u00ab sparse \u00bb sont allou\u00e9s avec une taille sup\u00e9rieur \u00e0 la taille r\u00e9ellement occup\u00e9e sur le disque dur. Cela permet de n\u2019occuper l\u2019espace disque que si le fichier fait face \u00e0 un accroissement. On les rencontre couramment en virtualisation o\u00f9 l\u2019on parle aussi de \u00ab thin provisioning \u00bb. Le terme \u00ab sparse \u00bb se traduit par \u00ab clairsem\u00e9 \u00bb en fran\u00e7ais et \u00ab thin provisionning \u00bb par \u00ab provisionnement all\u00e9g\u00e9 \u00bb. Avec rsync et l\u2019option -S ou \u2013sparse permet de respecter le caract\u00e8re \u00ab sparse \u00bb du fichier qui ne prendra pas plus de place disque sur la source que sur la cible. Cependant l\u2019utilisation de cette option a un inconv\u00e9nient : la taille d\u2019allocation totale transite par le r\u00e9seau, ce qui est peu efficient. Pour \u00e9viter ce d\u00e9sagr\u00e9ment on peut faire appel \u00e0 une archive tar en mode sparse (-S). Le fichier obtenu peut ainsi \u00eatre transf\u00e9r\u00e9 via n\u2019importe quel protocole pour \u00eatre d\u00e9-tar\u00e9 sur place. tar Scvf image.qcow2.tar image.qcow2 rsync image.qcow2.tar serveur-cible:/chemin/ Une variante consiste \u00e0 utiliser tar en mode flux avec un pipe comme indiqu\u00e9 sur cette page \u00ab How to copy sparse files faster\u00ab . tar cvzSpf \u2013 image.qcow2 | ssh user@serveur-distant \u2018(cd /tmp; tar xzSpf -)\u2019 L\u2019utilisation de tar conjointement avec SSH est une bonne id\u00e9e afin de b\u00e9n\u00e9ficier de l\u2019option sparse, mais aussi pour remplir les trames r\u00e9seau et acc\u00e9l\u00e9rer les \u00e9changes par rapport \u00e0 rsync, en particulier en cas de petits fichiers. Voir diff\u00e9rent exemples ici ou encore celui qui suit. tar -cS /dossier | ssh serveur-distant 'tar -xvf - -C /destination/'","title":"Copier un fichier sparse sur le r\u00e9seau"},{"location":"Linux/Copy_sparse_files/#copier-un-fichier-sparse-sur-le-reseau","text":"par Paul Chevalier | 24 Mai 2018 Les fichiers dits \u00ab sparse \u00bb sont allou\u00e9s avec une taille sup\u00e9rieur \u00e0 la taille r\u00e9ellement occup\u00e9e sur le disque dur. Cela permet de n\u2019occuper l\u2019espace disque que si le fichier fait face \u00e0 un accroissement. On les rencontre couramment en virtualisation o\u00f9 l\u2019on parle aussi de \u00ab thin provisioning \u00bb. Le terme \u00ab sparse \u00bb se traduit par \u00ab clairsem\u00e9 \u00bb en fran\u00e7ais et \u00ab thin provisionning \u00bb par \u00ab provisionnement all\u00e9g\u00e9 \u00bb. Avec rsync et l\u2019option -S ou \u2013sparse permet de respecter le caract\u00e8re \u00ab sparse \u00bb du fichier qui ne prendra pas plus de place disque sur la source que sur la cible. Cependant l\u2019utilisation de cette option a un inconv\u00e9nient : la taille d\u2019allocation totale transite par le r\u00e9seau, ce qui est peu efficient. Pour \u00e9viter ce d\u00e9sagr\u00e9ment on peut faire appel \u00e0 une archive tar en mode sparse (-S). Le fichier obtenu peut ainsi \u00eatre transf\u00e9r\u00e9 via n\u2019importe quel protocole pour \u00eatre d\u00e9-tar\u00e9 sur place. tar Scvf image.qcow2.tar image.qcow2 rsync image.qcow2.tar serveur-cible:/chemin/ Une variante consiste \u00e0 utiliser tar en mode flux avec un pipe comme indiqu\u00e9 sur cette page \u00ab How to copy sparse files faster\u00ab . tar cvzSpf \u2013 image.qcow2 | ssh user@serveur-distant \u2018(cd /tmp; tar xzSpf -)\u2019 L\u2019utilisation de tar conjointement avec SSH est une bonne id\u00e9e afin de b\u00e9n\u00e9ficier de l\u2019option sparse, mais aussi pour remplir les trames r\u00e9seau et acc\u00e9l\u00e9rer les \u00e9changes par rapport \u00e0 rsync, en particulier en cas de petits fichiers. Voir diff\u00e9rent exemples ici ou encore celui qui suit. tar -cS /dossier | ssh serveur-distant 'tar -xvf - -C /destination/'","title":"Copier un fichier sparse sur le r\u00e9seau"},{"location":"Linux/Create_floppy_disk/","text":"\u200bThe necessary commands to perform this task are as follows: dd if =/ dev / zero of = floppy . img bs = 1 k count = 1440 sudo losetup / dev / loop0 floppy . img sudo mkfs - t vfat / dev / loop0 sudo losetup - d / dev / loop0 sudo mkdir / media / floppy sudo mount floppy . img / media / floppy sudo cp * / media / floppy sudo umount / media / floppy sudo rmdir / media / floppy","title":"Create floppy disk"},{"location":"Linux/Create_img_disk/","text":"How to create and \u201cimg\u201d disk The necessary commands to perform this task are as follows: dd if=/dev/zero of=floppy.img bs=1k count=1440 sudo losetup /dev/loop0 floppy.img sudo mkfs -t vfat /dev/loop0 sudo losetup -d /dev/loop0 sudo mkdir /media/floppy sudo mount floppy.img /media/floppy sudo cp * /media/floppy sudo umount /media/floppy sudo rmdir /media/floppy","title":"Create img disk"},{"location":"Linux/Create_img_disk/#how-to-create-and-img-disk","text":"The necessary commands to perform this task are as follows: dd if=/dev/zero of=floppy.img bs=1k count=1440 sudo losetup /dev/loop0 floppy.img sudo mkfs -t vfat /dev/loop0 sudo losetup -d /dev/loop0 sudo mkdir /media/floppy sudo mount floppy.img /media/floppy sudo cp * /media/floppy sudo umount /media/floppy sudo rmdir /media/floppy","title":"How to create and \"img\" disk"},{"location":"Linux/Create_iso_file/","text":"How to create an ISO including custome files xorrisofs -output monIso.iso -volid cidata -joliet -rock fichier1 fichier2 ...","title":"Create iso file"},{"location":"Linux/Create_iso_file/#how-to-create-an-iso-including-custome-files","text":"xorrisofs -output monIso.iso -volid cidata -joliet -rock fichier1 fichier2 ...","title":"How to create an ISO including custome files"},{"location":"Linux/Curl/","text":"# Exemple basique curl -v -s https://linuxfr.org R\u00e9cup fichier curl -ROL https://fichier To tell curl to use a user and password for authentication: curl --user name:password http://www.example.com ## R\u00e9soudre sur un nom diff\u00e9rent curl --resolve www.server.com:443:213.162.53.103 https://www.server.com/ Mail curl -v imap://user:password@in.server.com/ curl \u201cimaps://user:password@in.example.com/\u201d In case you need to use \u201cspecial\u201d chars like @ you have to escape in according to RFC 3986 | Example (password: p@ssword) @ is escaped using %40 curl \u201cimap://username:p%40ssword@in.example.com\u201d Check new email INBOX curl \u201cimap://username:password@in.example.com/INBOX?NEW\u201d curl \u201cimap://username:password@in.example.com/<FOLDER>;UID=<UID_NUMBER>\u201d","title":"Curl"},{"location":"Linux/Curl/#exemple-basique","text":"curl -v -s https://linuxfr.org","title":"# Exemple basique"},{"location":"Linux/Curl/#recup-fichier","text":"curl -ROL https://fichier","title":"R\u00e9cup fichier"},{"location":"Linux/Curl/#to-tell-curl-to-use-a-user-and-password-for-authentication","text":"curl --user name:password http://www.example.com ## R\u00e9soudre sur un nom diff\u00e9rent curl --resolve www.server.com:443:213.162.53.103 https://www.server.com/","title":"To tell curl to use a user and password for authentication:"},{"location":"Linux/Curl/#mail","text":"curl -v imap://user:password@in.server.com/ curl \u201cimaps://user:password@in.example.com/\u201d","title":"Mail"},{"location":"Linux/Curl/#in-case-you-need-to-use-special-chars-like-you-have-to-escape-in-according-to-rfc-3986-example-password-pssword-is-escaped-using-40","text":"curl \u201cimap://username:p%40ssword@in.example.com\u201d","title":"In case you need to use \u201cspecial\u201d chars like @ you have to escape in according to  RFC 3986 | Example (password: p@ssword) @ is escaped using %40"},{"location":"Linux/Curl/#check-new-email-inbox","text":"curl \u201cimap://username:password@in.example.com/INBOX?NEW\u201d curl \u201cimap://username:password@in.example.com/<FOLDER>;UID=<UID_NUMBER>\u201d","title":"Check new email INBOX"},{"location":"Linux/Encrypt_files/","text":"Encrypt cat file | openssl aes-256-cbc -a -salt -out file_encrypted Decrypt cat file_encrypted | openssl aes-256-cbc -a -d -salt -out file_decrypted","title":"Encrypt files"},{"location":"Linux/Encrypt_files/#encrypt","text":"cat file | openssl aes-256-cbc -a -salt -out file_encrypted","title":"Encrypt"},{"location":"Linux/Encrypt_files/#decrypt","text":"cat file_encrypted | openssl aes-256-cbc -a -d -salt -out file_decrypted","title":"Decrypt"},{"location":"Linux/GPG/","text":"GPG Integrity Check - Verifying the File\u2019s Signature If you already have a trusted version of GnuPG installed, you can check the supplied signature. For example, to check the signature of the file gnupg-2.0.30.tar.bz2, you can use this command: gpg --verify gnupg-2.0.30.tar.bz2.sig gnupg-2.0.30.tar.bz2 Note: you should never use a GnuPG version you just downloaded to check the integrity of the source \u2014 use an existing, trusted GnuPG installation, e.g., the one provided by your distribution. If the output of the above command is similar to the following, then either you don\u2019t have our distribution keys (our signing keys are here) or the signature was generated by someone else and the file should be treated suspiciously. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ gpg: Signature made Fri 09 Oct 2015 05:41:55 PM CEST using RSA key ID 4F25E3B6 gpg: Can't check signature: No public key gpg: Signature made Tue 13 Oct 2015 10:18:01 AM CEST using RSA key ID 33BD3F06 gpg: Can't check signature: No public key ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ If you instead see: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ gpg : Good signature from \"Werner Koch (dist sig)\" [ unknown ] gpg : WARNING : This key is not certified with a trusted signature ! gpg : There is no indication that the signature belongs to the owner . Primary key fingerprint : D869 2123 C406 5 DEA 5E0 F 3 AB5 249 B 39 D2 4 F25 E3B6 gpg : Signature made Tue 13 Oct 2015 10 : 18 : 01 AM CEST using RSA key ID 33 BD3F06 gpg : Good signature from \"NIIBE Yutaka (GnuPG Release Key) <gniibe@fsij.org>\" [ unknown ] gpg : WARNING : This key is not certified with a trusted signature ! gpg : There is no indication that the signature belongs to the owner . Primary key fingerprint : 031 E C253 6E58 0 D8E A286 A9F2 2071 B08A 33 BD 3 F06 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ then you have a copy of our keys and the signatures are valid, but either you have not marked the keys as trusted or the keys are a forgery. In this case, at the very least, you should compare the fingerprints that are shown to those on the signing keys page. Even better is to compare the fingerprints with those shown on our business cards, which we handout at events that we attend. Ideally, you\u2019ll see something like: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ gpg : Signature made Fri 09 Oct 2015 05 : 41 : 55 PM CEST using RSA key ID 4 F25E3B6 gpg : Good signature from \"Werner Koch (dist sig)\" [ full ] gpg : Signature made Tue 13 Oct 2015 10 : 18 : 01 AM CEST using RSA key ID 33 BD3F06 gpg : Good signature from \"NIIBE Yutaka (GnuPG Release Key) <gniibe@fsij.org>\" [ full ] ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ This means that the signature is valid and that you trust this key (either you signed it or someone you trusted did). Comparing Checksums If you are not able to use an old version of GnuPG, you can still verify the file\u2019s SHA-1 checksum. This is less secure, because if someone modified the files as they were transferred to you, it would not be much more effort to modify the checksums that you see on this webpage. As such, if you use this method, you should compare the checksums with those in release announcement. This is sent to the gnupg-announce mailing list (among others), which is widely mirrored. Don\u2019t use the mailing list archive on this website, but find the announcement on several other websites and make sure the checksum is consistent. This makes it more difficult for an attacker to trick you into installing a modified version of the software. Assuming you downloaded the file gnupg-2.0.30.tar.bz2, you can run the sha1sum command like this: sha1sum gnupg-2.0.30.tar.bz2 and check that the output matches the SHA-1 checksum reported on this site. An example of a sha1sum output is: a9f024588c356a55e2fd413574bfb55b2e18794a gnupg-2.0.30.tar.bz2 List of SHA-1 check-sums For your convenience, all SHA-1 check-sums available for software that can be downloaded from our site, have been gathered below. 67540161c9fe289153c4a5ea60f7cdce0ef48897 gnupg-2.1.16.tar.bz2 50b0bd286faa90e5c71417b5f2f36cf5de964084 gnupg-w32-2.1.16_20161118.exe a9f024588c356a55e2fd413574bfb55b2e18794a gnupg-2.0.30.tar.bz2 8ab7494e40f80f4138edc9516981bf4afe7d9dbf libgpg-error-1.25.tar.bz2 5a034291e7248592605db448481478e6c963aa9c libgcrypt-1.7.3.tar.bz2 a98385734a0c3f5b713198e8d6e6e4aeb0b76fde libksba-1.3.5.tar.bz2 27391cf4a820b5350ea789c30661830c9a271518 libassuan-2.4.3.tar.bz2 1b21507cfa3f58bdd19ef2f6800ab4cb67729972 npth-1.3.tar.bz2 85d9ac81ebad3fb082514c505c90c39a0456f1f6 pinentry-1.0.0.tar.bz2 efa043064dbf675fd713228c6fcfcc4116feb221 gpgme-1.8.0.tar.bz2 c629348725c1bf5dafd57f8a70187dc89815ce60 gpa-0.9.10.tar.bz2 e708d4aa5ce852f4de3f4b58f4e4f221f5e5c690 dirmngr-1.1.1.tar.bz2 e3bdb585026f752ae91360f45c28e76e4a15d338 gnupg-1.4.21.tar.bz2 8edea5cda7dc9e39d12b24cf12164b28b832918d gnupg-w32cli-1.4.21.exe","title":"GPG"},{"location":"Linux/GPG/#gpg","text":"","title":"GPG"},{"location":"Linux/GPG/#integrity-check-verifying-the-files-signature","text":"If you already have a trusted version of GnuPG installed, you can check the supplied signature. For example, to check the signature of the file gnupg-2.0.30.tar.bz2, you can use this command: gpg --verify gnupg-2.0.30.tar.bz2.sig gnupg-2.0.30.tar.bz2 Note: you should never use a GnuPG version you just downloaded to check the integrity of the source \u2014 use an existing, trusted GnuPG installation, e.g., the one provided by your distribution. If the output of the above command is similar to the following, then either you don\u2019t have our distribution keys (our signing keys are here) or the signature was generated by someone else and the file should be treated suspiciously. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ gpg: Signature made Fri 09 Oct 2015 05:41:55 PM CEST using RSA key ID 4F25E3B6 gpg: Can't check signature: No public key gpg: Signature made Tue 13 Oct 2015 10:18:01 AM CEST using RSA key ID 33BD3F06 gpg: Can't check signature: No public key ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ If you instead see: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ gpg : Good signature from \"Werner Koch (dist sig)\" [ unknown ] gpg : WARNING : This key is not certified with a trusted signature ! gpg : There is no indication that the signature belongs to the owner . Primary key fingerprint : D869 2123 C406 5 DEA 5E0 F 3 AB5 249 B 39 D2 4 F25 E3B6 gpg : Signature made Tue 13 Oct 2015 10 : 18 : 01 AM CEST using RSA key ID 33 BD3F06 gpg : Good signature from \"NIIBE Yutaka (GnuPG Release Key) <gniibe@fsij.org>\" [ unknown ] gpg : WARNING : This key is not certified with a trusted signature ! gpg : There is no indication that the signature belongs to the owner . Primary key fingerprint : 031 E C253 6E58 0 D8E A286 A9F2 2071 B08A 33 BD 3 F06 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ then you have a copy of our keys and the signatures are valid, but either you have not marked the keys as trusted or the keys are a forgery. In this case, at the very least, you should compare the fingerprints that are shown to those on the signing keys page. Even better is to compare the fingerprints with those shown on our business cards, which we handout at events that we attend. Ideally, you\u2019ll see something like: ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ gpg : Signature made Fri 09 Oct 2015 05 : 41 : 55 PM CEST using RSA key ID 4 F25E3B6 gpg : Good signature from \"Werner Koch (dist sig)\" [ full ] gpg : Signature made Tue 13 Oct 2015 10 : 18 : 01 AM CEST using RSA key ID 33 BD3F06 gpg : Good signature from \"NIIBE Yutaka (GnuPG Release Key) <gniibe@fsij.org>\" [ full ] ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ This means that the signature is valid and that you trust this key (either you signed it or someone you trusted did).","title":"Integrity Check - Verifying the File's Signature"},{"location":"Linux/GPG/#comparing-checksums","text":"If you are not able to use an old version of GnuPG, you can still verify the file\u2019s SHA-1 checksum. This is less secure, because if someone modified the files as they were transferred to you, it would not be much more effort to modify the checksums that you see on this webpage. As such, if you use this method, you should compare the checksums with those in release announcement. This is sent to the gnupg-announce mailing list (among others), which is widely mirrored. Don\u2019t use the mailing list archive on this website, but find the announcement on several other websites and make sure the checksum is consistent. This makes it more difficult for an attacker to trick you into installing a modified version of the software. Assuming you downloaded the file gnupg-2.0.30.tar.bz2, you can run the sha1sum command like this: sha1sum gnupg-2.0.30.tar.bz2 and check that the output matches the SHA-1 checksum reported on this site. An example of a sha1sum output is: a9f024588c356a55e2fd413574bfb55b2e18794a gnupg-2.0.30.tar.bz2","title":"Comparing Checksums"},{"location":"Linux/GPG/#list-of-sha-1-check-sums","text":"For your convenience, all SHA-1 check-sums available for software that can be downloaded from our site, have been gathered below. 67540161c9fe289153c4a5ea60f7cdce0ef48897 gnupg-2.1.16.tar.bz2 50b0bd286faa90e5c71417b5f2f36cf5de964084 gnupg-w32-2.1.16_20161118.exe a9f024588c356a55e2fd413574bfb55b2e18794a gnupg-2.0.30.tar.bz2 8ab7494e40f80f4138edc9516981bf4afe7d9dbf libgpg-error-1.25.tar.bz2 5a034291e7248592605db448481478e6c963aa9c libgcrypt-1.7.3.tar.bz2 a98385734a0c3f5b713198e8d6e6e4aeb0b76fde libksba-1.3.5.tar.bz2 27391cf4a820b5350ea789c30661830c9a271518 libassuan-2.4.3.tar.bz2 1b21507cfa3f58bdd19ef2f6800ab4cb67729972 npth-1.3.tar.bz2 85d9ac81ebad3fb082514c505c90c39a0456f1f6 pinentry-1.0.0.tar.bz2 efa043064dbf675fd713228c6fcfcc4116feb221 gpgme-1.8.0.tar.bz2 c629348725c1bf5dafd57f8a70187dc89815ce60 gpa-0.9.10.tar.bz2 e708d4aa5ce852f4de3f4b58f4e4f221f5e5c690 dirmngr-1.1.1.tar.bz2 e3bdb585026f752ae91360f45c28e76e4a15d338 gnupg-1.4.21.tar.bz2 8edea5cda7dc9e39d12b24cf12164b28b832918d gnupg-w32cli-1.4.21.exe","title":"List of SHA-1 check-sums"},{"location":"Linux/Get_file_creation_time/","text":"Step 1: Find inode number of any file using following command on terminal. ls -i /var/log/messages 13377 /var/log/messages Step 2: Find File Creation Time (crtime) debugfs -R 'stat <inode_number>' /dev/sda1","title":"Get file creation time"},{"location":"Linux/Get_file_creation_time/#step-1-find-inode-number-of-any-file-using-following-command-on-terminal","text":"ls -i /var/log/messages 13377 /var/log/messages","title":"Step 1: Find inode number of any file using following command on terminal."},{"location":"Linux/Get_file_creation_time/#step-2-find-file-creation-time-crtime","text":"debugfs -R 'stat <inode_number>' /dev/sda1","title":"Step 2: Find File Creation Time (crtime)"},{"location":"Linux/Get_process_info/","text":"List thread of a process ps -C firefox -L -o pid,tid,pcpu,state,nlwp,cmd See used Resident Memory ps -eF --sort -rss Sort by process using SWAP (echo \"COMM PID SWAP\"; for file in /proc/*/status ; do awk '/^Pid|VmSwap|Name/{printf $2 \" \" $3}END{ print \"\"}' $file; done | grep kB | grep -wv \"0 kB\" | sort -k 3 -n -r) | column -t","title":"Get process info"},{"location":"Linux/Get_process_info/#list-thread-of-a-process","text":"ps -C firefox -L -o pid,tid,pcpu,state,nlwp,cmd","title":"List thread of a process"},{"location":"Linux/Get_process_info/#see-used-resident-memory","text":"ps -eF --sort -rss","title":"See used Resident Memory"},{"location":"Linux/Get_process_info/#sort-by-process-using-swap","text":"(echo \"COMM PID SWAP\"; for file in /proc/*/status ; do awk '/^Pid|VmSwap|Name/{printf $2 \" \" $3}END{ print \"\"}' $file; done | grep kB | grep -wv \"0 kB\" | sort -k 3 -n -r) | column -t","title":"Sort by process using SWAP"},{"location":"Linux/Git/","text":"Rename Git Branch Start by switching to the local branch which you want to rename: git checkout <old_name> Rename the local branch by typing: git branch -m <new_name> At this point, you have renamed the local branch. If you\u2019ve already pushed the branch to the remote repository, perform the next steps to rename the remote branch. Push the local branch and reset the upstream branch: git push origin -u <new_name> Delete the remote branch: git push origin --delete <old_name> Sign Git commits with GPG Declare key to be used git config \u2013global user.signingkey A34RED67G4 now you can add the -S flag when committing git commit -S or you can ask Git to automatically sign all your future commits git config \u2013global commit.gpgsign true To check a commit git verify-commit cce09ca Clear Git history without removing repository cd myrepo rm - rf . git git init git add . git commit - m \"initial commit\" git remote add origin github . com : yourhandle / yourrepo . git git push - u --force origin master","title":"Git"},{"location":"Linux/Git/#rename-git-branch","text":"Start by switching to the local branch which you want to rename: git checkout <old_name> Rename the local branch by typing: git branch -m <new_name> At this point, you have renamed the local branch. If you\u2019ve already pushed the branch to the remote repository, perform the next steps to rename the remote branch. Push the local branch and reset the upstream branch: git push origin -u <new_name> Delete the remote branch: git push origin --delete <old_name>","title":"Rename Git Branch"},{"location":"Linux/Git/#sign-git-commits-with-gpg","text":"Declare key to be used git config \u2013global user.signingkey A34RED67G4 now you can add the -S flag when committing git commit -S or you can ask Git to automatically sign all your future commits git config \u2013global commit.gpgsign true To check a commit git verify-commit cce09ca","title":"Sign Git commits with GPG"},{"location":"Linux/Git/#clear-git-history-without-removing-repository","text":"cd myrepo rm - rf . git git init git add . git commit - m \"initial commit\" git remote add origin github . com : yourhandle / yourrepo . git git push - u --force origin master","title":"Clear Git history without removing repository"},{"location":"Linux/Grep_regex/","text":"Grep Adresse IP my_command | grep -oE \"\\b([0-9]{1,3}\\.){3}[0-9]{1,3}\\b\"","title":"Grep"},{"location":"Linux/Grep_regex/#grep","text":"","title":"Grep"},{"location":"Linux/Grep_regex/#adresse-ip","text":"my_command | grep -oE \"\\b([0-9]{1,3}\\.){3}[0-9]{1,3}\\b\"","title":"Adresse IP"},{"location":"Linux/MPD/","text":"Piper du fifo via netcat (pour MPD) Mettre nc en \u00e9coute sur le port 1234 nc -l 1234 Sur la machine distante : nc 192.168.11.90 1234 < /net/musique/mpd.fifo","title":"MPD"},{"location":"Linux/MPD/#piper-du-fifo-via-netcat-pour-mpd","text":"Mettre nc en \u00e9coute sur le port 1234 nc -l 1234 Sur la machine distante : nc 192.168.11.90 1234 < /net/musique/mpd.fifo","title":"Piper du fifo via netcat (pour MPD)"},{"location":"Linux/Memo/","text":"Memo Suppr all output >/dev/null 2>&1 Rescan disk size echo 1>/sys/class/block/sdd/device/rescan Python: Convertir un r\u00e9sultat pip\u00e9 en json \u2192 yaml python - c 'import sys, yaml, json; yaml.safe_dump(json.load(sys.stdin), sys.stdout, default_flow_style=False)' Generate Tilix bookmarks from SSH config 1 2 3 4 5 6 7 8 9 10 #!/bin/bash # FILE=$HOME/.config/tilix/bookmarks.json echo -e \"{\\n\\t\\\"list\\\": [\\n\" > \" $FILE \" for i in $( cat ~/.ssh/config | grep \"Host\\ \" | awk '{print $2}' | grep -v \"*\" | sort ) do echo -e \"\\t\\t{\\n\\t\\t\\t\\\"command\\\": \\\"\\\",\\n\\t\\t\\t\\\"host\\\": \\\" $i \\\",\\n\\t\\t\\t\\\"name\\\": \\\" $i \\\",\\n\\t\\t\\t\\\"params\\\": \\\"\\\",\\n\\t\\t\\t\\\"port\\\": 22,\\n\\t\\t\\t\\\"protocolType\\\": \\\"SSH\\\",\\n\\t\\t\\t\\\"type\\\": \\\"REMOTE\\\",\\n\\t\\t\\t\\\"user\\\": \\\"root\\\"\\n\\t\\t},\" \\ | sed 's/\\t/\\ \\ \\ \\ /g' done >> \" $FILE \" echo -e \"\\t]\\n}\\n\" >> \" $FILE \"","title":"Memo"},{"location":"Linux/Memo/#memo","text":"","title":"Memo"},{"location":"Linux/Memo/#suppr-all-output","text":">/dev/null 2>&1","title":"Suppr all output"},{"location":"Linux/Memo/#rescan-disk-size","text":"echo 1>/sys/class/block/sdd/device/rescan","title":"Rescan disk size"},{"location":"Linux/Memo/#python-convertir-un-resultat-pipe-en-json-yaml","text":"python - c 'import sys, yaml, json; yaml.safe_dump(json.load(sys.stdin), sys.stdout, default_flow_style=False)'","title":"Python: Convertir un r\u00e9sultat pip\u00e9 en json \u2192 yaml"},{"location":"Linux/Memo/#generate-tilix-bookmarks-from-ssh-config","text":"1 2 3 4 5 6 7 8 9 10 #!/bin/bash # FILE=$HOME/.config/tilix/bookmarks.json echo -e \"{\\n\\t\\\"list\\\": [\\n\" > \" $FILE \" for i in $( cat ~/.ssh/config | grep \"Host\\ \" | awk '{print $2}' | grep -v \"*\" | sort ) do echo -e \"\\t\\t{\\n\\t\\t\\t\\\"command\\\": \\\"\\\",\\n\\t\\t\\t\\\"host\\\": \\\" $i \\\",\\n\\t\\t\\t\\\"name\\\": \\\" $i \\\",\\n\\t\\t\\t\\\"params\\\": \\\"\\\",\\n\\t\\t\\t\\\"port\\\": 22,\\n\\t\\t\\t\\\"protocolType\\\": \\\"SSH\\\",\\n\\t\\t\\t\\\"type\\\": \\\"REMOTE\\\",\\n\\t\\t\\t\\\"user\\\": \\\"root\\\"\\n\\t\\t},\" \\ | sed 's/\\t/\\ \\ \\ \\ /g' done >> \" $FILE \" echo -e \"\\t]\\n}\\n\" >> \" $FILE \"","title":"Generate Tilix bookmarks from SSH config"},{"location":"Linux/RSyslog/","text":"Setup for rsyslog configuration ### Provides TCP syslog reception module ( load = \"imtcp\" maxSessions = \"500\" ) ### Config for secure TLS connection # streamDriver.name=\"gtls\" # streamDriver.mode=\"1\" # streamDriver.authMode=\"x509/name\" # permittedPeer=[\"*.accelance.net\",\"*.domain.tld\"]) ### Define name template for received logs $template RemoteHost , \"/var/log/hosts/%HOSTNAME%/%programname%.log\" ### Define rules to be applied to received logs # here we send them to the dynamic files defined in above template ruleset ( name = \"writeRemoteData\" queue . type = \"fixedArray\" queue . size = \"250000\" queue . dequeueBatchSize = \"4096\" queue . workerThreads = \"4\" queue . workerThreadMinimumMessages = \"60000\" ) { action ( type = \"omfile\" dynafile = \"RemoteHost\" ioBufferSize = \"64k\" flushOnTXEnd = \"off\" asyncWriting = \"on\" ) } ### Define input module input ( type = \"imtcp\" port = \"514\" address = \"10.10.48.48\" ruleset = \"writeRemoteData\" ) pf rules pass quick proto tcp from { 213.162.55.19 } to { 10.10.48.48 } port 514 configure log rotation for collected logs /var/log/hosts/*/*.log { missingok compress create 0400 root root daily dateformat %Y%m%d rotate 90 }","title":"RSyslog"},{"location":"Linux/RSyslog/#setup-for-rsyslog-configuration","text":"### Provides TCP syslog reception module ( load = \"imtcp\" maxSessions = \"500\" ) ### Config for secure TLS connection # streamDriver.name=\"gtls\" # streamDriver.mode=\"1\" # streamDriver.authMode=\"x509/name\" # permittedPeer=[\"*.accelance.net\",\"*.domain.tld\"]) ### Define name template for received logs $template RemoteHost , \"/var/log/hosts/%HOSTNAME%/%programname%.log\" ### Define rules to be applied to received logs # here we send them to the dynamic files defined in above template ruleset ( name = \"writeRemoteData\" queue . type = \"fixedArray\" queue . size = \"250000\" queue . dequeueBatchSize = \"4096\" queue . workerThreads = \"4\" queue . workerThreadMinimumMessages = \"60000\" ) { action ( type = \"omfile\" dynafile = \"RemoteHost\" ioBufferSize = \"64k\" flushOnTXEnd = \"off\" asyncWriting = \"on\" ) } ### Define input module input ( type = \"imtcp\" port = \"514\" address = \"10.10.48.48\" ruleset = \"writeRemoteData\" )","title":"Setup for rsyslog configuration"},{"location":"Linux/RSyslog/#pf-rules","text":"pass quick proto tcp from { 213.162.55.19 } to { 10.10.48.48 } port 514","title":"pf rules"},{"location":"Linux/RSyslog/#configure-log-rotation-for-collected-logs","text":"/var/log/hosts/*/*.log { missingok compress create 0400 root root daily dateformat %Y%m%d rotate 90 }","title":"configure log rotation for collected logs"},{"location":"Linux/Repair_broken_GRUB/","text":"How to fix a broken GRUB D\u00e9marrer sur un live Monter partition racine mount /dev/sda1 /mnt cd /mnt Monter les syst\u00e8mes volatils mount -t proc proc proc/ mount \u2013rbind /sys sys/ mount \u2013rbind /dev dev/ Chroot chroot /mnt M\u00e0J du path export PATH=/bin:/sbin:/usr/sbin:/usr/bin Installer Grub (commande d\u00e9pend de l\u2019OS grub2-install /dev/sda update-grub || grub2-mkconfig -o /boot/grub/grub.cfg update-initramfs -u","title":"Repair broken GRUB"},{"location":"Linux/Repair_broken_GRUB/#how-to-fix-a-broken-grub","text":"D\u00e9marrer sur un live Monter partition racine mount /dev/sda1 /mnt cd /mnt Monter les syst\u00e8mes volatils mount -t proc proc proc/ mount \u2013rbind /sys sys/ mount \u2013rbind /dev dev/ Chroot chroot /mnt M\u00e0J du path export PATH=/bin:/sbin:/usr/sbin:/usr/bin Installer Grub (commande d\u00e9pend de l\u2019OS grub2-install /dev/sda update-grub || grub2-mkconfig -o /boot/grub/grub.cfg update-initramfs -u","title":"How to fix a broken GRUB"},{"location":"Linux/SELinux/","text":"SELINUX tricks Changer le contexte des objets http chcon -R -v --type=httpd_sys_content_t /srv/repo/path/ Rendre ce r\u00e9glage persistent semanage fcontext -a -t httpd_sys_content_t \"/srv/repo/path(/.*)?\" d\u00e9sactier selinux temp setenforce 0 voir status sestatus Preserving SELinux Contexts When Copying Use the cp \u2013preserve=context command to preserve contexts when copying: touch file1 ls -Z file1 -rw-rw-r-- user1 group1 unconfined_u:object_r:user_home_t:s0 file1 ls -dZ /var/www/html/ drwxr-xr-x root root system_u:object_r:httpd_sys_content_t:s0 /var/www/html/ cp --preserve=context file1 /var/www/html/ ls -Z /var/www/html/file1 -rw-r--r-- root root unconfined_u:object_r:user_home_t:s0 /var/www/html/file1","title":"SELinux"},{"location":"Linux/SELinux/#selinux-tricks","text":"","title":"SELINUX tricks"},{"location":"Linux/SELinux/#changer-le-contexte-des-objets-http","text":"chcon -R -v --type=httpd_sys_content_t /srv/repo/path/","title":"Changer le contexte des objets http"},{"location":"Linux/SELinux/#rendre-ce-reglage-persistent","text":"semanage fcontext -a -t httpd_sys_content_t \"/srv/repo/path(/.*)?\"","title":"Rendre ce r\u00e9glage persistent"},{"location":"Linux/SELinux/#desactier-selinux-temp","text":"setenforce 0","title":"d\u00e9sactier selinux temp"},{"location":"Linux/SELinux/#voir-status","text":"sestatus","title":"voir status"},{"location":"Linux/SELinux/#preserving-selinux-contexts-when-copying","text":"Use the cp \u2013preserve=context command to preserve contexts when copying: touch file1 ls -Z file1 -rw-rw-r-- user1 group1 unconfined_u:object_r:user_home_t:s0 file1 ls -dZ /var/www/html/ drwxr-xr-x root root system_u:object_r:httpd_sys_content_t:s0 /var/www/html/ cp --preserve=context file1 /var/www/html/ ls -Z /var/www/html/file1 -rw-r--r-- root root unconfined_u:object_r:user_home_t:s0 /var/www/html/file1","title":"Preserving SELinux Contexts When Copying"},{"location":"Linux/SSH/","text":"Start graphical app via SSH You have to set DISPLAY and XAUTHORITY properly, e.g.: ssh host on host: export DISPLAY=:0.0 export XAUTHORITY=$HOME/.local/share/sddm/.Xauthority start_graphical_application Pipe between 2 servers ssh server1 'cat /root/file' | ssh server2 'cat > destfile' Reverse SSH Connexion directe Cr\u00e9ation d\u2019un utilisateur d\u00e9di\u00e9 sur le poste local Cr\u00e9ez un nouvel utilisateur sp\u00e9cialement pour cette connexion afin que l\u2019utilisateur userD du poste distant ne puisse pas avoir un acc\u00e8s complet au poste local. Ce nouvel utilisateur cr\u00e9\u00e9 pourra cependant avoir des droits personnalis\u00e9s. Saisissez dans un terminal sur local la commande suivante : sudo adduser --no-create-home userL o\u00f9 : \u2013no-create-home est l\u2019option sp\u00e9cifi\u00e9e pour ne pas cr\u00e9er de dossier /home/userL sur le poste local. userL est \u00e0 remplacer par le nom de votre choix mais suffisamment explicite pour savoir sur quelle machine vous \u00eates. Le mot de passe cr\u00e9\u00e9 servira pour se connecter lors de l\u2019\u00e9tape suivante. Connexion au poste local depuis le poste distant Initiez une connexion \u00e0 local en saisissant sur le poste distant: ssh -NR 12345:localhost:22 userL@local o\u00f9 12345 est \u00e0 remplacer par un num\u00e9ro de port al\u00e9atoire (entre 1024 et 65535 qui sont r\u00e9serv\u00e9s pour des applications utilisateurs) et non utilis\u00e9 de votre choix userL et le mot de passe de connexion sont ceux d\u00e9fini pr\u00e9c\u00e9demment. local est l\u2019adresse IP publique de la machine locale (au besoin avec une r\u00e8gle NAT dans la box locale pour \u00eatre joignable de l\u2019ext\u00e9rieur) Connexion au poste distant depuis le poste local La connexion \u00e9tant d\u00e9sormais activ\u00e9e depuis distant vers local, le pare-feu va donc laisser rentrer la connexion reverse, \u00e0 savoir depuis local vers distant. Pour cela taper dans un terminal sur local: ssh -p 12345 userD@localhost o\u00f9 12345 est le port choisi auparavant userD est \u00e0 remplacer par le nom d\u2019utilisateur permettant de se connecter au serveur ssh sur distant Cette configuration est pratique quand le poste local est lui-m\u00eame derri\u00e8re un pare-feu et/ou ne dispose pas d\u2019un serveur ssh. Prenez l\u2019exemple de configuration suivant: Connexion par serveur tiers userD@distant et userL@local ne sont pas accessibles depuis l\u2019ext\u00e9rieur Ici userD@distant correspond \u00e0 l\u2019utilisateur userD, sur le poste appel\u00e9 distant qui a les ports entrants bloqu\u00e9s et donc inaccessible depuis l\u2019ext\u00e9rieur userS@serveur correspond \u00e0 l\u2019utilisateur userS, sur le poste appel\u00e9 serveur qui dispose d\u2019un acc\u00e8s libre \u00e0 son serveur ssh. userL@local correspond \u00e0 l\u2019utilisateur userL, sur le poste appel\u00e9 local qui va acc\u00e9der \u00e0 la machine serveur pour atteindre distant Pour r\u00e9sum\u00e9 le principe, il s\u2019agira de: connecter distant sur serveur connecter local sur serveur depuis le terminal qui a initi\u00e9 la connexion local sur serveur pour atteindre distant Cr\u00e9ation d\u2019un utilisateur d\u00e9di\u00e9 sur le poste serveur Cette partie est facultative si la machine serveur dispose d\u00e9j\u00e0 d\u2019un utilisateur public Taper dans un terminal : sudo adduser --no-create-home userS Connexion sur le poste serveur depuis le poste distant Initiez une connexion sur serveur en tapant dans un terminal de la machine distant : ssh -R 12345:localhost:22 userS@serveur o\u00f9 12345 est \u00e0 remplacer par un num\u00e9ro de port al\u00e9atoire de votre choix, le port 22 est le port d\u2019\u00e9coute ssh sur la machine distant, userS et le mot de passe de connexion sont ceux d\u00e9fini pr\u00e9c\u00e9demment serveur est l\u2019adresse ip ou le nom de domaine du serveur tiers Note L\u2019option -N peut \u00e9galement \u00eatre ajout\u00e9e pour ne pas faire apparaitre d\u2019invite de terminal sur distant Connexion sur le poste serveur depuis le poste local Cr\u00e9er un pont entre serveur et local en tapant dans un terminal de ce dernier ssh userS@serveur Acc\u00e8s \u00e0 la machine distante depuis la machine locale Vous pouvez d\u00e9sormais atteindre le poste distant en saisissant dans le terminal du poste local connect\u00e9 pr\u00e9c\u00e9demment sur serveur ssh -p 12345 userD@localhost","title":"SSH"},{"location":"Linux/SSH/#start-graphical-app-via-ssh","text":"You have to set DISPLAY and XAUTHORITY properly, e.g.: ssh host on host: export DISPLAY=:0.0 export XAUTHORITY=$HOME/.local/share/sddm/.Xauthority start_graphical_application","title":"Start graphical app via SSH"},{"location":"Linux/SSH/#pipe-between-2-servers","text":"ssh server1 'cat /root/file' | ssh server2 'cat > destfile'","title":"Pipe between 2 servers"},{"location":"Linux/SSH/#reverse-ssh","text":"","title":"Reverse SSH"},{"location":"Linux/SSH/#connexion-directe","text":"","title":"Connexion directe"},{"location":"Linux/SSH/#creation-dun-utilisateur-dedie-sur-le-poste-local","text":"Cr\u00e9ez un nouvel utilisateur sp\u00e9cialement pour cette connexion afin que l\u2019utilisateur userD du poste distant ne puisse pas avoir un acc\u00e8s complet au poste local. Ce nouvel utilisateur cr\u00e9\u00e9 pourra cependant avoir des droits personnalis\u00e9s. Saisissez dans un terminal sur local la commande suivante : sudo adduser --no-create-home userL o\u00f9 : \u2013no-create-home est l\u2019option sp\u00e9cifi\u00e9e pour ne pas cr\u00e9er de dossier /home/userL sur le poste local. userL est \u00e0 remplacer par le nom de votre choix mais suffisamment explicite pour savoir sur quelle machine vous \u00eates. Le mot de passe cr\u00e9\u00e9 servira pour se connecter lors de l\u2019\u00e9tape suivante.","title":"Cr\u00e9ation d'un utilisateur d\u00e9di\u00e9 sur le poste local"},{"location":"Linux/SSH/#connexion-au-poste-local-depuis-le-poste-distant","text":"Initiez une connexion \u00e0 local en saisissant sur le poste distant: ssh -NR 12345:localhost:22 userL@local o\u00f9 12345 est \u00e0 remplacer par un num\u00e9ro de port al\u00e9atoire (entre 1024 et 65535 qui sont r\u00e9serv\u00e9s pour des applications utilisateurs) et non utilis\u00e9 de votre choix userL et le mot de passe de connexion sont ceux d\u00e9fini pr\u00e9c\u00e9demment. local est l\u2019adresse IP publique de la machine locale (au besoin avec une r\u00e8gle NAT dans la box locale pour \u00eatre joignable de l\u2019ext\u00e9rieur)","title":"Connexion au poste local depuis le poste distant"},{"location":"Linux/SSH/#connexion-au-poste-distant-depuis-le-poste-local","text":"La connexion \u00e9tant d\u00e9sormais activ\u00e9e depuis distant vers local, le pare-feu va donc laisser rentrer la connexion reverse, \u00e0 savoir depuis local vers distant. Pour cela taper dans un terminal sur local: ssh -p 12345 userD@localhost o\u00f9 12345 est le port choisi auparavant userD est \u00e0 remplacer par le nom d\u2019utilisateur permettant de se connecter au serveur ssh sur distant Cette configuration est pratique quand le poste local est lui-m\u00eame derri\u00e8re un pare-feu et/ou ne dispose pas d\u2019un serveur ssh. Prenez l\u2019exemple de configuration suivant:","title":"Connexion au poste distant depuis le poste local"},{"location":"Linux/SSH/#connexion-par-serveur-tiers","text":"userD@distant et userL@local ne sont pas accessibles depuis l\u2019ext\u00e9rieur Ici userD@distant correspond \u00e0 l\u2019utilisateur userD, sur le poste appel\u00e9 distant qui a les ports entrants bloqu\u00e9s et donc inaccessible depuis l\u2019ext\u00e9rieur userS@serveur correspond \u00e0 l\u2019utilisateur userS, sur le poste appel\u00e9 serveur qui dispose d\u2019un acc\u00e8s libre \u00e0 son serveur ssh. userL@local correspond \u00e0 l\u2019utilisateur userL, sur le poste appel\u00e9 local qui va acc\u00e9der \u00e0 la machine serveur pour atteindre distant Pour r\u00e9sum\u00e9 le principe, il s\u2019agira de: connecter distant sur serveur connecter local sur serveur depuis le terminal qui a initi\u00e9 la connexion local sur serveur pour atteindre distant","title":"Connexion par serveur tiers"},{"location":"Linux/SSH/#creation-dun-utilisateur-dedie-sur-le-poste-serveur","text":"Cette partie est facultative si la machine serveur dispose d\u00e9j\u00e0 d\u2019un utilisateur public Taper dans un terminal : sudo adduser --no-create-home userS","title":"Cr\u00e9ation d'un utilisateur d\u00e9di\u00e9 sur le poste serveur"},{"location":"Linux/SSH/#connexion-sur-le-poste-serveur-depuis-le-poste-distant","text":"Initiez une connexion sur serveur en tapant dans un terminal de la machine distant : ssh -R 12345:localhost:22 userS@serveur o\u00f9 12345 est \u00e0 remplacer par un num\u00e9ro de port al\u00e9atoire de votre choix, le port 22 est le port d\u2019\u00e9coute ssh sur la machine distant, userS et le mot de passe de connexion sont ceux d\u00e9fini pr\u00e9c\u00e9demment serveur est l\u2019adresse ip ou le nom de domaine du serveur tiers Note L\u2019option -N peut \u00e9galement \u00eatre ajout\u00e9e pour ne pas faire apparaitre d\u2019invite de terminal sur distant","title":"Connexion sur le poste serveur depuis le poste distant"},{"location":"Linux/SSH/#connexion-sur-le-poste-serveur-depuis-le-poste-local","text":"Cr\u00e9er un pont entre serveur et local en tapant dans un terminal de ce dernier ssh userS@serveur","title":"Connexion sur le poste serveur depuis le poste local"},{"location":"Linux/SSH/#acces-a-la-machine-distante-depuis-la-machine-locale","text":"Vous pouvez d\u00e9sormais atteindre le poste distant en saisissant dans le terminal du poste local connect\u00e9 pr\u00e9c\u00e9demment sur serveur ssh -p 12345 userD@localhost","title":"Acc\u00e8s \u00e0 la machine distante depuis la machine locale"},{"location":"Linux/Sed/","text":"Sed tricks To remove the line and print the output to standard out: sed '/pattern to match/d' ./infile To directly modify the file: sed -i '/pattern to match/d' ./infile To directly modify the file (and create a backup): sed -i.bak '/pattern to match/d' ./infile Delete N lines in a file As long as the file is not a symlink or hardlink, you can use sed, tail, or awk. Example below. $ cat t.txt 12 34 56 78 90 $ sed -e '1,3d' < t.txt 78 90 Delete empty lines sed '/^\\s*$/d' sed + remove # and empty lines with one sed command sed -e 's/#.*$//' -e '/^$/d' inputFile Remove trailing witespaces sed -i 's/[ \\t]*$//' file Replace between strings sed -n -e '/Word A/,/Word D/ p' file","title":"Sed"},{"location":"Linux/Sed/#sed-tricks","text":"","title":"Sed tricks"},{"location":"Linux/Sed/#to-remove-the-line-and-print-the-output-to-standard-out","text":"sed '/pattern to match/d' ./infile","title":"To remove the line and print the output to standard out:"},{"location":"Linux/Sed/#to-directly-modify-the-file","text":"sed -i '/pattern to match/d' ./infile","title":"To directly modify the file:"},{"location":"Linux/Sed/#to-directly-modify-the-file-and-create-a-backup","text":"sed -i.bak '/pattern to match/d' ./infile","title":"To directly modify the file (and create a backup):"},{"location":"Linux/Sed/#delete-n-lines-in-a-file","text":"As long as the file is not a symlink or hardlink, you can use sed, tail, or awk. Example below. $ cat t.txt 12 34 56 78 90 $ sed -e '1,3d' < t.txt 78 90","title":"Delete N lines in a file"},{"location":"Linux/Sed/#delete-empty-lines","text":"sed '/^\\s*$/d'","title":"Delete empty lines"},{"location":"Linux/Sed/#sed-remove-and-empty-lines-with-one-sed-command","text":"sed -e 's/#.*$//' -e '/^$/d' inputFile","title":"sed + remove # and empty lines with one sed command"},{"location":"Linux/Sed/#remove-trailing-witespaces","text":"sed -i 's/[ \\t]*$//' file","title":"Remove trailing witespaces"},{"location":"Linux/Sed/#replace-between-strings","text":"sed -n -e '/Word A/,/Word D/ p' file","title":"Replace between strings"},{"location":"Linux/Set_Kernel_in_Grub/","text":"Set Kernel version in Grub 1) Find the $menuentry_id_option for the submenu: grep submenu / boot / grub / grub . cfg submenu 'Advanced options for Debian GNU/Linux' $ menuentry_id_option 'gnulinux-advanced-38ea4a12-6cfe-4ed9-a8b5-036295e62ffc' { 1) Find the $menuentry_id_option for the menu entry for the kernel you want to use: grep gnulinux / boot / grub / grub . cfg menuentry 'Debian GNU/Linux' --class debian --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-simple-38ea4a12-6cfe-4ed9-a8b5-036295e62ffc' { submenu 'Advanced options for Debian GNU/Linux' $ menuentry_id_option 'gnulinux-advanced-38ea4a12-6cfe-4ed9-a8b5-036295e62ffc' { menuentry 'Debian GNU/Linux, with Linux 4.18.0-0.bpo.1-rt-amd64' --class debian --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-4.18.0-0.bpo.1-rt-amd64-advanced-38ea4a12-6cfe-4ed9-a8b5-036295e62ffc' { menuentry 'Debian GNU/Linux, with Linux 4.18.0-0.bpo.1-rt-amd64 (recovery mode)' --class debian --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-4.18.0-0.bpo.1-rt-amd64-recovery-38ea4a12-6cfe-4ed9-a8b5-036295e62ffc' { menuentry 'Debian GNU/Linux, with Linux 4.18.0-0.bpo.1-amd64' --class debian --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-4.18.0-0.bpo.1-amd64-advanced-38ea4a12-6cfe-4ed9-a8b5-036295e62ffc' { menuentry 'Debian GNU/Linux, with Linux 4.18.0-0.bpo.1-amd64 (recovery mode)' --class debian --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-4.18.0-0.bpo.1-amd64-recovery-38ea4a12-6cfe-4ed9-a8b5-036295e62ffc' { menuentry 'Debian GNU/Linux, with Linux 4.17.0-0.bpo.1-amd64' --class debian --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-4.17.0-0.bpo.1-amd64-advanced-38ea4a12-6cfe-4ed9-a8b5-036295e62ffc' { menuentry 'Debian GNU/Linux, with Linux 4.17.0-0.bpo.1-amd64 (recovery mode)' --class debian --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-4.17.0-0.bpo.1-amd64-recovery-38ea4a12-6cfe-4ed9-a8b5-036295e62ffc' { menuentry 'Debian GNU/Linux, with Linux 4.9.0-8-amd64' --class debian --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-4.9.0-8-amd64-advanced-38ea4a12-6cfe-4ed9-a8b5-036295e62ffc' { menuentry 'Debian GNU/Linux, with Linux 4.9.0-8-amd64 (recovery mode)' --class debian --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-4.9.0-8-amd64-recovery-38ea4a12-6cfe-4ed9-a8b5-036295e62ffc' { 1) Comment out your current default grub in /etc/default/grub and replace it with the sub-menu\u2019s $menuentry_id_option from step one, and the selected kernel\u2019s $menuentry_id_option from step two separated by \u2018>\u2019. In my case the modified GRUB_DEFAULT is: # GRUB_DEFAULT = 0 GRUB_DEFAULT = \"gnulinux-advanced-38ea4a12-6cfe-4ed9-a8b5-036295e62ffc>gnulinux-4.18.0-0.bpo.1-amd64-advanced-38ea4a12-6cfe-4ed9-a8b5-036295e62ffc\" 1) Update grub to make the changes. For Debian this is done like so: update-grub Done. Now when you boot, the advanced menu should have an asterisk and you should boot into the selected kernel. You can confirm this with uname. uname -a Linux NAME 4.18.0-0.bpo.1-amd64 #1 SMP Debian 4.18.0-0 (2018-09-13) x86_64 GNU/Linux Changing this back to the most recent kernel is as simple as commenting out the new line and uncommenting #GRUB_DEFAULT=0 GRUB_DEFAULT = 0 # GRUB_DEFAULT = \"gnulinux-advanced-38ea4a12-6cfe-4ed9-a8b5-036295e62ffc>gnulinux-4.18.0-0.bpo.1-amd64-advanced-38ea4a12-6cfe-4ed9-a8b5-036295e62ffc\" then rerunning update-grub","title":"Set Kernel version in Grub"},{"location":"Linux/Set_Kernel_in_Grub/#set-kernel-version-in-grub","text":"1) Find the $menuentry_id_option for the submenu: grep submenu / boot / grub / grub . cfg submenu 'Advanced options for Debian GNU/Linux' $ menuentry_id_option 'gnulinux-advanced-38ea4a12-6cfe-4ed9-a8b5-036295e62ffc' { 1) Find the $menuentry_id_option for the menu entry for the kernel you want to use: grep gnulinux / boot / grub / grub . cfg menuentry 'Debian GNU/Linux' --class debian --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-simple-38ea4a12-6cfe-4ed9-a8b5-036295e62ffc' { submenu 'Advanced options for Debian GNU/Linux' $ menuentry_id_option 'gnulinux-advanced-38ea4a12-6cfe-4ed9-a8b5-036295e62ffc' { menuentry 'Debian GNU/Linux, with Linux 4.18.0-0.bpo.1-rt-amd64' --class debian --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-4.18.0-0.bpo.1-rt-amd64-advanced-38ea4a12-6cfe-4ed9-a8b5-036295e62ffc' { menuentry 'Debian GNU/Linux, with Linux 4.18.0-0.bpo.1-rt-amd64 (recovery mode)' --class debian --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-4.18.0-0.bpo.1-rt-amd64-recovery-38ea4a12-6cfe-4ed9-a8b5-036295e62ffc' { menuentry 'Debian GNU/Linux, with Linux 4.18.0-0.bpo.1-amd64' --class debian --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-4.18.0-0.bpo.1-amd64-advanced-38ea4a12-6cfe-4ed9-a8b5-036295e62ffc' { menuentry 'Debian GNU/Linux, with Linux 4.18.0-0.bpo.1-amd64 (recovery mode)' --class debian --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-4.18.0-0.bpo.1-amd64-recovery-38ea4a12-6cfe-4ed9-a8b5-036295e62ffc' { menuentry 'Debian GNU/Linux, with Linux 4.17.0-0.bpo.1-amd64' --class debian --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-4.17.0-0.bpo.1-amd64-advanced-38ea4a12-6cfe-4ed9-a8b5-036295e62ffc' { menuentry 'Debian GNU/Linux, with Linux 4.17.0-0.bpo.1-amd64 (recovery mode)' --class debian --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-4.17.0-0.bpo.1-amd64-recovery-38ea4a12-6cfe-4ed9-a8b5-036295e62ffc' { menuentry 'Debian GNU/Linux, with Linux 4.9.0-8-amd64' --class debian --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-4.9.0-8-amd64-advanced-38ea4a12-6cfe-4ed9-a8b5-036295e62ffc' { menuentry 'Debian GNU/Linux, with Linux 4.9.0-8-amd64 (recovery mode)' --class debian --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-4.9.0-8-amd64-recovery-38ea4a12-6cfe-4ed9-a8b5-036295e62ffc' { 1) Comment out your current default grub in /etc/default/grub and replace it with the sub-menu\u2019s $menuentry_id_option from step one, and the selected kernel\u2019s $menuentry_id_option from step two separated by \u2018>\u2019. In my case the modified GRUB_DEFAULT is: # GRUB_DEFAULT = 0 GRUB_DEFAULT = \"gnulinux-advanced-38ea4a12-6cfe-4ed9-a8b5-036295e62ffc>gnulinux-4.18.0-0.bpo.1-amd64-advanced-38ea4a12-6cfe-4ed9-a8b5-036295e62ffc\" 1) Update grub to make the changes. For Debian this is done like so: update-grub Done. Now when you boot, the advanced menu should have an asterisk and you should boot into the selected kernel. You can confirm this with uname. uname -a Linux NAME 4.18.0-0.bpo.1-amd64 #1 SMP Debian 4.18.0-0 (2018-09-13) x86_64 GNU/Linux Changing this back to the most recent kernel is as simple as commenting out the new line and uncommenting #GRUB_DEFAULT=0 GRUB_DEFAULT = 0 # GRUB_DEFAULT = \"gnulinux-advanced-38ea4a12-6cfe-4ed9-a8b5-036295e62ffc>gnulinux-4.18.0-0.bpo.1-amd64-advanced-38ea4a12-6cfe-4ed9-a8b5-036295e62ffc\" then rerunning update-grub","title":"Set Kernel version in Grub"},{"location":"Linux/Tmux/","text":"Tmux Autostart Tmux with systemd \\# / etc / systemd / system / tmux @ . service [ Unit ] Description = Start tmux in detached session [ Service ] Type = forking User =% I WorkingDirectory =/ home /% I ExecStart =/ usr / bin / tmux new - session - s % u - d ExecStop =/ usr / bin / tmux kill - session - t % u [ Install ] WantedBy = multi - user . target","title":"Tmux"},{"location":"Linux/Tmux/#tmux","text":"","title":"Tmux"},{"location":"Linux/Tmux/#autostart-tmux-with-systemd","text":"\\# / etc / systemd / system / tmux @ . service [ Unit ] Description = Start tmux in detached session [ Service ] Type = forking User =% I WorkingDirectory =/ home /% I ExecStart =/ usr / bin / tmux new - session - s % u - d ExecStop =/ usr / bin / tmux kill - session - t % u [ Install ] WantedBy = multi - user . target","title":"Autostart Tmux with systemd"},{"location":"Linux/Vim/","text":"Vim Remplacer un caract\u00e8re par un retour \u00e0 la ligne : Utiliser \u201c \\r \u201c au lieu de \u201d\\n\u201d. Running a macro ( http://vim.wikia.com/wiki/Macros ) Use this mapping as a convenient way to play a macro recorded to register q: :nnoremap <Space> @q \u2022 Start recording keystrokes by typing qq. \u2022 End recording with q (first press Escape if you are in insert mode). \u2022 Play the recorded keystrokes by hitting space. Suppose you have a macro which operates on the text in a single line. You can run the macro on each line in a visual selection in a single operation: \u2022 Visually select some lines (for example, type vip to select the current paragraph). \u2022 Type :normal @q to run the macro from register q on each line. Vim has a very powerful built-in sort utility, or it can interface with an external one. In order to keep only unique lines in Vim, you would: :{range}sort u Remove all trailing spaces :%s/\\s+$//e Vimdiff local and remote files via ssh vimdiff /path/to/file scp://remotehost//path/to/file","title":"Vim"},{"location":"Linux/Vim/#vim","text":"Remplacer un caract\u00e8re par un retour \u00e0 la ligne : Utiliser \u201c \\r \u201c au lieu de \u201d\\n\u201d. Running a macro ( http://vim.wikia.com/wiki/Macros ) Use this mapping as a convenient way to play a macro recorded to register q: :nnoremap <Space> @q \u2022 Start recording keystrokes by typing qq. \u2022 End recording with q (first press Escape if you are in insert mode). \u2022 Play the recorded keystrokes by hitting space. Suppose you have a macro which operates on the text in a single line. You can run the macro on each line in a visual selection in a single operation: \u2022 Visually select some lines (for example, type vip to select the current paragraph). \u2022 Type :normal @q to run the macro from register q on each line. Vim has a very powerful built-in sort utility, or it can interface with an external one. In order to keep only unique lines in Vim, you would: :{range}sort u Remove all trailing spaces :%s/\\s+$//e Vimdiff local and remote files via ssh vimdiff /path/to/file scp://remotehost//path/to/file","title":"Vim"},{"location":"Linux/openSSL/","text":"Check SSL state openssl s_client -connect HOTE:443 -CApath /etc/ssl/certs Generate a self signed certificate 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 #!/bin/sh _HOST = $1 openssl genrsa -out $_HOST .key.pem 2048 openssl req -x509 -new -nodes -days 1460 -sha256 -key $_HOST .key.pem -out $_HOST .cert.pem openssl genrsa -out $_HOST .key 2048 openssl req -new -sha256 -key $_HOST .key -out $_HOST .csr openssl x509 -req -days 1460 -sha256 -in $_HOST .csr -CA $_HOST .cert.pem -CAkey $_HOST .key.pem -CAcreateserial -out $_HOST .crt # Optional #openssl dhparam -out /etc/ssl/$_HOST.dhparams.pem 2048 chmod 444 $_HOST .cert.pem chmod 444 $_HOST .crt chmod 400 $_HOST .key.pem chmod 400 $_HOST .key # Optionel #mv $_HOST.key.pem private/ #mv $_HOST.cert.pem certs/ #mv $_HOST.key private/ #mv $_HOST.crt certs/ Decode certif p12 openssl pkcs12 -in keyStore.pfx -out keyStore.pem -nodes Libvirt cert creation script 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 #!/bin/bash SERVER_KEY = server-key.pem # creating a key for our ca if [ ! -e ca-key.pem ] ; then openssl genrsa -des3 -out ca-key.pem 1024 fi # creating a ca if [ ! -e ca-cert.pem ] ; then openssl req -new -x509 -days 1095 -key ca-key.pem -out ca-cert.pem -subj \"/C=IL/L=Raanana/O=Red Hat/CN=my CA\" fi # create server key if [ ! -e $SERVER_KEY ] ; then openssl genrsa -out $SERVER_KEY 1024 fi # create a certificate signing request (csr) if [ ! -e server-key.csr ] ; then openssl req -new -key $SERVER_KEY -out server-key.csr -subj \"/C=IL/L=Raanana/O=Red Hat/CN=my server\" fi # signing our server certificate with this ca if [ ! -e server-cert.pem ] ; then openssl x509 -req -days 1095 -in server-key.csr -CA ca-cert.pem -CAkey ca-key.pem -set_serial 01 -out server-cert.pem fi # now create a key that doesn't require a passphrase openssl rsa -in $SERVER_KEY -out $SERVER_KEY .insecure mv $SERVER_KEY $SERVER_KEY .secure mv $SERVER_KEY .insecure $SERVER_KEY # show the results (no other effect) openssl rsa -noout -text -in $SERVER_KEY openssl rsa -noout -text -in ca-key.pem openssl req -noout -text -in server-key.csr openssl x509 -noout -text -in server-cert.pem openssl x509 -noout -text -in ca-cert.pem # copy *.pem file to /etc/pki/libvirt-spice if [[ ! -d \"/etc/pki/libvirt-spice\" ]] then mkdir -p /etc/pki/libvirt-spice fi cp ./*.pem /etc/pki/libvirt-spice # echo --host-subject echo \"your --host-subject is\" \\\" ` openssl x509 -noout -text -in server-cert.pem | grep Subject: | cut -f 10 - -d \" \" ` \\\" echo \"copy ca-cert.pem to %APPDATA%\\spicec\\spice_truststore.pem or ~/.spice/spice_truststore.pem in your clients\"","title":"openSSL"},{"location":"Linux/openSSL/#check-ssl-state","text":"openssl s_client -connect HOTE:443 -CApath /etc/ssl/certs","title":"Check SSL state"},{"location":"Linux/openSSL/#generate-a-self-signed-certificate","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 #!/bin/sh _HOST = $1 openssl genrsa -out $_HOST .key.pem 2048 openssl req -x509 -new -nodes -days 1460 -sha256 -key $_HOST .key.pem -out $_HOST .cert.pem openssl genrsa -out $_HOST .key 2048 openssl req -new -sha256 -key $_HOST .key -out $_HOST .csr openssl x509 -req -days 1460 -sha256 -in $_HOST .csr -CA $_HOST .cert.pem -CAkey $_HOST .key.pem -CAcreateserial -out $_HOST .crt # Optional #openssl dhparam -out /etc/ssl/$_HOST.dhparams.pem 2048 chmod 444 $_HOST .cert.pem chmod 444 $_HOST .crt chmod 400 $_HOST .key.pem chmod 400 $_HOST .key # Optionel #mv $_HOST.key.pem private/ #mv $_HOST.cert.pem certs/ #mv $_HOST.key private/ #mv $_HOST.crt certs/","title":"Generate a self signed certificate"},{"location":"Linux/openSSL/#decode-certif-p12","text":"openssl pkcs12 -in keyStore.pfx -out keyStore.pem -nodes","title":"Decode certif p12"},{"location":"Linux/openSSL/#libvirt-cert-creation-script","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 #!/bin/bash SERVER_KEY = server-key.pem # creating a key for our ca if [ ! -e ca-key.pem ] ; then openssl genrsa -des3 -out ca-key.pem 1024 fi # creating a ca if [ ! -e ca-cert.pem ] ; then openssl req -new -x509 -days 1095 -key ca-key.pem -out ca-cert.pem -subj \"/C=IL/L=Raanana/O=Red Hat/CN=my CA\" fi # create server key if [ ! -e $SERVER_KEY ] ; then openssl genrsa -out $SERVER_KEY 1024 fi # create a certificate signing request (csr) if [ ! -e server-key.csr ] ; then openssl req -new -key $SERVER_KEY -out server-key.csr -subj \"/C=IL/L=Raanana/O=Red Hat/CN=my server\" fi # signing our server certificate with this ca if [ ! -e server-cert.pem ] ; then openssl x509 -req -days 1095 -in server-key.csr -CA ca-cert.pem -CAkey ca-key.pem -set_serial 01 -out server-cert.pem fi # now create a key that doesn't require a passphrase openssl rsa -in $SERVER_KEY -out $SERVER_KEY .insecure mv $SERVER_KEY $SERVER_KEY .secure mv $SERVER_KEY .insecure $SERVER_KEY # show the results (no other effect) openssl rsa -noout -text -in $SERVER_KEY openssl rsa -noout -text -in ca-key.pem openssl req -noout -text -in server-key.csr openssl x509 -noout -text -in server-cert.pem openssl x509 -noout -text -in ca-cert.pem # copy *.pem file to /etc/pki/libvirt-spice if [[ ! -d \"/etc/pki/libvirt-spice\" ]] then mkdir -p /etc/pki/libvirt-spice fi cp ./*.pem /etc/pki/libvirt-spice # echo --host-subject echo \"your --host-subject is\" \\\" ` openssl x509 -noout -text -in server-cert.pem | grep Subject: | cut -f 10 - -d \" \" ` \\\" echo \"copy ca-cert.pem to %APPDATA%\\spicec\\spice_truststore.pem or ~/.spice/spice_truststore.pem in your clients\"","title":"Libvirt cert creation script"},{"location":"Linux/Cockpit/cockpit/","text":"Cockpit Configuration de l\u2019authentification \u00e0 double facteur Installation du paquet requis dnf install google-authenticator Premi\u00e8re configuration En tant qu\u2019utilisateur, lancer la commande suivante pour configuration le paquet: google-authenticator R\u00e9pondre aux questions, v\u00e9rifier le code et sauvegarder les recovery codes . Pour \u00e9viter un soucis avec SELinux qui interdit l\u2019acc\u00e8s \u00e0 Cockpit \u00e0 ce fichier et aux fichiers temporaires cr\u00e9\u00e9s, il faut cr\u00e9er un dossier d\u00e9di\u00e9 et y mettre le bon contexte SELinux (voir ci-apr\u00e8s). mkdir ~/.secrets mv .google_authenticator* .secrets/ Configurer pam \u00c9diter /etc/pam.d/cockpit auth required pam_google_authenticator.so secret=/home/ ${ USER } /.secrets/.google_authenticator Configurer SELinux pour Cockpit Mettre le bon contexte semanage fcontext -a -t cockpit_tmp_t \"/home/$USER/.secrets(/.*)?\" restorecon -R -v /home/$USER/.secrets","title":"Cockpit"},{"location":"Linux/Cockpit/cockpit/#cockpit","text":"","title":"Cockpit"},{"location":"Linux/Cockpit/cockpit/#configuration-de-lauthentification-a-double-facteur","text":"Installation du paquet requis dnf install google-authenticator Premi\u00e8re configuration En tant qu\u2019utilisateur, lancer la commande suivante pour configuration le paquet: google-authenticator R\u00e9pondre aux questions, v\u00e9rifier le code et sauvegarder les recovery codes . Pour \u00e9viter un soucis avec SELinux qui interdit l\u2019acc\u00e8s \u00e0 Cockpit \u00e0 ce fichier et aux fichiers temporaires cr\u00e9\u00e9s, il faut cr\u00e9er un dossier d\u00e9di\u00e9 et y mettre le bon contexte SELinux (voir ci-apr\u00e8s). mkdir ~/.secrets mv .google_authenticator* .secrets/ Configurer pam \u00c9diter /etc/pam.d/cockpit auth required pam_google_authenticator.so secret=/home/ ${ USER } /.secrets/.google_authenticator","title":"Configuration de l'authentification \u00e0 double facteur"},{"location":"Linux/Cockpit/cockpit/#configurer-selinux-pour-cockpit","text":"Mettre le bon contexte semanage fcontext -a -t cockpit_tmp_t \"/home/$USER/.secrets(/.*)?\" restorecon -R -v /home/$USER/.secrets","title":"Configurer SELinux pour Cockpit"},{"location":"Mail/Postfix/","text":"Various notes For system admins who are using postfix as their mail server : As my routine system administration I usually use some of the following commands frequently. View the postfix version postconf mail_version mail_version = 2.3.3 Check the postfix installation postfix check Show default postfix values postconf -d To show non default postfix values postconf -n To restart postfix mail server postfix reload Flush the mail queue postfix flush Or you can use postfix -f To see mail queue : mailq ( in send mail sendmail -bp ) mailq | wc -l (will give the total no of mails in queue ) To remove all mail from the queue postsuper -d ALL To remove all mails in the deferred queue postsuper -d ALL deferred To see the mails in a tree structure qshape View the mail content postcat -q AFD4A228 37C You will get the above id from mailq or you can view the mails from postfix mail spool. Usually postfix will store the mails in /var/spool/postfix/active/ from this location also you can view the mails. We can change the queue directory from the postfix conf. Sort by from address mailq | awk '/^[0-9,A-F]/ {print $7}' | sort | uniq -c | sort -n To remove all mails sent by [email protected] from the queue mailq | grep ' ^ [ A - Z0 - 9 ] ' | grep [ email protected ] | cut - f1 - d ' ' | tr - d \\ *| postsuper - d - To remove all mails being sent using the From address \u201c[email protected]\u201d mailq | awk ' /^ [ 0 - 9 , A - F ]. * [ email protected ] / { print $ 1 } ' | cut - d '!' - f 1 | postsuper - d - To remove all mails sent by the domain adminlogs.info from the queue mailq| grep '^[A-Z0-9]'|grep @adminlogs.info|cut -f1 -d' ' |tr -d \\*|postsuper -d Test your own Mailserver against attacks telnet mail-abuse.org","title":"Postfix"},{"location":"Mail/Postfix/#various-notes","text":"For system admins who are using postfix as their mail server : As my routine system administration I usually use some of the following commands frequently.","title":"Various notes"},{"location":"Mail/Postfix/#view-the-postfix-version","text":"postconf mail_version mail_version = 2.3.3","title":"View the postfix version"},{"location":"Mail/Postfix/#check-the-postfix-installation","text":"postfix check","title":"Check the postfix installation"},{"location":"Mail/Postfix/#show-default-postfix-values","text":"postconf -d","title":"Show default postfix values"},{"location":"Mail/Postfix/#to-show-non-default-postfix-values","text":"postconf -n","title":"To show non default postfix values"},{"location":"Mail/Postfix/#to-restart-postfix-mail-server","text":"postfix reload","title":"To restart postfix mail server"},{"location":"Mail/Postfix/#flush-the-mail-queue","text":"postfix flush","title":"Flush the mail queue"},{"location":"Mail/Postfix/#or-you-can-use","text":"postfix -f","title":"Or you can use"},{"location":"Mail/Postfix/#to-see-mail-queue","text":"mailq","title":"To see mail queue :"},{"location":"Mail/Postfix/#in-send-mail-sendmail-bp","text":"mailq | wc -l (will give the total no of mails in queue )","title":"( in send mail sendmail -bp )"},{"location":"Mail/Postfix/#to-remove-all-mail-from-the-queue","text":"postsuper -d ALL","title":"To remove all mail from the queue"},{"location":"Mail/Postfix/#to-remove-all-mails-in-the-deferred-queue","text":"postsuper -d ALL deferred","title":"To remove all mails in the deferred queue"},{"location":"Mail/Postfix/#to-see-the-mails-in-a-tree-structure","text":"qshape","title":"To see the mails in a tree structure"},{"location":"Mail/Postfix/#view-the-mail-content","text":"postcat -q AFD4A228 37C You will get the above id from mailq or you can view the mails from postfix mail spool. Usually postfix will store the mails in /var/spool/postfix/active/ from this location also you can view the mails. We can change the queue directory from the postfix conf.","title":"View the mail content"},{"location":"Mail/Postfix/#sort-by-from-address","text":"mailq | awk '/^[0-9,A-F]/ {print $7}' | sort | uniq -c | sort -n","title":"Sort by from address"},{"location":"Mail/Postfix/#to-remove-all-mails-sent-by-email-protected-from-the-queue","text":"mailq | grep ' ^ [ A - Z0 - 9 ] ' | grep [ email protected ] | cut - f1 - d ' ' | tr - d \\ *| postsuper - d -","title":"To remove all mails sent by [email protected] from the queue"},{"location":"Mail/Postfix/#to-remove-all-mails-being-sent-using-the-from-address-email-protected","text":"mailq | awk ' /^ [ 0 - 9 , A - F ]. * [ email protected ] / { print $ 1 } ' | cut - d '!' - f 1 | postsuper - d -","title":"To remove all mails being sent using the From address \u201c[email protected]\u201d"},{"location":"Mail/Postfix/#to-remove-all-mails-sent-by-the-domain-adminlogsinfo-from-the-queue","text":"mailq| grep '^[A-Z0-9]'|grep @adminlogs.info|cut -f1 -d' ' |tr -d \\*|postsuper -d","title":"To remove all mails sent by the domain adminlogs.info from the queue"},{"location":"Mail/Postfix/#test-your-own-mailserver-against-attacks","text":"telnet mail-abuse.org","title":"Test your own Mailserver against attacks"},{"location":"Mail/Telnet/","text":"Use Telnet to send mail telnet mail . server . tld 25 EHLO my . host . name MAIL FROM : < me @address . com > RCPT TO : < you @other . net > DATA Subject : Test Blabla . 250 2.0.0 Ok : queued as XXXXXXX","title":"Use Telnet to send mail"},{"location":"Mail/Telnet/#use-telnet-to-send-mail","text":"telnet mail . server . tld 25 EHLO my . host . name MAIL FROM : < me @address . com > RCPT TO : < you @other . net > DATA Subject : Test Blabla . 250 2.0.0 Ok : queued as XXXXXXX","title":"Use Telnet to send mail"},{"location":"Network/App_jail/","text":"Launch an app in network jail You may try network namespaces: https://lmddgtfy.net/?q=linux%20netns You can create a new network namespace, without attaching any interfaces to it, and run your application in it. Example: sudo ip netns add isolated sudo ip netns exec isolated sudo -u my_username -i This will start new shell session running as your user, but without any access to network. If you want to start graphical application in it, you need to execute export DISPLAY=unix:0","title":"App jail"},{"location":"Network/App_jail/#launch-an-app-in-network-jail","text":"You may try network namespaces: https://lmddgtfy.net/?q=linux%20netns You can create a new network namespace, without attaching any interfaces to it, and run your application in it. Example: sudo ip netns add isolated sudo ip netns exec isolated sudo -u my_username -i This will start new shell session running as your user, but without any access to network. If you want to start graphical application in it, you need to execute export DISPLAY=unix:0","title":"Launch an app in network jail"},{"location":"Network/Carp/","text":"CARP (openBSD) Si un FW master a bascul\u00e9 en backup ifconfig -g carp le compte le plus petit est Master, donc augmenter le compte de celui qui doit passer en Backup : ifconfig -g carp carpdemote 40","title":"CARP (openBSD)"},{"location":"Network/Carp/#carp-openbsd","text":"","title":"CARP (openBSD)"},{"location":"Network/Carp/#si-un-fw-master-a-bascule-en-backup","text":"ifconfig -g carp","title":"Si un FW master a bascul\u00e9 en backup"},{"location":"Network/Carp/#le-compte-le-plus-petit-est-master-donc-augmenter-le-compte-de-celui-qui-doit-passer-en-backup","text":"ifconfig -g carp carpdemote 40","title":"le compte le plus petit est Master, donc augmenter le compte de celui qui doit passer en Backup :"},{"location":"Network/FirewallD/","text":"Voir l\u2019\u00e9tat de fonctionnement du pare-feu : firewall-cmd --state Obtenir la liste des zones support\u00e9es : firewall-cmd --get-zones Obtenir la liste des services support\u00e9s : firewall-cmd --get-services Lister ce qui est activ\u00e9 sur toutes les zones : firewall-cmd --list-all-zones Voir ce qui est activ\u00e9 sur la zone \u2018public\u2019 : firewall-cmd --zone=public --list-all Lister les services actifs de la zone \u2018public\u2019 : firewall-cmd --zone=public --list-services Voir la zone par d\u00e9faut pour les connexions r\u00e9seau : firewall-cmd --get-default-zone D\u00e9finir la zone par d\u00e9faut \u00e0 \u2018public\u2019 : firewall-cmd --set-default-zone=public Lister les zones actives : firewall-cmd --get-active-zones Ajouter (ouvrir) le port 8080 (protocole tcp) \u00e0 la zone \u2018public\u2019 : firewall-cmd --zone=public --add-port=8080/tcp --permanent firewall-cmd --zone=public --add-port=7000-8000/tcp Supprimer (fermer) le port 8080 (protocole tcp) pour la zone \u2018public\u2019 : firewall-cmd --zone=public --remove-port=8080/tcp firewall-cmd --zone=public --remove-port=7000-8000/tcp Ajouter (ouvrir) le service http pour la zone \u2018public\u2019 : firewall-cmd --zone=public --add-service=http Supprimer (fermer) le service http pour la zone \u2018public\u2019 : firewall-cmd --zone=public --remove-service=http V\u00e9rifier si le service http est actif pour la zone \u2018public\u2019 : firewall-cmd --zone=public --query-service=http Recharger la configuration : firewall-cmd --reload Rich rules examples: firewall-cmd --permanent --zone=testing --add-rich-rule='rule family=ipv4 source address=10.0.0.0/24 destination address=192.168.0.10/32 port port=8080-8090 protocol=tcp accept' firewall-cmd --permanent --zone=public --add-rich-rule='rule family=ipv4 source address=92.154.6.35/32 destination address=2.56.156.11/32 port port=22 protocol=tcp accept' List rich rules\u202f: firewall-cmd --permanent --zone=testing --list-rich-rules Remove riche rule\u202f: firewall-cmd --permanent --zone=testing --remove-rich-rule='rule family=ipv4 source address=10.0.0.0/24 destination address=192.168.0.10/32 port port=8080-8090 protocol=tcp accept'","title":"FirewallD"},{"location":"Network/FirewallD/#voir-letat-de-fonctionnement-du-pare-feu","text":"firewall-cmd --state","title":"Voir l'\u00e9tat de fonctionnement du pare-feu :"},{"location":"Network/FirewallD/#obtenir-la-liste-des-zones-supportees","text":"firewall-cmd --get-zones","title":"Obtenir la liste des zones support\u00e9es :"},{"location":"Network/FirewallD/#obtenir-la-liste-des-services-supportes","text":"firewall-cmd --get-services","title":"Obtenir la liste des services support\u00e9s :"},{"location":"Network/FirewallD/#lister-ce-qui-est-active-sur-toutes-les-zones","text":"firewall-cmd --list-all-zones","title":"Lister ce qui est activ\u00e9 sur toutes les zones :"},{"location":"Network/FirewallD/#voir-ce-qui-est-active-sur-la-zone-public","text":"firewall-cmd --zone=public --list-all","title":"Voir ce qui est activ\u00e9 sur la zone 'public' :"},{"location":"Network/FirewallD/#lister-les-services-actifs-de-la-zone-public","text":"firewall-cmd --zone=public --list-services","title":"Lister les services actifs de la zone 'public' :"},{"location":"Network/FirewallD/#voir-la-zone-par-defaut-pour-les-connexions-reseau","text":"firewall-cmd --get-default-zone","title":"Voir la zone par d\u00e9faut pour les connexions r\u00e9seau :"},{"location":"Network/FirewallD/#definir-la-zone-par-defaut-a-public","text":"firewall-cmd --set-default-zone=public","title":"D\u00e9finir la zone par d\u00e9faut \u00e0 'public' :"},{"location":"Network/FirewallD/#lister-les-zones-actives","text":"firewall-cmd --get-active-zones","title":"Lister les zones actives :"},{"location":"Network/FirewallD/#ajouter-ouvrir-le-port-8080-protocole-tcp-a-la-zone-public","text":"firewall-cmd --zone=public --add-port=8080/tcp --permanent firewall-cmd --zone=public --add-port=7000-8000/tcp","title":"Ajouter (ouvrir) le port 8080 (protocole tcp) \u00e0 la zone 'public' :"},{"location":"Network/FirewallD/#supprimer-fermer-le-port-8080-protocole-tcp-pour-la-zone-public","text":"firewall-cmd --zone=public --remove-port=8080/tcp firewall-cmd --zone=public --remove-port=7000-8000/tcp","title":"Supprimer (fermer) le port 8080 (protocole tcp) pour la zone 'public' :"},{"location":"Network/FirewallD/#ajouter-ouvrir-le-service-http-pour-la-zone-public","text":"firewall-cmd --zone=public --add-service=http","title":"Ajouter (ouvrir) le service http pour la zone 'public' :"},{"location":"Network/FirewallD/#supprimer-fermer-le-service-http-pour-la-zone-public","text":"firewall-cmd --zone=public --remove-service=http","title":"Supprimer (fermer) le service http pour la zone 'public' :"},{"location":"Network/FirewallD/#verifier-si-le-service-http-est-actif-pour-la-zone-public","text":"firewall-cmd --zone=public --query-service=http","title":"V\u00e9rifier si le service http est actif pour la zone 'public' :"},{"location":"Network/FirewallD/#recharger-la-configuration","text":"firewall-cmd --reload","title":"Recharger la configuration :"},{"location":"Network/FirewallD/#rich-rules-examples","text":"firewall-cmd --permanent --zone=testing --add-rich-rule='rule family=ipv4 source address=10.0.0.0/24 destination address=192.168.0.10/32 port port=8080-8090 protocol=tcp accept' firewall-cmd --permanent --zone=public --add-rich-rule='rule family=ipv4 source address=92.154.6.35/32 destination address=2.56.156.11/32 port port=22 protocol=tcp accept'","title":"Rich rules examples:"},{"location":"Network/FirewallD/#list-rich-rules","text":"firewall-cmd --permanent --zone=testing --list-rich-rules","title":"List rich rules\u202f:"},{"location":"Network/FirewallD/#remove-riche-rule","text":"firewall-cmd --permanent --zone=testing --remove-rich-rule='rule family=ipv4 source address=10.0.0.0/24 destination address=192.168.0.10/32 port port=8080-8090 protocol=tcp accept'","title":"Remove riche rule\u202f:"},{"location":"Network/NetworkManager/","text":"Start shell script on Network Manager successful connection by Marko The other day I was writing a script that needed to do its job only when specific network interface is triggered (wireless broadband ppp0 in my case). Pinging Google every 10 seconds to detect Internet access was out of the question. There is a more elegant way to do this. If you are interested please proceed. Do you know that authors of Network Manager built option to trigger scripts right into this great application. To use this option you need to write bash script with some specific bash variables and put it to \u201c/etc/NetworkManager/dispatcher.d/\u201d directory. Specific variables are necessary to receive instructions from Network Manager about network interface that triggers execution of your script and should it be executed on \u201cup\u201d or \u201cdown\u201d operation on that interface. The following example script starts command1 after ppp0 goes \u201cup\u201d, \u201ccommand2\u201d just before ppp0 goes \u201cdown\u201d, \u201ccommand3\u201d just before ppp0 is \u201cup\u201d, and \u201ccommand4\u201d just after ppp0 goes \u201cdown\u201d. Replace commands with what you want to accomplish, you can leave some of the command \u201cfields\u201d blank if you\u2019re not interested in some use cases like \u201cpre-up\u201d or \u201cpost-down\u201d. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #!/bin/bash IF = $1 STATUS = $2 if [ \" $IF \" == \"ppp0\" ] then case \" $2 \" in up ) logger -s \"NM Script up triggered\" command1 ;; down ) logger -s \"NM Script down triggered\" command2 ;; pre-up ) logger -s \"NM Script pre-up triggered\" command3 ;; post-down ) logger -s \"NM Script post-down triggered\" command4 ;; * ) ;; esac fi This \u201c90\u201d in the name of the script means that this script will be executed in the last 10% of all scripts if you have a bunch of scripts to execute when your interface starts. You probably don\u2019t have any other scrips in \u201c/etc/NetworkManager/dispatcher.d/\u201d directory but this option is here if you need it. Now we should give permission to execute by doing \u201cchmod +x\u201d on our script and copy it in place. chmod +x /home/$USER/Desktop/90myscriptname.sh sudo cp /home/$USER/Desktop/90myscriptname.sh /etc/NetworkManager/dispatcher.d/90myscriptname.sh Finally we will monitor /var/log/syslog as it changes to make sure that everything is in order: sudo tail -f /var/log/syslog If everything is OK you will see \u201cNM Script action triggered\u201d when you connect or disconnect network interface in question. If not, retrace your steps and double check everything.","title":"Start shell script on Network Manager successful connection"},{"location":"Network/NetworkManager/#start-shell-script-on-network-manager-successful-connection","text":"by Marko The other day I was writing a script that needed to do its job only when specific network interface is triggered (wireless broadband ppp0 in my case). Pinging Google every 10 seconds to detect Internet access was out of the question. There is a more elegant way to do this. If you are interested please proceed. Do you know that authors of Network Manager built option to trigger scripts right into this great application. To use this option you need to write bash script with some specific bash variables and put it to \u201c/etc/NetworkManager/dispatcher.d/\u201d directory. Specific variables are necessary to receive instructions from Network Manager about network interface that triggers execution of your script and should it be executed on \u201cup\u201d or \u201cdown\u201d operation on that interface. The following example script starts command1 after ppp0 goes \u201cup\u201d, \u201ccommand2\u201d just before ppp0 goes \u201cdown\u201d, \u201ccommand3\u201d just before ppp0 is \u201cup\u201d, and \u201ccommand4\u201d just after ppp0 goes \u201cdown\u201d. Replace commands with what you want to accomplish, you can leave some of the command \u201cfields\u201d blank if you\u2019re not interested in some use cases like \u201cpre-up\u201d or \u201cpost-down\u201d. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #!/bin/bash IF = $1 STATUS = $2 if [ \" $IF \" == \"ppp0\" ] then case \" $2 \" in up ) logger -s \"NM Script up triggered\" command1 ;; down ) logger -s \"NM Script down triggered\" command2 ;; pre-up ) logger -s \"NM Script pre-up triggered\" command3 ;; post-down ) logger -s \"NM Script post-down triggered\" command4 ;; * ) ;; esac fi This \u201c90\u201d in the name of the script means that this script will be executed in the last 10% of all scripts if you have a bunch of scripts to execute when your interface starts. You probably don\u2019t have any other scrips in \u201c/etc/NetworkManager/dispatcher.d/\u201d directory but this option is here if you need it. Now we should give permission to execute by doing \u201cchmod +x\u201d on our script and copy it in place. chmod +x /home/$USER/Desktop/90myscriptname.sh sudo cp /home/$USER/Desktop/90myscriptname.sh /etc/NetworkManager/dispatcher.d/90myscriptname.sh Finally we will monitor /var/log/syslog as it changes to make sure that everything is in order: sudo tail -f /var/log/syslog If everything is OK you will see \u201cNM Script action triggered\u201d when you connect or disconnect network interface in question. If not, retrace your steps and double check everything.","title":"Start shell script on Network Manager successful connection"},{"location":"Network/Network_Diag/","text":"Shows how many packets are matched by each rule (iptables -Z to zero the counters) iptables -nvL pour voir la table ARP arp ip neigh Voir le cheminement mtr traceroute tracepath socat - d - d - d - TCP4 : www . server . tld : 80 dump the full iptables tables iptables-save Monitorer la BP sur iface iftop -nNpP -i <iface>","title":"Network Diag"},{"location":"Network/Network_Diag/#shows-how-many-packets-are-matched-by-each-rule-iptables-z-to-zero-the-counters","text":"iptables -nvL","title":"Shows how many packets are matched by each rule (iptables -Z to zero the counters)"},{"location":"Network/Network_Diag/#pour-voir-la-table-arp","text":"arp ip neigh","title":"pour voir la table ARP"},{"location":"Network/Network_Diag/#voir-le-cheminement","text":"mtr traceroute tracepath socat - d - d - d - TCP4 : www . server . tld : 80","title":"Voir le cheminement"},{"location":"Network/Network_Diag/#dump-the-full-iptables-tables","text":"iptables-save","title":"dump the full iptables tables"},{"location":"Network/Network_Diag/#monitorer-la-bp-sur-iface","text":"iftop -nNpP -i <iface>","title":"Monitorer la BP sur iface"},{"location":"Network/TCPdump/","text":"Pipe tcpdump into Wireshark ssh root@server.tld tcpdump -i any -s0 -v -w - port not ssh | wireshark -k -i -","title":"TCPdump"},{"location":"Network/TCPdump/#pipe-tcpdump-into-wireshark","text":"ssh root@server.tld tcpdump -i any -s0 -v -w - port not ssh | wireshark -k -i -","title":"Pipe tcpdump into Wireshark"},{"location":"Network/Trafic_error/","text":"Trafi error Fix traffic : interface must be down error http://blackhold.nusepas.com/2016/08/06/centreon-critical-interface-speed-equal-0-interface-must-be-down/ use -T 1000 option","title":"Trafi error"},{"location":"Network/Trafic_error/#trafi-error","text":"","title":"Trafi error"},{"location":"Network/Trafic_error/#fix-traffic-interface-must-be-down-error","text":"http://blackhold.nusepas.com/2016/08/06/centreon-critical-interface-speed-equal-0-interface-must-be-down/ use -T 1000 option","title":"Fix traffic : interface must be down error"},{"location":"OS/Clonezilla_PXE/","text":"Mise en place d\u2019un serveur Clonezilla via PXE Pr\u00e9-requis et notions pr\u00e9liminaires Le serveur PXE utilise Debian GNU/Linux en tant que syst\u00e8me d\u2019exploitation. Le serveur PXE (Preboot eXecution Environment, environnement de d\u00e9marrage d\u2019ordinateurs en r\u00e9seau) aura ici pour but de fournir aux postes clients connect\u00e9s une image ISO bootable de Clonezilla en vue du d\u00e9ploiement d\u2019images openSUSE adapt\u00e9es aux sp\u00e9cificit\u00e9s d\u2019\u00c9veha. Des paquets doivent \u00eatre install\u00e9s, le serveur doit donc avoir une connexion Internet lors de sa mise en place. Par la suite, le serveur peut n\u2019avoir qu\u2019une seule interface r\u00e9seau configur\u00e9e comme suit: adresse IP : 192.168.1.1 netmask : 255.255.255.0 broadcast : 192.168.1.255 network : 192.168.1.0 (r\u00e9seau priv\u00e9e entre serveur et clients) gateway : 192.168.1.1 (le serveur lui-m\u00eame, aucune importance dans ce cadre) nameserver : 127.0.0.1 (le serveur lui-m\u00eame, aucune importance dans ce cadre) Le serveur va fournir un service DHCP au sein du r\u00e9seau priv\u00e9, il est donc important qu\u2019il ne soit pas reli\u00e9 au r\u00e9seau ext\u00e9rieur afin d\u2019\u00e9viter tout conflit dans la fourniture du service DHCP. Installation des paquets n\u00e9cessaires apt-get install isc-dhcp-server tftpd-hpa syslinux pxe nfs-kernel-server Configuration du service DHCP sur le serveur Ce paquet fournit le daemon dhcpd et le service isc-dhcp-server. Il se configure via le fichier /etc/dhcp/dhcpd.conf, il faut ajouter ce qui suit: # D\u00e9but de configuration du service DHCP # # D\u00e9claration des leases et plages de service du serveur PXE : subnet 192 . 168 . 1 . 0 netmask 255 . 255 . 255 . 0 { range 192 . 168 . 1 . 10 192 . 168 . 1 . 20 ; # \u00e0 adapter aux nombres de postes clients option broadcast - address 192 . 168 . 1 . 255 ; option routers 192 . 168 . 1 . 1 ; # IP du serveur PXE si on veut faire du routage option domain - name - servers 192 . 168 . 1 . 1 ; # idem filename \"pxelinux.0\" ; } group { next - server 192 . 168 . 1 . 1 ; # si n\u00e9cessaire host tftpclient { filename \"pxelinux.0\" ; } } # # Fin de configuration du service DHCP Puis red\u00e9marrer le service: service isc-dhcp-server restart Il est possible que l\u2019on obtienne un fail si aucun client n\u2019est connect\u00e9 ou que la connexion est inactive. Configuration du service TFTP sur le serveur Ce service se configure comme suit via le fichier /etc/default/tftpd-hpa: # D\u00e9but de configuration du service TFTP : # TFTP_USERNAME = \"tftp\" TFTP_DIRECTORY = \"/srv/tftp\" TFTP_ADDRESS = \"0.0.0.0:69\" TFTP_OPTIONS = \"--secure\" # # Fin de configuration du service TFTP Puis red\u00e9marrer le service: service tftpd-hpa restart Ceci peut \u00e9chouer dans le cas o\u00f9 le dossier /srv/tftp/ n\u2019est pas pr\u00e9sent (il est normalement cr\u00e9\u00e9 automatiquement \u00e0 l\u2019installation du paquet), il suffit alors de le cr\u00e9er. Pr\u00e9paration et mise en place de l\u2019image Clonezilla T\u00e9l\u00e9charger une image .iso live de Clonezilla: wget http://heanet.dl.sourceforge.net/project/clonezilla/clonezilla_live_stable/2.2.3-25/clonezilla-live-2.2.3-25-amd64.iso Monter l\u2019image dans /mnt: mount -o loop -t iso9660 /root/clonezilla-live-2.2.3-25-amd64.iso /mnt Copier tous les fichiers dans le dossier r\u00e9serv\u00e9 du serveur tftp: cp -ar /mnt/* /srv/tftp/clonezilla Configuration du serveur NFS Les images des syst\u00e8mes que nous souhaitons booter \u00e0 distance vont \u00eatre servis aux clients via un partage NFS. Mise en place des exports NFS Cr\u00e9ation du dossier contenant l\u2019image live de Clonezilla: mkdir /srv/tftp/clonezilla emacs (ou vi) /etc/exports et ajout des lignes correspondantes: /srv/tftp/clonezilla 192.168.1.0/24(async,no_root_squash,no_subtree_check,ro) Note: l\u2019IP et le masque CIDR sont \u00e0 adapter au r\u00e9seau et l\u2019image doit \u00eatre de pr\u00e9f\u00e9rence en read-only. Puis on active les partages NFS: service nfs-kernel-server restart Et on les v\u00e9rifie: exportfs -v Cette commande doit renvoyer la liste des partages actifs. Mise en place d\u2019une image de boot Le paquet syslinux fournit une collection de bootloader dont certains nous seront n\u00e9cessaires pour d\u00e9marrer en PXE et afficher le menu qui va bien. Copie des \u00e9l\u00e9ments n\u00e9cessaires cd /usr/lib/syslinux cp chain.c32 mboot.c32 menu.c32 pxelinux.0 reboot.c32 vesamenu.c32 -t /srv/tftp/ Configuration du service PXE emacs ( ou vi ) / etc / pxe . conf # D\u00e9but de configuration du service PXE : # # which interface to use : interface = eth0 default_address = 192 . 168 . 1 . 1 # tftpd base dir : tftpdbase =/ srv / tftp # domain name : domain = domain . fr # # Fin de configuration du service PXE Mise en place du dossier et fichier menu PXE mkdir /srv/tftp/pxelinux.cfg emacs (ou vi) /srv/tftp/pxelinux.cfg/default Ce fichier va contenir les instructions pour le menu de boot via PXE # D\u00e9but de configuration du menu PXE : # # Interface visuelle : DEFAULT vesamenu . c32 MENU TITLE Bienvenue sur le serveur Clonezilla prompt 0 kbdmap french . kbd # Entr\u00e9e du menu Clonezilla LABEL Demarrer Clonezilla KERNEL clonezilla / live / vmlinuz APPEND boot = live rootfstype = nfs netboot = nfs nfsroot = 192 . 168 . 1 . 1 : / srv / tftp / clonezilla initrd = clonezilla / live / initrd . img config -- # Entr\u00e9e du menu de red\u00e9marrage LABEL Reboot MENU LABEL Redemarrer KERNEL reboot . c32 # # Fin de configuration du menu PXE Un chmod -R 775 dans ce m\u00eame dossier peut \u00eatre n\u00e9cessaire pour que le daemon TFTPD puisse les lire. Sp\u00e9cificit\u00e9s de Clonezilla L\u2019id\u00e9e \u00e9tant de booter sur un live Clonezilla pour cr\u00e9er et d\u00e9ployer des images disques, nous allons adapter notre environnement \u00e0 ce but: (/partimag est le dossier par d\u00e9faut de Clonezilla, on peut mettre autre chose mais cela implique de le sp\u00e9cifier manuellement \u00e0 chaque clonage/copie.) Ces images vont transiter par un partage NFS sur un NAS: IP du NAS: 192.168.87.21 Nom du NAS: nfsclone Version de NFS: NFSv4 Emplacement du dossier partag\u00e9: /volume1/partimag Utilisation du serveur Il ne reste plus qu\u2019\u00e0 connecter les clients, les d\u00e9marrer via PXE, s\u00e9lectionner l\u2019entr\u00e9e de menu \u2018Clonezilla\u2019. Le reste est une utilisation classique de Clonezilla \u00e0 ceci pr\u00e8s que les images vont \u00eatre \u00e9crites/lues vers/depuis un partage NFS.","title":"Clonezilla PXE"},{"location":"OS/Clonezilla_PXE/#mise-en-place-dun-serveur-clonezilla-via-pxe","text":"","title":"Mise en place d'un serveur Clonezilla via PXE"},{"location":"OS/Clonezilla_PXE/#pre-requis-et-notions-preliminaires","text":"Le serveur PXE utilise Debian GNU/Linux en tant que syst\u00e8me d\u2019exploitation. Le serveur PXE (Preboot eXecution Environment, environnement de d\u00e9marrage d\u2019ordinateurs en r\u00e9seau) aura ici pour but de fournir aux postes clients connect\u00e9s une image ISO bootable de Clonezilla en vue du d\u00e9ploiement d\u2019images openSUSE adapt\u00e9es aux sp\u00e9cificit\u00e9s d\u2019\u00c9veha. Des paquets doivent \u00eatre install\u00e9s, le serveur doit donc avoir une connexion Internet lors de sa mise en place. Par la suite, le serveur peut n\u2019avoir qu\u2019une seule interface r\u00e9seau configur\u00e9e comme suit: adresse IP : 192.168.1.1 netmask : 255.255.255.0 broadcast : 192.168.1.255 network : 192.168.1.0 (r\u00e9seau priv\u00e9e entre serveur et clients) gateway : 192.168.1.1 (le serveur lui-m\u00eame, aucune importance dans ce cadre) nameserver : 127.0.0.1 (le serveur lui-m\u00eame, aucune importance dans ce cadre) Le serveur va fournir un service DHCP au sein du r\u00e9seau priv\u00e9, il est donc important qu\u2019il ne soit pas reli\u00e9 au r\u00e9seau ext\u00e9rieur afin d\u2019\u00e9viter tout conflit dans la fourniture du service DHCP.","title":"Pr\u00e9-requis et notions pr\u00e9liminaires"},{"location":"OS/Clonezilla_PXE/#installation-des-paquets-necessaires","text":"apt-get install isc-dhcp-server tftpd-hpa syslinux pxe nfs-kernel-server","title":"Installation des paquets n\u00e9cessaires"},{"location":"OS/Clonezilla_PXE/#configuration-du-service-dhcp-sur-le-serveur","text":"Ce paquet fournit le daemon dhcpd et le service isc-dhcp-server. Il se configure via le fichier /etc/dhcp/dhcpd.conf, il faut ajouter ce qui suit: # D\u00e9but de configuration du service DHCP # # D\u00e9claration des leases et plages de service du serveur PXE : subnet 192 . 168 . 1 . 0 netmask 255 . 255 . 255 . 0 { range 192 . 168 . 1 . 10 192 . 168 . 1 . 20 ; # \u00e0 adapter aux nombres de postes clients option broadcast - address 192 . 168 . 1 . 255 ; option routers 192 . 168 . 1 . 1 ; # IP du serveur PXE si on veut faire du routage option domain - name - servers 192 . 168 . 1 . 1 ; # idem filename \"pxelinux.0\" ; } group { next - server 192 . 168 . 1 . 1 ; # si n\u00e9cessaire host tftpclient { filename \"pxelinux.0\" ; } } # # Fin de configuration du service DHCP Puis red\u00e9marrer le service: service isc-dhcp-server restart Il est possible que l\u2019on obtienne un fail si aucun client n\u2019est connect\u00e9 ou que la connexion est inactive.","title":"Configuration du service DHCP sur le serveur"},{"location":"OS/Clonezilla_PXE/#configuration-du-service-tftp-sur-le-serveur","text":"Ce service se configure comme suit via le fichier /etc/default/tftpd-hpa: # D\u00e9but de configuration du service TFTP : # TFTP_USERNAME = \"tftp\" TFTP_DIRECTORY = \"/srv/tftp\" TFTP_ADDRESS = \"0.0.0.0:69\" TFTP_OPTIONS = \"--secure\" # # Fin de configuration du service TFTP Puis red\u00e9marrer le service: service tftpd-hpa restart Ceci peut \u00e9chouer dans le cas o\u00f9 le dossier /srv/tftp/ n\u2019est pas pr\u00e9sent (il est normalement cr\u00e9\u00e9 automatiquement \u00e0 l\u2019installation du paquet), il suffit alors de le cr\u00e9er.","title":"Configuration du service TFTP sur le serveur"},{"location":"OS/Clonezilla_PXE/#preparation-et-mise-en-place-de-limage-clonezilla","text":"T\u00e9l\u00e9charger une image .iso live de Clonezilla: wget http://heanet.dl.sourceforge.net/project/clonezilla/clonezilla_live_stable/2.2.3-25/clonezilla-live-2.2.3-25-amd64.iso Monter l\u2019image dans /mnt: mount -o loop -t iso9660 /root/clonezilla-live-2.2.3-25-amd64.iso /mnt Copier tous les fichiers dans le dossier r\u00e9serv\u00e9 du serveur tftp: cp -ar /mnt/* /srv/tftp/clonezilla","title":"Pr\u00e9paration et mise en place de l'image Clonezilla"},{"location":"OS/Clonezilla_PXE/#configuration-du-serveur-nfs","text":"Les images des syst\u00e8mes que nous souhaitons booter \u00e0 distance vont \u00eatre servis aux clients via un partage NFS.","title":"Configuration du serveur NFS"},{"location":"OS/Clonezilla_PXE/#mise-en-place-des-exports-nfs","text":"Cr\u00e9ation du dossier contenant l\u2019image live de Clonezilla: mkdir /srv/tftp/clonezilla emacs (ou vi) /etc/exports et ajout des lignes correspondantes: /srv/tftp/clonezilla 192.168.1.0/24(async,no_root_squash,no_subtree_check,ro) Note: l\u2019IP et le masque CIDR sont \u00e0 adapter au r\u00e9seau et l\u2019image doit \u00eatre de pr\u00e9f\u00e9rence en read-only. Puis on active les partages NFS: service nfs-kernel-server restart Et on les v\u00e9rifie: exportfs -v Cette commande doit renvoyer la liste des partages actifs.","title":"Mise en place des exports NFS"},{"location":"OS/Clonezilla_PXE/#mise-en-place-dune-image-de-boot","text":"Le paquet syslinux fournit une collection de bootloader dont certains nous seront n\u00e9cessaires pour d\u00e9marrer en PXE et afficher le menu qui va bien.","title":"Mise en place d'une image de boot"},{"location":"OS/Clonezilla_PXE/#copie-des-elements-necessaires","text":"cd /usr/lib/syslinux cp chain.c32 mboot.c32 menu.c32 pxelinux.0 reboot.c32 vesamenu.c32 -t /srv/tftp/","title":"Copie des \u00e9l\u00e9ments n\u00e9cessaires"},{"location":"OS/Clonezilla_PXE/#configuration-du-service-pxe","text":"emacs ( ou vi ) / etc / pxe . conf # D\u00e9but de configuration du service PXE : # # which interface to use : interface = eth0 default_address = 192 . 168 . 1 . 1 # tftpd base dir : tftpdbase =/ srv / tftp # domain name : domain = domain . fr # # Fin de configuration du service PXE","title":"Configuration du service PXE"},{"location":"OS/Clonezilla_PXE/#mise-en-place-du-dossier-et-fichier-menu-pxe","text":"mkdir /srv/tftp/pxelinux.cfg emacs (ou vi) /srv/tftp/pxelinux.cfg/default Ce fichier va contenir les instructions pour le menu de boot via PXE # D\u00e9but de configuration du menu PXE : # # Interface visuelle : DEFAULT vesamenu . c32 MENU TITLE Bienvenue sur le serveur Clonezilla prompt 0 kbdmap french . kbd # Entr\u00e9e du menu Clonezilla LABEL Demarrer Clonezilla KERNEL clonezilla / live / vmlinuz APPEND boot = live rootfstype = nfs netboot = nfs nfsroot = 192 . 168 . 1 . 1 : / srv / tftp / clonezilla initrd = clonezilla / live / initrd . img config -- # Entr\u00e9e du menu de red\u00e9marrage LABEL Reboot MENU LABEL Redemarrer KERNEL reboot . c32 # # Fin de configuration du menu PXE Un chmod -R 775 dans ce m\u00eame dossier peut \u00eatre n\u00e9cessaire pour que le daemon TFTPD puisse les lire.","title":"Mise en place du dossier et fichier menu PXE"},{"location":"OS/Clonezilla_PXE/#specificites-de-clonezilla","text":"L\u2019id\u00e9e \u00e9tant de booter sur un live Clonezilla pour cr\u00e9er et d\u00e9ployer des images disques, nous allons adapter notre environnement \u00e0 ce but: (/partimag est le dossier par d\u00e9faut de Clonezilla, on peut mettre autre chose mais cela implique de le sp\u00e9cifier manuellement \u00e0 chaque clonage/copie.) Ces images vont transiter par un partage NFS sur un NAS: IP du NAS: 192.168.87.21 Nom du NAS: nfsclone Version de NFS: NFSv4 Emplacement du dossier partag\u00e9: /volume1/partimag","title":"Sp\u00e9cificit\u00e9s de Clonezilla"},{"location":"OS/Clonezilla_PXE/#utilisation-du-serveur","text":"Il ne reste plus qu\u2019\u00e0 connecter les clients, les d\u00e9marrer via PXE, s\u00e9lectionner l\u2019entr\u00e9e de menu \u2018Clonezilla\u2019. Le reste est une utilisation classique de Clonezilla \u00e0 ceci pr\u00e8s que les images vont \u00eatre \u00e9crites/lues vers/depuis un partage NFS.","title":"Utilisation du serveur"},{"location":"OS/openSUSE_memo/","text":"Zypper tricks To ensure old kernels are purged, check /etc/zypp/zypp.conf for the lines multiversion = provides:multiversion(kernel) and multiversion.kernels = latest,latest-1,running and ensure both are uncommented. (Refer to https://lizards.opensuse.org/tag/kernel-update/ ) You will possibly need to create an empty file in /boot named do_purge_kernels , then run (as root): systemctl enable purge-kernels Check the status by running: systemctl status purge-kernels If this shows inactive (dead) or not enabled , it may be simply because there are no kernels to purge. Sort RPM packages by size rpm -q -a --queryformat \"%{SIZE}\\t%{INSTALLTIME:day} \\ %{BUILDTIME:day}\\t %{SIZE}\\t %{ARCHIVESIZE}\\t %{FILESIZES}\\t %{LONGARCHIVESIZE}\\t %{LONGFILESIZES}\\t %{LONGSIZE}\\t %-30{NAME}\\t%15{VERSION}-%-7{RELEASE}\\t%{arch} \\ %25{VENDOR}%25{PACKAGER} == %{DISTRIBUTION} %{DISTTAG}\\n\" | sort --numeric-sort | cut --fields=\"2-\" | tee rpmlist | less -S","title":"openSUSE memo"},{"location":"OS/openSUSE_memo/#zypper-tricks","text":"To ensure old kernels are purged, check /etc/zypp/zypp.conf for the lines multiversion = provides:multiversion(kernel) and multiversion.kernels = latest,latest-1,running and ensure both are uncommented. (Refer to https://lizards.opensuse.org/tag/kernel-update/ ) You will possibly need to create an empty file in /boot named do_purge_kernels , then run (as root): systemctl enable purge-kernels Check the status by running: systemctl status purge-kernels If this shows inactive (dead) or not enabled , it may be simply because there are no kernels to purge.","title":"Zypper tricks"},{"location":"OS/openSUSE_memo/#sort-rpm-packages-by-size","text":"rpm -q -a --queryformat \"%{SIZE}\\t%{INSTALLTIME:day} \\ %{BUILDTIME:day}\\t %{SIZE}\\t %{ARCHIVESIZE}\\t %{FILESIZES}\\t %{LONGARCHIVESIZE}\\t %{LONGFILESIZES}\\t %{LONGSIZE}\\t %-30{NAME}\\t%15{VERSION}-%-7{RELEASE}\\t%{arch} \\ %25{VENDOR}%25{PACKAGER} == %{DISTRIBUTION} %{DISTTAG}\\n\" | sort --numeric-sort | cut --fields=\"2-\" | tee rpmlist | less -S","title":"Sort RPM packages by size"},{"location":"OS/CentOS/Apache_SeLinux/","text":"Apache Mod_proxy \u2018[Error] (13)Permission Denied\u2019 Error on RHEL Had an interesting issue today working on a mod_proxy setup of Apache forwarding requests in a reverse proxy setup to a backend Tomcat server. No matter what I did, I kept getting this in Apache\u2019s error log: [ error ] ( 13 ) Permission denied : proxy : AJP : attempt to connect to 10. x . x . x : 7009 ( virtualhost . virtualdomain . com ) failed I thought for sure it was proxy permissions, but nothing I did fixed the issue. Then it hit me: SELinux! Why I always think of SELinux last when it\u2019s responsible for 90% of my problems, I\u2019ll never know. SELinux on RHEL/CentOS by default ships so that httpd processes cannot initiate outbound connections, which is just what mod_proxy attempts to do. If this is your problem, you\u2019ll see something like this in /var/log/audit/audit.log: type=AVC msg=audit(1265039669.305:14): avc: denied { name_connect } for pid=4343 comm=\"httpd\" dest=7009 scontext=system_u:system_r:httpd_t:s0 tcontext=system_u:object_r:port_t:s0 tclass=tcp_socket To fix this, first test by setting the boolean dynamically (not permanent yet): /usr/sbin/setsebool httpd_can_network_connect 1 If that works, you can set it so that the default policy is changed and this setting will persist across reboots: /usr/sbin/setsebool -P httpd_can_network_connect 1","title":"Apache SeLinux"},{"location":"OS/CentOS/Apache_SeLinux/#apache-mod_proxy-error-13permission-denied-error-on-rhel","text":"Had an interesting issue today working on a mod_proxy setup of Apache forwarding requests in a reverse proxy setup to a backend Tomcat server. No matter what I did, I kept getting this in Apache\u2019s error log: [ error ] ( 13 ) Permission denied : proxy : AJP : attempt to connect to 10. x . x . x : 7009 ( virtualhost . virtualdomain . com ) failed I thought for sure it was proxy permissions, but nothing I did fixed the issue. Then it hit me: SELinux! Why I always think of SELinux last when it\u2019s responsible for 90% of my problems, I\u2019ll never know. SELinux on RHEL/CentOS by default ships so that httpd processes cannot initiate outbound connections, which is just what mod_proxy attempts to do. If this is your problem, you\u2019ll see something like this in /var/log/audit/audit.log: type=AVC msg=audit(1265039669.305:14): avc: denied { name_connect } for pid=4343 comm=\"httpd\" dest=7009 scontext=system_u:system_r:httpd_t:s0 tcontext=system_u:object_r:port_t:s0 tclass=tcp_socket To fix this, first test by setting the boolean dynamically (not permanent yet): /usr/sbin/setsebool httpd_can_network_connect 1 If that works, you can set it so that the default policy is changed and this setting will persist across reboots: /usr/sbin/setsebool -P httpd_can_network_connect 1","title":"Apache Mod_proxy '[Error] (13)Permission Denied' Error on RHEL"},{"location":"OS/CentOS/Colorful_root_prompt/","text":"Colorized prompt for root # . bashrc # User specific aliases and functions alias rm = 'rm -i' alias cp = 'cp -i' alias mv = 'mv -i' # Source global definitions if [ - f / etc / bashrc ]; then . / etc / bashrc fi # Colorful prompt PS1 = '[ \\[\\033[01;31m\\]\\u@\\H \\w\\[\\033[02;00m\\] ]\\[\\033[00m\\] '","title":"Colorized prompt for root"},{"location":"OS/CentOS/Colorful_root_prompt/#colorized-prompt-for-root","text":"# . bashrc # User specific aliases and functions alias rm = 'rm -i' alias cp = 'cp -i' alias mv = 'mv -i' # Source global definitions if [ - f / etc / bashrc ]; then . / etc / bashrc fi # Colorful prompt PS1 = '[ \\[\\033[01;31m\\]\\u@\\H \\w\\[\\033[02;00m\\] ]\\[\\033[00m\\] '","title":"Colorized prompt for root"},{"location":"OS/CentOS/vimrc/","text":"set nocompatible set confirm \u201d highlight ugly trailing whitespaces highlight TrailWhitespace ctermbg=red guibg=red match TrailWhitespace /\\v\\s+(%#)@<!\u2013$/ match TrailWhitespace /\\v\\s+(%#)@<!$/ let mapleader=\u201d \u201c nnoremap :nohlsearch :echo \u201cset pastetoggle= p set completeopt=longest set splitright set splitbelow \u201d Coloration syntaxique syn enable set background=dark \u201d Affiche les commandes au fur et \u00e0 mesure qu\u2019on les tape set showcmd \u201d Surligne les recherches set hlsearch \u201d Use case insensitive search, except when using capital letters set ignorecase set smartcase \u201d Affiche les possibilit\u00e9s de compl\u00e9tion dans la barre de statut set wildmenu fun! SetStatusLine() let l:s1=\u201d%-3.3n\\ %f\\ %h%m%r%w\u201d let l:s2=\u201d[%{strlen(&filetype)?&filetype:\u2019?\u2019},%{&encoding}]\u201d let l:s3=\u201d%=\\ 0x%-8B\\ \\ %-14.(%l,%c%V%)\\ %<%P\u201d execute \u201cset statusline=\u201d . l:s1 . l:s2 . l:s3 endfun set laststatus=2 hi StatusLine ctermfg=blue hi StatusLine ctermbg=white call SetStatusLine() \u201d Don\u2019t save backups of .gpg files set backupskip+= .gpg \u201d donner des droits d\u2019ex\u00e9cution si le fichier commence par #! function! ModeChange() if getline(1) =~ \u201c^#!\u201d silent !chmod a+x endif endfunction au BufWritePost * call ModeChange() \u201d Toujours laisser des lignes visibles (ici 3) au dessus/en dessous \u201d du curseur quand on atteint le d\u00e9but ou la fin de l\u2019\u00e9cran : set scrolloff=3 \u201d shebang automatique lors de l\u2019ouverture d\u2019un nouveau fichier \u201d .py, .sh (bash), modifier l\u2019ent\u00eate selon les besoins : autocmd BufNewFile .sh, .bash 0put =\"#!/bin/sh\\ # - - coding: UTF8 - -\\ \\ \"|$ autocmd BufNewFile .py 0put=\"#!/usr/bin/env python\"|1put=\"# - - coding: UTF8 - -\\ \\ \"|$ autocmd BufNewFile .c 0put=\"#include \"|1put=\"#include \\ \"|3put=\"int main(int argc, char* argv[]) {\\ \\ }\"|$ set modeline set modelines=5 set completeopt=longest set wildmode=longest,list,full if $TERM == \u2018xterm-256color\u2019 set t_Co=256 endif \u201d pour sortir du mode insert facilement inoremap kj autocmd FileType yaml set shiftwidth=2 tabstop=2 expandtab \u201d d\u00e9sactive les param\u00e8tres par d\u00e9faut de /usr/share/vim/vim80/defaults.vim \u2026 let g:skip_defaults_vim = 1 \u201d et surtout la souris set mouse=\u201d\u201c \u201d modif pour backspace nocompatible set backspace=indent,eol,start \u201d utilisation des espaces set tabstop=4 softtabstop=0 expandtab shiftwidth=4 smarttab","title":"Vimrc"},{"location":"OS/Centreon/SNMP_Config/","text":"Centreon Config SNMPv3 pour les serveurs Arreter le service snmpd service snmpd stop Cr\u00e9ation de l\u2019utilisateur Il existe 2 m\u00e9thodes pour la cr\u00e9ation d\u2019utilisateur SNMP V3 Une avec une commande et une autre manuelle 1 \u00e8re m\u00e9thode (avec le script) net - snmp - config --create-snmpv3-user -a SHA -x AES ##################################################### Enter a SNMPv3 user name to create : snmpuser Enter authentication pass - phrase : sfjslkfjslkgjsm Enter encryption pass - phrase : RTRGrbtyyb4566UUJ ##################################################### ceci rajoute 1 entr\u00e9e dans le fichier /usr/share/snmp/snmpd.conf et /var/lib/net-snmp/snmpd.conf / usr / share / snmp / snmpd . conf ########################## rwuser snmpuser / var / lib / net - snmp / snmpd . conf ########################## createUser snmpuser SHA \"sfjslkfjslkgjsm\" AES \"RTRGrbtyyb4566UUJ\" Modifier les droits de l\u2019utilisateur snmpuser en lecture seule vi /usr/share/snmp/snmpd.conf rouser snmpuser SNMPUSERNAME : snmpuser SNMPPASSWORD : sfjslkfjslkgjsm SNMPAUTHPROTOCOL : SHA SNMPPRIVPASSWORD : RTRGrbtyyb4566UUJ SNMPPRIVPROTOCOL : AES ---- 2 \u00e8me m\u00e9thode ---- (manuelle) Ajouter l\u2019utilisateur SNMP V3 avec les droits en lecture seule vi /usr/share/snmp/snmpd.conf rouser snmpuser Rajouter votre utilisateur avec les param\u00e8tres requis (Nom d\u2019utilisateur, algorithme de hachage, mot de passe, algorithme de chiffrement, clef de chiffrement) /var/lib/net-snmp/snmpd.conf createUser snmpuser SHA \u201cVuldpGEQTI4o2p\u201d AES \u201caRtz0OqaIcbHKs\u201d D\u00e9marrage du service snmpd Le d\u00e9marrage du service aura pour effet de modifier l\u2019entr\u00e9e dans le fichier /var/lib/net-snmp/snmpd.conf et cacher les clefs d\u2019authentification et de chiffrement service snmpd start /var/lib/net-snmp/snmpd.conf usmUser 1 3 0x80001f8880d03ab707afecd75700000000 \u201csnmpuser\u201d \u201csnmpuser\u201d NULL .1.3.6.1.6.3.10.1.1.3 0xc6910771242151aca36878ce38cda60d50b86d4a .1.3.6.1.6.3.10.1.2.4 0xb1248618de541a5995ea635aab1fb5b6 \u201c\u201d Pour tester la connexion snmp en local snmpwalk -v 3 -u snmpuser -a SHA -A \u2018VuldpGEQTI4o2p\u2019 -x AES -X \u2018aRtz0OqaIcbHKs\u2019 -l authPriv localhost pour tester la connexion \u00e0 partir du serveur Centreon /usr/lib/nagios/plugins/check_centreon_snmp_remote_storage -H bareos -n -d /backup-01 -w 80 -c 90 -v 3 -u snmpuser -p \u2018VuldpGEQTI4o2p\u2019 \u2013authprotocol SHA \u2013privpassword \u2018aRtz0OqaIcbHKs\u2019 \u2013privprotocol AES Pour finir ne pas oublier de d\u00e9sactiver les acc\u00e8s SNMP v1 et v2 en commentant les lignes suivantes rocommunity public 127.0.0.1 rwcommunity mysecret 127.0.0.1 com2sec local localhost public com2sec mynetwork 192.168.87.65 public com2sec mynetwork 192.168.87.65 public Pare-Feu des \u00e9quipements \u00e0 monitorer UDP : 161 et 162 Config SNMPv3 pour les PDU User Name : snmpuser Authentication Passphrase : VuldpGEQTI4o24ffrl845fGtfb8 Privacy Passphrase : aRtz0OqaIcbHKsQTI485dhJHt8s Config SNMPv3 sur les Synology Nom d\u2019utilisateur : snmpuser Mot de passe : VuldpGEQTI4o2p Protocole d\u2019authentification : MD5 (pour info) Installer module Perl manquan pour la crypto yum install -y epel-release.noarch yum install -y perl-Crypt-Rijndael Config commune des commandes -v 3 -u $_SERVICESNMPUSERNAME$ -p $_SERVICESNMPPASSWORD$ --authprotocol $_SERVICESNMPAUTHPROTOCOL$ --privpassword $_SERVICESNMPPRIVPASSWORD$ --privprotocol $_SERVICESNMPPRIVPROTOCOL$ --snmp-timeout 60","title":"Centreon"},{"location":"OS/Centreon/SNMP_Config/#centreon","text":"","title":"Centreon"},{"location":"OS/Centreon/SNMP_Config/#config-snmpv3-pour-les-serveurs","text":"Arreter le service snmpd service snmpd stop Cr\u00e9ation de l\u2019utilisateur Il existe 2 m\u00e9thodes pour la cr\u00e9ation d\u2019utilisateur SNMP V3 Une avec une commande et une autre manuelle","title":"Config SNMPv3 pour les serveurs"},{"location":"OS/Centreon/SNMP_Config/#1-ere-methode-avec-le-script","text":"net - snmp - config --create-snmpv3-user -a SHA -x AES ##################################################### Enter a SNMPv3 user name to create : snmpuser Enter authentication pass - phrase : sfjslkfjslkgjsm Enter encryption pass - phrase : RTRGrbtyyb4566UUJ ##################################################### ceci rajoute 1 entr\u00e9e dans le fichier /usr/share/snmp/snmpd.conf et /var/lib/net-snmp/snmpd.conf / usr / share / snmp / snmpd . conf ########################## rwuser snmpuser / var / lib / net - snmp / snmpd . conf ########################## createUser snmpuser SHA \"sfjslkfjslkgjsm\" AES \"RTRGrbtyyb4566UUJ\" Modifier les droits de l\u2019utilisateur snmpuser en lecture seule vi /usr/share/snmp/snmpd.conf","title":"1 \u00e8re m\u00e9thode (avec le script)"},{"location":"OS/Centreon/SNMP_Config/#_1","text":"rouser snmpuser SNMPUSERNAME : snmpuser SNMPPASSWORD : sfjslkfjslkgjsm SNMPAUTHPROTOCOL : SHA SNMPPRIVPASSWORD : RTRGrbtyyb4566UUJ SNMPPRIVPROTOCOL : AES ---- 2 \u00e8me m\u00e9thode ---- (manuelle) Ajouter l\u2019utilisateur SNMP V3 avec les droits en lecture seule","title":""},{"location":"OS/Centreon/SNMP_Config/#_2","text":"vi /usr/share/snmp/snmpd.conf","title":""},{"location":"OS/Centreon/SNMP_Config/#_3","text":"rouser snmpuser Rajouter votre utilisateur avec les param\u00e8tres requis (Nom d\u2019utilisateur, algorithme de hachage, mot de passe, algorithme de chiffrement, clef de chiffrement) /var/lib/net-snmp/snmpd.conf","title":""},{"location":"OS/Centreon/SNMP_Config/#_4","text":"createUser snmpuser SHA \u201cVuldpGEQTI4o2p\u201d AES \u201caRtz0OqaIcbHKs\u201d D\u00e9marrage du service snmpd","title":""},{"location":"OS/Centreon/SNMP_Config/#_5","text":"Le d\u00e9marrage du service aura pour effet de modifier l\u2019entr\u00e9e dans le fichier /var/lib/net-snmp/snmpd.conf et cacher les clefs d\u2019authentification et de chiffrement","title":""},{"location":"OS/Centreon/SNMP_Config/#_6","text":"service snmpd start","title":""},{"location":"OS/Centreon/SNMP_Config/#_7","text":"/var/lib/net-snmp/snmpd.conf","title":""},{"location":"OS/Centreon/SNMP_Config/#_8","text":"usmUser 1 3 0x80001f8880d03ab707afecd75700000000 \u201csnmpuser\u201d \u201csnmpuser\u201d NULL .1.3.6.1.6.3.10.1.1.3 0xc6910771242151aca36878ce38cda60d50b86d4a .1.3.6.1.6.3.10.1.2.4 0xb1248618de541a5995ea635aab1fb5b6 \u201c\u201d Pour tester la connexion snmp en local","title":""},{"location":"OS/Centreon/SNMP_Config/#_9","text":"snmpwalk -v 3 -u snmpuser -a SHA -A \u2018VuldpGEQTI4o2p\u2019 -x AES -X \u2018aRtz0OqaIcbHKs\u2019 -l authPriv localhost pour tester la connexion \u00e0 partir du serveur Centreon","title":""},{"location":"OS/Centreon/SNMP_Config/#_10","text":"/usr/lib/nagios/plugins/check_centreon_snmp_remote_storage -H bareos -n -d /backup-01 -w 80 -c 90 -v 3 -u snmpuser -p \u2018VuldpGEQTI4o2p\u2019 \u2013authprotocol SHA \u2013privpassword \u2018aRtz0OqaIcbHKs\u2019 \u2013privprotocol AES Pour finir ne pas oublier de d\u00e9sactiver les acc\u00e8s SNMP v1 et v2 en commentant les lignes suivantes","title":""},{"location":"OS/Centreon/SNMP_Config/#_11","text":"","title":""},{"location":"OS/Centreon/SNMP_Config/#rocommunity-public-127001","text":"","title":"rocommunity public 127.0.0.1"},{"location":"OS/Centreon/SNMP_Config/#rwcommunity-mysecret-127001","text":"","title":"rwcommunity mysecret 127.0.0.1"},{"location":"OS/Centreon/SNMP_Config/#com2sec-local-localhost-public","text":"","title":"com2sec local     localhost           public"},{"location":"OS/Centreon/SNMP_Config/#com2sec-mynetwork-1921688765-public","text":"","title":"com2sec mynetwork 192.168.87.65      public"},{"location":"OS/Centreon/SNMP_Config/#com2sec-mynetwork-1921688765-public_1","text":"Pare-Feu des \u00e9quipements \u00e0 monitorer","title":"com2sec mynetwork 192.168.87.65      public"},{"location":"OS/Centreon/SNMP_Config/#_12","text":"UDP : 161 et 162 Config SNMPv3 pour les PDU User Name : snmpuser Authentication Passphrase : VuldpGEQTI4o24ffrl845fGtfb8 Privacy Passphrase : aRtz0OqaIcbHKsQTI485dhJHt8s Config SNMPv3 sur les Synology","title":""},{"location":"OS/Centreon/SNMP_Config/#_13","text":"Nom d\u2019utilisateur : snmpuser Mot de passe : VuldpGEQTI4o2p Protocole d\u2019authentification : MD5 (pour info) Installer module Perl manquan pour la crypto","title":""},{"location":"OS/Centreon/SNMP_Config/#_14","text":"yum install -y epel-release.noarch yum install -y perl-Crypt-Rijndael Config commune des commandes","title":""},{"location":"OS/Centreon/SNMP_Config/#_15","text":"-v 3 -u $_SERVICESNMPUSERNAME$ -p $_SERVICESNMPPASSWORD$ --authprotocol $_SERVICESNMPAUTHPROTOCOL$ --privpassword $_SERVICESNMPPRIVPASSWORD$ --privprotocol $_SERVICESNMPPRIVPROTOCOL$ --snmp-timeout 60","title":""},{"location":"OS/Debian/Custom_image/","text":"Cr\u00e9er sa debian Cr\u00e9er sa propre distribution avec live-build Live-build est un petit outil permettant de construire des images pour cdrom ou usb de debian. Vous pourrez pr\u00e9-configurer cette \u00abdistribution perso\u00bb comme si c\u2019\u00e9tait la v\u00f4tre, l\u2019utiliser comme syst\u00e8me \u201clive\u201d, et aussi l\u2019installer telle quelle. Cette page tentera d\u2019apporter quelques \u00e9claircissements sur l\u2019utilisation de live-build, parfois obscure, en particulier sur la version 3. Sachez toutefois que la meilleure documentation se trouve sur votre ordinateur. Une fois live-build d\u2019install\u00e9, allez jeter un oeil \u00e0 la documentation pr\u00e9sente dans /usr/share/doc/live-manual/html/ . Installation Je vous conseille d\u2019installer la version disponible dans les d\u00e9p\u00f4ts. Pour une version plus r\u00e9cente si vous \u00eates en \u201coldstable\u201d, utilisez les d\u00e9pots suivants : deb http://live.debian.net/ wheezy-snapshots main contrib non-free Puis installez live-build, live-manual et live-tools. apt-get update && apt-get install live-build live-manual live-tools Pr\u00e9parons le travail On va cr\u00e9er un r\u00e9pertoire de travail, s\u2019y d\u00e9placer, puis cr\u00e9er l\u2019arbre de live build pour la suite : mkdir masuperdebian cd masuperdebian lb config D\u00e9sormais sont pr\u00e9sent dans ce dossier de nouveaux r\u00e9pertoires, tels que config. Configuration La configuration se r\u00e9alise par l\u2019\u00e9dition de plusieurs fichiers pr\u00e9sents dans le r\u00e9pertoire config : binary, bootstrap, chroot, common. Cependant, comme les options de live-build peuvent changer lors du changement de version, nous allons plut\u00f4t utiliser la ligne de commande pour faire \u00e7a. Autrement dit, lorsque lb config sera lanc\u00e9, il appliquera automatiquement certains r\u00e9glages. Tout d\u2019abord, copiez les scripts clean, config et build pr\u00e9sents dans /usr/share/live/build/examples/auto/ dans le r\u00e9pertoire auto du dossier masuperdebian : cp /usr/share/doc/live-build/examples/auto/* auto/ Avec une ancienne version, c\u2019\u00e9tait cp /usr/share/live/build/examples/auto/* auto/. D\u00e9sormais, ce seront ces scripts qui seront utilis\u00e9s lors des lb build, live config, etc. Nous les modifierons directement, afin d\u2019\u00e9viter des lignes de commande \u00e0 rallonge. Autrement dit, lorsque la commande lb config sera de nouveau ex\u00e9cut\u00e9e, ce sera le script pr\u00e9sent dans /auto/config qui sera lanc\u00e9. Voyons donc comment configurer le tout, en modifiant le fichier auto/config. Tout d\u2019abord, voici \u00e0 quoi il ressemble par d\u00e9faut : 1 2 3 #!/bin/sh lb config noauto \\ \" ${ @ } \" Nous allons rajouter des options selon les besoin \u00e0 la suite. Le caract\u00e8re permet de passer \u00e0 la ligne sans souci. Les options seront donc rajout\u00e9es entre ces deux lignes : 1 2 3 4 5 6 #!/bin/sh lb config noauto \\ # AJOUTER DES OPTIONS ICI \\ # ICI AUSSI \\ # ET ENCORE ICI SI VOUS VOULEZ \\ \" ${ @ } \" Installer une liste de paquets personnalis\u00e9e Cr\u00e9ez un fichier contenant la liste des paquets \u00e0 installer, dans le dossier config/package-lists. Attention : le fichier doit avoir l\u2019extension .list.chroot . Par exemple : maliste.list.chroot. Remplissez ce fichier avec les paquets vous int\u00e9ressant. Pr\u00e9parer les fichiers de configuration de l\u2019utilisateur (le home) Vous souhaitez avoir vos marques pages de navigateur d\u00e9j\u00e0 pr\u00eat? Utiliser un th\u00e8me graphique pr\u00e9cis? Des raccourcis d\u00e9j\u00e0 tout pr\u00eats? Il s\u2019agit de la configuration de l\u2019utilisateur, g\u00e9n\u00e9ralement pr\u00e9sente sous forme de fichier cach\u00e9 dans le /home/. Pour avoir tout \u00e7a de d\u00e9j\u00e0 pr\u00eat, on va utiliser le dossier config/includes.chroot . Dans ce dernier, cr\u00e9ez un dossier etc/skel . Il s\u2019agit du dossier contenant tout ce qu\u2019un utilisateur a dans son dossier personnel lorsqu\u2019il est cr\u00e9\u00e9. Copiez dans le config/includes.chroot/etc/skel les fichiers de configuration, comme un .bashrc, ou un .config/openbox \u2026 Tout ce que vous voulez! L\u2019ensemble du contenu de ce dossier sera rajout\u00e9 \u00e0 votre syst\u00e8me personnalis\u00e9. Bien s\u00fbr, cette fonctionnalit\u00e9 fonctionne pour tout, pas seulement pour /etc/skel. note en passant : L\u2019utilisateur par d\u00e9faut s\u2019appelle user Un syst\u00e8me en fran\u00e7ais Dans auto/config, ajoutez ces options : \u2013bootappend-live \u201clocales=fr_FR.UTF-8 keyboard-layouts=fr\u201d \\ \u2013bootappend-install \u201clocales=fr_FR.UTF-8\u201d \\ Pr\u00e9ciser une autre distribution Vous pouvez construire sid, ou oldstable, avec l\u2019option \u2013distribution : \u2013distribution \u201cwheezy\u201d \\ Utiliser autre chose que main Vous pouvez ajouter les sections contrib et non-free (bouh!): \u2013archive-areas \u201cmain contrib non-free\u201d \\ Installer un clone du syt\u00e8me pr\u00e9configur\u00e9 Pour installer le syst\u00e8me tel qu\u2019il est sur votre cl\u00e9 sur un ordinateur, voici l\u2019option \u00e0 utiliser : \u2013debian-installer \u201clive\u201d \\ Puis assurez vous d\u2019installer le paquet debian-installer-launcher dans la liste des paquets \u00e0 installer. Pour une cl\u00e9 usb, pas une iso cdrom --binary-images \"hdd\" \\ Configurer l\u2019utilisateur (nom d\u2019utilisateur, groupes) Pour configurer diff\u00e9rents aspects de la session utilisateur, cela se passe en modifiant les param\u00e8tres de d\u00e9marrage, c\u2019est \u00e0 dire en rajoutant des choses dans la partie apr\u00e8s \u201cboot\u201d de cette ligne \u2013bootappend-live \u201clocales=fr_FR.UTF-8 keyboard-layouts=fr boot=live\u201d \\ Ainsi, pour changer les groupes et par exemple ajouter l\u2019utilisateur au groupe fuse, rajoutez : live-config.user-default-groups=audio,cdrom,dip,floppy,video,plugdev,netdev,powerdev,scanner,bluetooth,fuse` Ce qui donne : \u2013bootappend-live \u201clocales=fr_FR.UTF-8 keyboard-layouts=fr boot=live user-default-groups=audio,cdrom,dip,floppy,video,plugdev,netdev,powerdev,scanner,bluetooth,fuse\u201d \\ Pour modifier le nom d\u2019utilisateur, c\u2019est avec username=nom_d_utilisateur Lancer des scripts pour configurer le syst\u00e8me Vous pouvez avoir besoin de lancer des scripts suppl\u00e9mentaire pour personnaliser un peu plus le syst\u00e8me. Ces scripts doivent \u00eatre lanc\u00e9s dans le chroot, avant que l\u2019image soit construite. Heureusement, tout est d\u00e9j\u00e0 pr\u00e9vu. Il suffit de placer ces scripts dans le dossier config/hooks. Attention, ces scripts doivent avoir l\u2019extension .chroot. Par exemple, pour changer l\u2019alternative par d\u00e9faut du terminal, on peut cr\u00e9er un script alternatives.chroot : !/bin/sh set -e update-alternatives --install /usr/bin/x-terminal-emulator x-terminal-emulator /usr/bin/urxvtcd 90 Dans le syst\u00e8me, ce sera urxvtcd qui sera le terminal par d\u00e9faut gr\u00e2ce \u00e0 ce script. Persistence des donn\u00e9es Si vous souhaitez retrouvez vos donn\u00e9es entre chaque d\u00e9marrage, pour pouvez cr\u00e9er ce qu\u2019on appelle un live persistant. Pour cela, assurez-vous d\u2019ajouter cette option : \u2013bootappend-live \u201cpersistence\u201d Ce qui peut donner avec d\u2019autres param\u00e8tres : \u2013bootappend-live \u201cpersistence locales=fr_FR.UTF-8 keyboard-layouts=fr boot=live\u201d \\ Je passe rapidement sur la suite des d\u00e9tails puisqu\u2019ils sont d\u00e9taill\u00e9s ensuite : * Construction de l\u2019image : lb config && lb build * Copie de l\u2019image : # dd if=binary.hybrid.iso of=/dev/sdb On cr\u00e9e ensuite une nouvelle partition sur la cl\u00e9 avec cfdisk ou gparted. Ensuite, on formate cette partition, par exemple en ext4. Le point important ici est de donner le label \u201cpersistence\u201d \u00e0 cette partition : mkfs.ext4 -L persistence /dev/sdb2 Pour finir, on va cr\u00e9er un fichier persistence.conf (anciennement live-persistence.conf) dans cette partition, et y pr\u00e9ciser quel dossier on veut garder pour les d\u00e9marrages suivants. Ici, on choisit de conserver le /home, qui contient toutes les donn\u00e9es des utilisateurs : Montage de la partition : # mount -t ext4 /dev/sdb2 /mnt Cr\u00e9ation du fichier persistence.conf avec l\u2019option voulue : # echo \u201c/home\u201d >> /mnt/persistence.conf D\u00e9montage de la cl\u00e9 : #umount /mnt Et voil\u00e0, lors du premier d\u00e9marrage sur la cl\u00e9, /home sera copi\u00e9 sur la partition /dev/sdb2, autrement dit la deuxi\u00e8me partition de la cl\u00e9. \u00c0 chaque red\u00e9marrage suivant, vous retrouverez les changements r\u00e9alis\u00e9s au dernier lancement, sans autre manipulation. Un autre exemple de fichier persistence.conf pour garder aussi les \u00e9ventuels programmes install\u00e9s \u00e0 posteriori sur la cl\u00e9, ainsi que la configuration de l\u2019utilisateur (merci LeDub) : /usr union /home /var/cache/apt Construction de l\u2019image Il suffit de lancer, toujours dans le r\u00e9pertoire masuperdebian : lb config lb build lb config pr\u00e9pare l\u2019image selon les options d\u00e9finies dans le fichier auto/config, et lb build fabrique l\u2019image. Gravure/ copie sur usb Une image iso est maintenant disponible dans le r\u00e9pertoire masuperdebian. Si vous avez choisi le format usb, vous pouvez copier le tout sur une cl\u00e9 de cette fa\u00e7on : dd if=binary.img of=/dev/sdb o\u00f9 binary.img est l\u2019image de votre debian personnalis\u00e9e, et /dev/sdb est le chemin vers votre cl\u00e9 usb. Attention, ce n\u2019est pas /dev/sdb1 ou /dev/sdc2, mais seulement /dev/sdb. De plus, tout sera effac\u00e9 sur votre cl\u00e9. Assurez-vous de copier sur le bon p\u00e9riph\u00e9rique! Il se peut aussi que vous ayez plut\u00f4t binary.hybrid.iso, ce n\u2019est pas un probl\u00e8me, il s\u2019agit juste d\u2019un format pouvant servir \u00e0 la fois pour les cl\u00e9 usb que pour les cdroms. On recommence La derni\u00e8re image ne correspondait pas \u00e0 vos attentes? On recommence alors. Tout d\u2019abord, on nettoie le tout : lb clean Puis vous appliquez vos changements de configuration, et ensuite : lb config on recommence au d\u00e9but. Rendre les constructions futures plus rapides Si vous planifiez de construire des ISO r\u00e9guli\u00e8rement, une bonne id\u00e9e serait de mettre en cache les paquets localement. Installez simplement apt-cacher-ng et configurez la variable d\u2019environnement http_proxy avant la construction: apt-get install apt-cacher-ng /etc/init.d/apt-cacher-ng start export http_proxy=http://localhost:3142/ .... # setup and configure your live build lb config --apt-http-proxy http://127.0.0.1:3142/ lb build D\u2019apr\u00e8s la documentation de Kali linux Exemple 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #!/bin/sh lb config noauto \\ --architectures \"i386\" \\ --linux-flavours \"686\" \\ --bootappend-live \"locales=fr_FR.UTF-8 keyboard-layouts=fr\" \\ --bootappend-install \"locales=fr_FR.UTF-8\" \\ --binary-images \"hdd\" \\ --distribution \"wheezy\" \\ --archive-areas \"main contrib non-free\" \\ --apt-indices \"false\" \\ --apt-recommends \"false\" \\ --includes \"none\" \\ --memtest \"none\" \\ --win32-loader \"false\" \\ --source \"false\" \\ --debug \\ \" ${ @ } \" Liste des options disponibles \u00e0 lb config Pour conna\u00eetre toutes les options pouvant \u00eatre ajout\u00e9es au fichier auto/config, tapez la commande man lb_config, et vous obtiendrez quelque chose du genre : [--apt apt|aptitude] [--apt-ftp-proxy URL] [--apt-http-proxy URL] [--apt-indices true|false|none] [--apt-options OPTION|\"OPTIONS\"] [--aptitude-options OPTION|\"OPTIONS\"] [--apt-pipeline DEPTH] [--apt-recommends true|false] [--apt-secure true|false] [--apt-source-archives true|false] [-a|--architectures ARCHITECTURE] [-b|--binary-images iso|iso-hybrid|net|tar|hdd|virtual-hdd] [--binary-filesystem fat16|fat32|ext2|ext3|ext4] [--bootappend-install PARAMETER|\"PARAMETERS\"] [--bootappend-live PARAMETER|\"PARAMETERS\"] [--bootloader grub|syslinux|yaboot] [--bootstrap cdebootstrap|cdebootstrap-static|debootstrap|copy] [-f|--bootstrap-flavour minimal|standard] [--bootstrap-keyring PACKAGE] [--cache true|false] [--cache-indices true|false] [--cache-packages true|false] [--cache-stages STAGE|\"STAGES\"] [--checksums md5|sha1|sha256|none] [--compression bzip2|gzip|lzip|none] [--build-with-chroot true|false] [--chroot-filesystem ext2|ext3|ext4|squashfs|jffs2|none] [--clean] [-c|--conffile FILE] [--debconf-frontend dialog|editor|noninteractive|readline] [--debconf-nowarnings true|false] [--debconf-priority low|medium|high|critical] [--debian-installer true|cdrom|netinst|netboot|businesscard|live|false] [--debian-installer-distribution daily|CODENAME] [--debian-installer-preseedfile FILE|URL] [--debian-installer-gui true|false] [--debug] [-d|--distribution CODENAME] [--parent-distribution CODENAME] [--parent-debian-installer-distribution CODENAME] R\u00e9f\u00e9rences * https://debian-live.alioth.debian.org/live-manual/stable/manual/html/live-manual.en.html#117 * http://www.esdebian.org/wiki/live-helper * http://live.debian.net/ * http://live.debian.net/devel/live-build/","title":"Custom image"},{"location":"OS/Debian/Custom_image/#creer-sa-debian","text":"","title":"Cr\u00e9er sa debian"},{"location":"OS/Debian/Custom_image/#creer-sa-propre-distribution-avec-live-build","text":"Live-build est un petit outil permettant de construire des images pour cdrom ou usb de debian. Vous pourrez pr\u00e9-configurer cette \u00abdistribution perso\u00bb comme si c\u2019\u00e9tait la v\u00f4tre, l\u2019utiliser comme syst\u00e8me \u201clive\u201d, et aussi l\u2019installer telle quelle. Cette page tentera d\u2019apporter quelques \u00e9claircissements sur l\u2019utilisation de live-build, parfois obscure, en particulier sur la version 3. Sachez toutefois que la meilleure documentation se trouve sur votre ordinateur. Une fois live-build d\u2019install\u00e9, allez jeter un oeil \u00e0 la documentation pr\u00e9sente dans /usr/share/doc/live-manual/html/ .","title":"Cr\u00e9er sa propre distribution avec live-build"},{"location":"OS/Debian/Custom_image/#installation","text":"Je vous conseille d\u2019installer la version disponible dans les d\u00e9p\u00f4ts. Pour une version plus r\u00e9cente si vous \u00eates en \u201coldstable\u201d, utilisez les d\u00e9pots suivants : deb http://live.debian.net/ wheezy-snapshots main contrib non-free Puis installez live-build, live-manual et live-tools. apt-get update && apt-get install live-build live-manual live-tools","title":"Installation"},{"location":"OS/Debian/Custom_image/#preparons-le-travail","text":"On va cr\u00e9er un r\u00e9pertoire de travail, s\u2019y d\u00e9placer, puis cr\u00e9er l\u2019arbre de live build pour la suite : mkdir masuperdebian cd masuperdebian lb config D\u00e9sormais sont pr\u00e9sent dans ce dossier de nouveaux r\u00e9pertoires, tels que config.","title":"Pr\u00e9parons le travail"},{"location":"OS/Debian/Custom_image/#configuration","text":"La configuration se r\u00e9alise par l\u2019\u00e9dition de plusieurs fichiers pr\u00e9sents dans le r\u00e9pertoire config : binary, bootstrap, chroot, common. Cependant, comme les options de live-build peuvent changer lors du changement de version, nous allons plut\u00f4t utiliser la ligne de commande pour faire \u00e7a. Autrement dit, lorsque lb config sera lanc\u00e9, il appliquera automatiquement certains r\u00e9glages. Tout d\u2019abord, copiez les scripts clean, config et build pr\u00e9sents dans /usr/share/live/build/examples/auto/ dans le r\u00e9pertoire auto du dossier masuperdebian : cp /usr/share/doc/live-build/examples/auto/* auto/ Avec une ancienne version, c\u2019\u00e9tait cp /usr/share/live/build/examples/auto/* auto/. D\u00e9sormais, ce seront ces scripts qui seront utilis\u00e9s lors des lb build, live config, etc. Nous les modifierons directement, afin d\u2019\u00e9viter des lignes de commande \u00e0 rallonge. Autrement dit, lorsque la commande lb config sera de nouveau ex\u00e9cut\u00e9e, ce sera le script pr\u00e9sent dans /auto/config qui sera lanc\u00e9. Voyons donc comment configurer le tout, en modifiant le fichier auto/config. Tout d\u2019abord, voici \u00e0 quoi il ressemble par d\u00e9faut : 1 2 3 #!/bin/sh lb config noauto \\ \" ${ @ } \" Nous allons rajouter des options selon les besoin \u00e0 la suite. Le caract\u00e8re permet de passer \u00e0 la ligne sans souci. Les options seront donc rajout\u00e9es entre ces deux lignes : 1 2 3 4 5 6 #!/bin/sh lb config noauto \\ # AJOUTER DES OPTIONS ICI \\ # ICI AUSSI \\ # ET ENCORE ICI SI VOUS VOULEZ \\ \" ${ @ } \"","title":"Configuration"},{"location":"OS/Debian/Custom_image/#installer-une-liste-de-paquets-personnalisee","text":"Cr\u00e9ez un fichier contenant la liste des paquets \u00e0 installer, dans le dossier config/package-lists. Attention : le fichier doit avoir l\u2019extension .list.chroot . Par exemple : maliste.list.chroot. Remplissez ce fichier avec les paquets vous int\u00e9ressant.","title":"Installer une liste de paquets personnalis\u00e9e"},{"location":"OS/Debian/Custom_image/#preparer-les-fichiers-de-configuration-de-lutilisateur-le-home","text":"Vous souhaitez avoir vos marques pages de navigateur d\u00e9j\u00e0 pr\u00eat? Utiliser un th\u00e8me graphique pr\u00e9cis? Des raccourcis d\u00e9j\u00e0 tout pr\u00eats? Il s\u2019agit de la configuration de l\u2019utilisateur, g\u00e9n\u00e9ralement pr\u00e9sente sous forme de fichier cach\u00e9 dans le /home/. Pour avoir tout \u00e7a de d\u00e9j\u00e0 pr\u00eat, on va utiliser le dossier config/includes.chroot . Dans ce dernier, cr\u00e9ez un dossier etc/skel . Il s\u2019agit du dossier contenant tout ce qu\u2019un utilisateur a dans son dossier personnel lorsqu\u2019il est cr\u00e9\u00e9. Copiez dans le config/includes.chroot/etc/skel les fichiers de configuration, comme un .bashrc, ou un .config/openbox \u2026 Tout ce que vous voulez! L\u2019ensemble du contenu de ce dossier sera rajout\u00e9 \u00e0 votre syst\u00e8me personnalis\u00e9. Bien s\u00fbr, cette fonctionnalit\u00e9 fonctionne pour tout, pas seulement pour /etc/skel. note en passant : L\u2019utilisateur par d\u00e9faut s\u2019appelle user Un syst\u00e8me en fran\u00e7ais Dans auto/config, ajoutez ces options : \u2013bootappend-live \u201clocales=fr_FR.UTF-8 keyboard-layouts=fr\u201d \\ \u2013bootappend-install \u201clocales=fr_FR.UTF-8\u201d \\","title":"Pr\u00e9parer les fichiers de configuration de l\u2019utilisateur (le home)"},{"location":"OS/Debian/Custom_image/#preciser-une-autre-distribution","text":"Vous pouvez construire sid, ou oldstable, avec l\u2019option \u2013distribution : \u2013distribution \u201cwheezy\u201d \\","title":"Pr\u00e9ciser une autre distribution"},{"location":"OS/Debian/Custom_image/#utiliser-autre-chose-que-main","text":"Vous pouvez ajouter les sections contrib et non-free (bouh!): \u2013archive-areas \u201cmain contrib non-free\u201d \\","title":"Utiliser autre chose que main"},{"location":"OS/Debian/Custom_image/#installer-un-clone-du-syteme-preconfigure","text":"Pour installer le syst\u00e8me tel qu\u2019il est sur votre cl\u00e9 sur un ordinateur, voici l\u2019option \u00e0 utiliser : \u2013debian-installer \u201clive\u201d \\ Puis assurez vous d\u2019installer le paquet debian-installer-launcher dans la liste des paquets \u00e0 installer.","title":"Installer un clone du syt\u00e8me pr\u00e9configur\u00e9"},{"location":"OS/Debian/Custom_image/#pour-une-cle-usb-pas-une-iso-cdrom","text":"--binary-images \"hdd\" \\","title":"Pour une cl\u00e9 usb, pas une iso cdrom"},{"location":"OS/Debian/Custom_image/#configurer-lutilisateur-nom-dutilisateur-groupes","text":"Pour configurer diff\u00e9rents aspects de la session utilisateur, cela se passe en modifiant les param\u00e8tres de d\u00e9marrage, c\u2019est \u00e0 dire en rajoutant des choses dans la partie apr\u00e8s \u201cboot\u201d de cette ligne \u2013bootappend-live \u201clocales=fr_FR.UTF-8 keyboard-layouts=fr boot=live\u201d \\ Ainsi, pour changer les groupes et par exemple ajouter l\u2019utilisateur au groupe fuse, rajoutez : live-config.user-default-groups=audio,cdrom,dip,floppy,video,plugdev,netdev,powerdev,scanner,bluetooth,fuse` Ce qui donne : \u2013bootappend-live \u201clocales=fr_FR.UTF-8 keyboard-layouts=fr boot=live user-default-groups=audio,cdrom,dip,floppy,video,plugdev,netdev,powerdev,scanner,bluetooth,fuse\u201d \\ Pour modifier le nom d\u2019utilisateur, c\u2019est avec username=nom_d_utilisateur","title":"Configurer l'utilisateur (nom d'utilisateur, groupes)"},{"location":"OS/Debian/Custom_image/#lancer-des-scripts-pour-configurer-le-systeme","text":"Vous pouvez avoir besoin de lancer des scripts suppl\u00e9mentaire pour personnaliser un peu plus le syst\u00e8me. Ces scripts doivent \u00eatre lanc\u00e9s dans le chroot, avant que l\u2019image soit construite. Heureusement, tout est d\u00e9j\u00e0 pr\u00e9vu. Il suffit de placer ces scripts dans le dossier config/hooks. Attention, ces scripts doivent avoir l\u2019extension .chroot. Par exemple, pour changer l\u2019alternative par d\u00e9faut du terminal, on peut cr\u00e9er un script alternatives.chroot : !/bin/sh set -e update-alternatives --install /usr/bin/x-terminal-emulator x-terminal-emulator /usr/bin/urxvtcd 90 Dans le syst\u00e8me, ce sera urxvtcd qui sera le terminal par d\u00e9faut gr\u00e2ce \u00e0 ce script.","title":"Lancer des scripts pour configurer le syst\u00e8me"},{"location":"OS/Debian/Custom_image/#persistence-des-donnees","text":"Si vous souhaitez retrouvez vos donn\u00e9es entre chaque d\u00e9marrage, pour pouvez cr\u00e9er ce qu\u2019on appelle un live persistant. Pour cela, assurez-vous d\u2019ajouter cette option : \u2013bootappend-live \u201cpersistence\u201d Ce qui peut donner avec d\u2019autres param\u00e8tres : \u2013bootappend-live \u201cpersistence locales=fr_FR.UTF-8 keyboard-layouts=fr boot=live\u201d \\ Je passe rapidement sur la suite des d\u00e9tails puisqu\u2019ils sont d\u00e9taill\u00e9s ensuite : * Construction de l\u2019image : lb config && lb build * Copie de l\u2019image : # dd if=binary.hybrid.iso of=/dev/sdb On cr\u00e9e ensuite une nouvelle partition sur la cl\u00e9 avec cfdisk ou gparted. Ensuite, on formate cette partition, par exemple en ext4. Le point important ici est de donner le label \u201cpersistence\u201d \u00e0 cette partition : mkfs.ext4 -L persistence /dev/sdb2 Pour finir, on va cr\u00e9er un fichier persistence.conf (anciennement live-persistence.conf) dans cette partition, et y pr\u00e9ciser quel dossier on veut garder pour les d\u00e9marrages suivants. Ici, on choisit de conserver le /home, qui contient toutes les donn\u00e9es des utilisateurs : Montage de la partition : # mount -t ext4 /dev/sdb2 /mnt Cr\u00e9ation du fichier persistence.conf avec l\u2019option voulue : # echo \u201c/home\u201d >> /mnt/persistence.conf D\u00e9montage de la cl\u00e9 : #umount /mnt Et voil\u00e0, lors du premier d\u00e9marrage sur la cl\u00e9, /home sera copi\u00e9 sur la partition /dev/sdb2, autrement dit la deuxi\u00e8me partition de la cl\u00e9. \u00c0 chaque red\u00e9marrage suivant, vous retrouverez les changements r\u00e9alis\u00e9s au dernier lancement, sans autre manipulation. Un autre exemple de fichier persistence.conf pour garder aussi les \u00e9ventuels programmes install\u00e9s \u00e0 posteriori sur la cl\u00e9, ainsi que la configuration de l\u2019utilisateur (merci LeDub) : /usr union /home /var/cache/apt","title":"Persistence des donn\u00e9es"},{"location":"OS/Debian/Custom_image/#construction-de-limage","text":"Il suffit de lancer, toujours dans le r\u00e9pertoire masuperdebian : lb config lb build lb config pr\u00e9pare l\u2019image selon les options d\u00e9finies dans le fichier auto/config, et lb build fabrique l\u2019image.","title":"Construction de l\u2019image"},{"location":"OS/Debian/Custom_image/#gravure-copie-sur-usb","text":"Une image iso est maintenant disponible dans le r\u00e9pertoire masuperdebian. Si vous avez choisi le format usb, vous pouvez copier le tout sur une cl\u00e9 de cette fa\u00e7on : dd if=binary.img of=/dev/sdb o\u00f9 binary.img est l\u2019image de votre debian personnalis\u00e9e, et /dev/sdb est le chemin vers votre cl\u00e9 usb. Attention, ce n\u2019est pas /dev/sdb1 ou /dev/sdc2, mais seulement /dev/sdb. De plus, tout sera effac\u00e9 sur votre cl\u00e9. Assurez-vous de copier sur le bon p\u00e9riph\u00e9rique! Il se peut aussi que vous ayez plut\u00f4t binary.hybrid.iso, ce n\u2019est pas un probl\u00e8me, il s\u2019agit juste d\u2019un format pouvant servir \u00e0 la fois pour les cl\u00e9 usb que pour les cdroms. On recommence La derni\u00e8re image ne correspondait pas \u00e0 vos attentes? On recommence alors. Tout d\u2019abord, on nettoie le tout : lb clean Puis vous appliquez vos changements de configuration, et ensuite : lb config on recommence au d\u00e9but.","title":"Gravure/ copie sur usb"},{"location":"OS/Debian/Custom_image/#rendre-les-constructions-futures-plus-rapides","text":"Si vous planifiez de construire des ISO r\u00e9guli\u00e8rement, une bonne id\u00e9e serait de mettre en cache les paquets localement. Installez simplement apt-cacher-ng et configurez la variable d\u2019environnement http_proxy avant la construction: apt-get install apt-cacher-ng /etc/init.d/apt-cacher-ng start export http_proxy=http://localhost:3142/ .... # setup and configure your live build lb config --apt-http-proxy http://127.0.0.1:3142/ lb build D\u2019apr\u00e8s la documentation de Kali linux","title":"Rendre les constructions futures plus rapides"},{"location":"OS/Debian/Custom_image/#exemple","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #!/bin/sh lb config noauto \\ --architectures \"i386\" \\ --linux-flavours \"686\" \\ --bootappend-live \"locales=fr_FR.UTF-8 keyboard-layouts=fr\" \\ --bootappend-install \"locales=fr_FR.UTF-8\" \\ --binary-images \"hdd\" \\ --distribution \"wheezy\" \\ --archive-areas \"main contrib non-free\" \\ --apt-indices \"false\" \\ --apt-recommends \"false\" \\ --includes \"none\" \\ --memtest \"none\" \\ --win32-loader \"false\" \\ --source \"false\" \\ --debug \\ \" ${ @ } \" Liste des options disponibles \u00e0 lb config Pour conna\u00eetre toutes les options pouvant \u00eatre ajout\u00e9es au fichier auto/config, tapez la commande man lb_config, et vous obtiendrez quelque chose du genre : [--apt apt|aptitude] [--apt-ftp-proxy URL] [--apt-http-proxy URL] [--apt-indices true|false|none] [--apt-options OPTION|\"OPTIONS\"] [--aptitude-options OPTION|\"OPTIONS\"] [--apt-pipeline DEPTH] [--apt-recommends true|false] [--apt-secure true|false] [--apt-source-archives true|false] [-a|--architectures ARCHITECTURE] [-b|--binary-images iso|iso-hybrid|net|tar|hdd|virtual-hdd] [--binary-filesystem fat16|fat32|ext2|ext3|ext4] [--bootappend-install PARAMETER|\"PARAMETERS\"] [--bootappend-live PARAMETER|\"PARAMETERS\"] [--bootloader grub|syslinux|yaboot] [--bootstrap cdebootstrap|cdebootstrap-static|debootstrap|copy] [-f|--bootstrap-flavour minimal|standard] [--bootstrap-keyring PACKAGE] [--cache true|false] [--cache-indices true|false] [--cache-packages true|false] [--cache-stages STAGE|\"STAGES\"] [--checksums md5|sha1|sha256|none] [--compression bzip2|gzip|lzip|none] [--build-with-chroot true|false] [--chroot-filesystem ext2|ext3|ext4|squashfs|jffs2|none] [--clean] [-c|--conffile FILE] [--debconf-frontend dialog|editor|noninteractive|readline] [--debconf-nowarnings true|false] [--debconf-priority low|medium|high|critical] [--debian-installer true|cdrom|netinst|netboot|businesscard|live|false] [--debian-installer-distribution daily|CODENAME] [--debian-installer-preseedfile FILE|URL] [--debian-installer-gui true|false] [--debug] [-d|--distribution CODENAME] [--parent-distribution CODENAME] [--parent-debian-installer-distribution CODENAME] R\u00e9f\u00e9rences * https://debian-live.alioth.debian.org/live-manual/stable/manual/html/live-manual.en.html#117 * http://www.esdebian.org/wiki/live-helper * http://live.debian.net/ * http://live.debian.net/devel/live-build/","title":"Exemple"},{"location":"OS/Debian/Tricks/","text":"Debian-tricks If you\u2019d like to prevent daemons from starting after installing a package, just toss a few lines into /usr/sbin/policy-rc.d: cat > /usr/sbin/policy-rc.d << EOF #!/bin/sh echo \"All runlevel operations denied by policy\" >&2 exit 101 EOF","title":"Tricks"},{"location":"OS/Debian/Tricks/#debian-tricks","text":"If you\u2019d like to prevent daemons from starting after installing a package, just toss a few lines into /usr/sbin/policy-rc.d: cat > /usr/sbin/policy-rc.d << EOF #!/bin/sh echo \"All runlevel operations denied by policy\" >&2 exit 101 EOF","title":"Debian-tricks"},{"location":"OS/Debian/apt_conf/","text":"Configuration APT Ne jamais utiliser les recommends cat > /etc/apt/apt.conf.d/01norecommend << EOF APT::Install-Recommends \"0\"; APT::Install-Suggests \"0\"; EOF","title":"Configuration APT"},{"location":"OS/Debian/apt_conf/#configuration-apt","text":"","title":"Configuration APT"},{"location":"OS/Debian/apt_conf/#ne-jamais-utiliser-les-recommends","text":"cat > /etc/apt/apt.conf.d/01norecommend << EOF APT::Install-Recommends \"0\"; APT::Install-Suggests \"0\"; EOF","title":"Ne jamais utiliser les recommends"},{"location":"OS/Fedora/Manage_repositories/","text":"Configuring with the command line See a list of all enabled repos sudo dnf repolist Change configuration for just one command. To enable or disable a repo just once, use a command option: sudo dnf --enablerepo=<reponame>... sudo dnf --disablerepo=<reponame>... For instance, to install the latest kernel from Fedora\u2019s test repo: sudo dnf --enablerepo=updates-testing install kernel\\* You can combine several enable and disable options together. For example: sudo dnf --enablerepo=repo1 --disablerepo=repo2,repo3 install <package> If you want to change the defaults permanently, use these commands: sudo dnf config-manager --set-enabled <reponame> sudo dnf config-manager --set-disabled <reponame> Perhaps you install, update, or remove a lot of software using different setups. In this case, things may get confusing. You might not know which software is installed from what repos. If that happens, try this. First, disable extra repos such as those ending in \u2013testing. Ideally, enable only fedora and updates repos. Run this command for each unwanted repo: sudo dnf config-manager --set-disabled <unwanted-repo> Then run this command to synchronize your system with just stable, updated packages: sudo dnf distro-sync This ensures your Fedora system is only using the latest packages from specific repos.","title":"Manage repositories"},{"location":"OS/Fedora/Manage_repositories/#configuring-with-the-command-line","text":"","title":"Configuring with the command line"},{"location":"OS/Fedora/Manage_repositories/#see-a-list-of-all-enabled-repos","text":"sudo dnf repolist","title":"See a list of all enabled repos"},{"location":"OS/Fedora/Manage_repositories/#change-configuration-for-just-one-command-to-enable-or-disable-a-repo-just-once-use-a-command-option","text":"sudo dnf --enablerepo=<reponame>... sudo dnf --disablerepo=<reponame>...","title":"Change configuration for just one command. To enable or disable a repo just once, use a command option:"},{"location":"OS/Fedora/Manage_repositories/#for-instance-to-install-the-latest-kernel-from-fedoras-test-repo","text":"sudo dnf --enablerepo=updates-testing install kernel\\*","title":"For instance, to install the latest kernel from Fedora\u2019s test repo:"},{"location":"OS/Fedora/Manage_repositories/#you-can-combine-several-enable-and-disable-options-together-for-example","text":"sudo dnf --enablerepo=repo1 --disablerepo=repo2,repo3 install <package>","title":"You can combine several enable and disable options together. For example:"},{"location":"OS/Fedora/Manage_repositories/#if-you-want-to-change-the-defaults-permanently-use-these-commands","text":"sudo dnf config-manager --set-enabled <reponame> sudo dnf config-manager --set-disabled <reponame> Perhaps you install, update, or remove a lot of software using different setups. In this case, things may get confusing. You might not know which software is installed from what repos. If that happens, try this.","title":"If you want to change the defaults permanently, use these commands:"},{"location":"OS/Fedora/Manage_repositories/#first-disable-extra-repos-such-as-those-ending-in-testing-ideally-enable-only-fedora-and-updates-repos-run-this-command-for-each-unwanted-repo","text":"sudo dnf config-manager --set-disabled <unwanted-repo>","title":"First, disable extra repos such as those ending in \u2013testing. Ideally, enable only fedora and updates repos. Run this command for each unwanted repo:"},{"location":"OS/Fedora/Manage_repositories/#then-run-this-command-to-synchronize-your-system-with-just-stable-updated-packages","text":"sudo dnf distro-sync This ensures your Fedora system is only using the latest packages from specific repos.","title":"Then run this command to synchronize your system with just stable, updated packages:"},{"location":"PAM_SSSD/Disable_fprintd/","text":"Disable fprintd Issue Problem was found in CentOS, GDM was wainting for service to start before accepting password input. Log excerpt juin 23 19 : 09 : 48 . net dbus - daemon [ 1338 ] : [ system ] Activating via systemd : service name = 'net.reactivated.Fprint' unit = 'fprintd.service' requested by ':1.218' ( uid = 1573400 > juin 23 19 : 09 : 48 . net systemd [ 1 ] : Starting Fingerprint Authentication Daemon ... juin 23 19 : 09 : 48 . net dbus - daemon [ 1338 ] : [ system ] Successfully activated service 'net.reactivated.Fprint' juin 23 19 : 09 : 48 . net systemd [ 1 ] : Started Fingerprint Authentication Daemon . Service status [ root . net ~ ] systemctl status fprintd . service \u25cf fprintd . service - Fingerprint Authentication Daemon Loaded : loaded (/ usr / lib / systemd / system / fprintd . service ; static ; vendor preset : disabled ) Active : active ( running ) since Tue 2020-06-23 19 : 11 : 22 CEST ; 24s ago Docs : man : fprintd ( 1 ) Main PID : 5801 ( fprintd ) Tasks : 3 ( limit : 48352 ) Memory : 3 . 8M CGroup : / system . slice / fprintd . service \u2514\u2500 5801 / usr / libexec / fprintd juin 23 19 : 11 : 22 . net systemd [ 1 ] : Starting Fingerprint Authentication Daemon ... juin 23 19 : 11 : 22 . net systemd [ 1 ] : Started Fingerprint Authentication Daemon . Solution [ root . net ~ ] authconfig --disablefingerprint --update Running authconfig compatibility tool . The purpose of this tool is to enable authentication against chosen services with authselect and minimum configuration . It does not provide all capabilities of authconfig . IMPORTANT : authconfig is replaced by authselect , please update your scripts . See man authselect - migration ( 7 ) to help you with migration to authselect Executing : / usr / bin / authselect check Executing : / usr / bin / authselect select sssd --force dnf remove fprintd - pam fprintd","title":"Disable fprintd"},{"location":"PAM_SSSD/Disable_fprintd/#disable-fprintd","text":"","title":"Disable fprintd"},{"location":"PAM_SSSD/Disable_fprintd/#issue","text":"Problem was found in CentOS, GDM was wainting for service to start before accepting password input. Log excerpt juin 23 19 : 09 : 48 . net dbus - daemon [ 1338 ] : [ system ] Activating via systemd : service name = 'net.reactivated.Fprint' unit = 'fprintd.service' requested by ':1.218' ( uid = 1573400 > juin 23 19 : 09 : 48 . net systemd [ 1 ] : Starting Fingerprint Authentication Daemon ... juin 23 19 : 09 : 48 . net dbus - daemon [ 1338 ] : [ system ] Successfully activated service 'net.reactivated.Fprint' juin 23 19 : 09 : 48 . net systemd [ 1 ] : Started Fingerprint Authentication Daemon . Service status [ root . net ~ ] systemctl status fprintd . service \u25cf fprintd . service - Fingerprint Authentication Daemon Loaded : loaded (/ usr / lib / systemd / system / fprintd . service ; static ; vendor preset : disabled ) Active : active ( running ) since Tue 2020-06-23 19 : 11 : 22 CEST ; 24s ago Docs : man : fprintd ( 1 ) Main PID : 5801 ( fprintd ) Tasks : 3 ( limit : 48352 ) Memory : 3 . 8M CGroup : / system . slice / fprintd . service \u2514\u2500 5801 / usr / libexec / fprintd juin 23 19 : 11 : 22 . net systemd [ 1 ] : Starting Fingerprint Authentication Daemon ... juin 23 19 : 11 : 22 . net systemd [ 1 ] : Started Fingerprint Authentication Daemon .","title":"Issue"},{"location":"PAM_SSSD/Disable_fprintd/#solution","text":"[ root . net ~ ] authconfig --disablefingerprint --update Running authconfig compatibility tool . The purpose of this tool is to enable authentication against chosen services with authselect and minimum configuration . It does not provide all capabilities of authconfig . IMPORTANT : authconfig is replaced by authselect , please update your scripts . See man authselect - migration ( 7 ) to help you with migration to authselect Executing : / usr / bin / authselect check Executing : / usr / bin / authselect select sssd --force dnf remove fprintd - pam fprintd","title":"Solution"},{"location":"PAM_SSSD/polkit/","text":"Autoriser Polkit Pam avec SSSD Ajouter l\u2019option suivante \u00e0 la section [pam] : pam_p11_allowed_services = +polkit-1 puis red\u00e9marrer sssd .","title":"Autoriser Polkit Pam avec SSSD"},{"location":"PAM_SSSD/polkit/#autoriser-polkit-pam-avec-sssd","text":"Ajouter l\u2019option suivante \u00e0 la section [pam] : pam_p11_allowed_services = +polkit-1 puis red\u00e9marrer sssd .","title":"Autoriser Polkit Pam avec SSSD"},{"location":"Podman/Pods/","text":"Create pods with Podman Nextcloud example First create the pod which is just the structure podman pod create --name nc-test -p 8080:80 then add containers podman run - d --restart=always --pod=nc-test -e MYSQL_ROOT_PASSWORD=\"\" -e MYSQL_DATABASE=\"nc\" -e MYSQL_USER=\"nc_user\" -e MYSQL_PASSWORD=\"nextcloud\" --name=nc-db mariadb podman run --security-opt label=disable -d --restart=always --pod=nc-test -e NEXTCLOUD_TRUSTED_DOMAINS=\"domain.net\" -e NEXTCLOUD_ADMIN_USER=\"admin\" -e NEXTCLOUD_ADMIN_PASSWORD=\"nextcloud\" -e MYSQL_DATABASE=\"nc\" -e MYSQL_USER=\"nc_user\" -e MYSQL_PASSWORD=\"nextcloud\" -e MYSQL_HOST=\"127.0.0.1\" -v ./ncdata:/var/www/html:z --name=nc-app --memory=128M nextcloud Nextcloud must use 127.0.0.1 as DB host as all ports are managed by the pod.","title":"Create pods with Podman"},{"location":"Podman/Pods/#create-pods-with-podman","text":"","title":"Create pods with Podman"},{"location":"Podman/Pods/#nextcloud-example","text":"First create the pod which is just the structure podman pod create --name nc-test -p 8080:80 then add containers podman run - d --restart=always --pod=nc-test -e MYSQL_ROOT_PASSWORD=\"\" -e MYSQL_DATABASE=\"nc\" -e MYSQL_USER=\"nc_user\" -e MYSQL_PASSWORD=\"nextcloud\" --name=nc-db mariadb podman run --security-opt label=disable -d --restart=always --pod=nc-test -e NEXTCLOUD_TRUSTED_DOMAINS=\"domain.net\" -e NEXTCLOUD_ADMIN_USER=\"admin\" -e NEXTCLOUD_ADMIN_PASSWORD=\"nextcloud\" -e MYSQL_DATABASE=\"nc\" -e MYSQL_USER=\"nc_user\" -e MYSQL_PASSWORD=\"nextcloud\" -e MYSQL_HOST=\"127.0.0.1\" -v ./ncdata:/var/www/html:z --name=nc-app --memory=128M nextcloud Nextcloud must use 127.0.0.1 as DB host as all ports are managed by the pod.","title":"Nextcloud example"},{"location":"Podman/Tips/","text":"Tips and tricks about Podman Disable security in SELinux context Add --security-opt label=disable and the :z option to your volume podman run -dit --security-opt label=disable -v ./cockpit:/tmp/cockpit:z localhost/cockpit-builder","title":"Tips and tricks about Podman"},{"location":"Podman/Tips/#tips-and-tricks-about-podman","text":"","title":"Tips and tricks about Podman"},{"location":"Podman/Tips/#disable-security-in-selinux-context","text":"Add --security-opt label=disable and the :z option to your volume podman run -dit --security-opt label=disable -v ./cockpit:/tmp/cockpit:z localhost/cockpit-builder","title":"Disable security in SELinux context"},{"location":"SQL/MySQL/","text":"MySQL and MariaDB tricks Dumper une table mysqldump $base $table Restaurer le dump d\u2019une table mysql -uroot -p DatabaseName < path\\TableName.sql Extraire table d\u2019un dump grep - n \"Table structure\" [ MySQL_dump_filename ] . sql This will provide you with the starting line number in the MySQL dump file which defines each table. Using this, determine the starting and ending line numbers of the table you need (the ending line number will be the starting line number of the next table, minus one). extract the table from the MySQL database dump file sed - n '[starting_line_number],[ending_line_number] p' [ MySQL_dump_filename ] . sql > [ table_output_filename ] . sql The last remaining step is to use the extracted table mysql - u root - p [ some_database_name ] < [ table_output_filename ] . sql G\u00e9rer les bases de donn\u00e9es show databases; Utiliser une base USE nom_base; Listes les tables dans cette base show tables; Pour cr\u00e9e une base de donn\u00e9es, saisir simplement CREATE DATABASE superbase; Pour la supprimer DROP DATABASE superbase; Gestion des utilisateurs Voir tous les utilisateurs select * from mysql.user; Pour cr\u00e9er un utilisateur Quelque soit l\u2019h\u00f4te CREATE USER 'utilisateur'@'%' IDENTIFIED BY 'motdepasse'; Que pour localhost CREATE USER 'utilisateur'@'localhost' IDENTIFIED BY 'motdepasse'; Attribuer des droits aux utilisateurs Pour attribuer tous les droits \u00e0 un utilisateur (en faire en quelque sortes un deuxi\u00e8me root) : GRANT ALL PRIVILEGES ON * . * TO 'utilisateur'@'localhost' IDENTIFIED BY 'motdepasse' WITH GRANT OPTION MAX_QUERIES_PER_HOUR 0 MAX_CONNECTIONS_PER_HOUR 0 MAX_UPDATES_PER_HOUR 0 MAX_USER_CONNECTIONS 0 ; Ou m\u00eame en lecture seule GRANT SELECT ON * . * TO 'utilisateur'@'localhost' IDENTIFIED BY 'motdepasse' ; Tous les droits sur une base GRANT ALL PRIVILEGES ON mydb.* TO 'myuser'@'localhost' WITH GRANT OPTION; On peut faire du 2 en 1. Voici un exemple pour la cr\u00e9ation d\u2019un utilisateur sans mot de passe avec des droits en lecture seule GRANT SELECT ON *.* TO 'ro'@'localhost'; De la m\u00eame fa\u00e7on, on peut supprimer ds droits avec REVOKE REVOKE ALL ON *.* FROM 'utilisateur'@'localhost'; Changer un mot de passe d\u2019utilisateur de MySQL Cette commande fonctionne uniquement pour MySQL UPDATE mysql.USER SET password=PASSWORD(\"nouveau\") WHERE USER=\"utilisateur\"; Pour voir les utilisateurs cr\u00e9\u00e9s SELECT USER,host,password FROM mysql.USER; Pour un utilisateur donn\u00e9, on peut voir ses droits de la fa\u00e7on suivante SHOW GRANTS FOR \"utilisateur\"@\"localhost\" ; View permissions for individual databases SELECT user, host, db, select_priv, insert_priv, grant_priv FROM mysql.db; Voir les GRANTS sur une table: select user from mysql.db where db='DB_NAME'; Modifier mot de passe hasher en MD5 UPDATE users SET Password = (MD5('1cb826899b7')) WHERE User = 'sgennet'; Tables Pour cr\u00e9er une table simple, voici un exemple CREATE TABLE table1 ( id INT(10) NOT NULL AUTO_INCREMENT COMMENT 'id, autoincr\u00e9ment\u00e9', nom VARCHAR(20) NOT NULL, DATE DATE, message VARCHAR(255), PRIMARY KEY (id)); How to Back Up and Restore a MySQL Database If you\u2019re storing anything in MySQL databases that you do not want to lose, it is very important to make regular backups of your data to protect it from loss. This tutorial will show you two easy ways to backup and restore the data in your MySQL database. You can also use this process to move your data to a new web server. Back up From the Command Line (using mysqldump) If you have shell or telnet access to your web server, you can backup your MySQL data by using the mysqldump command. This command connects to the MySQL server and creates an SQL dump file. The dump file contains the SQL statements necessary to re-create the database. Here is the proper syntax: mysqldump --opt -u [uname] -p[pass] [dbname] > [backupfile.sql] For example, to backup a database named \u2018Tutorials\u2019 with the username \u2018root\u2019 and with no password to a file tut_backup.sql, you should accomplish this command: mysqldump -u root -p Tutorials > tut_backup.sql This command will backup the \u2018Tutorials\u2019 database into a file called tut_backup.sql which will contain all the SQL statements needed to re-create the database. With mysqldump command you can specify certain tables of your database you want to backup. For example, to back up only php_tutorials and asp_tutorials tables from the \u2018Tutorials\u2019 database accomplish the command below. Each table name has to be separated by space. mysqldump -u root -p Tutorials php_tutorials asp_tutorials > tut_backup.sql Sometimes it is necessary to back up more that one database at once. In this case you can use the \u2013database option followed by the list of databases you would like to backup. Each database name has to be separated by space. mysqldump -u root -p --databases Tutorials Articles Comments > content_backup.sql If you want to back up all the databases in the server at one time you should use the \u2013all-databases option. It tells MySQL to dump all the databases it has in storage. mysqldump -u root -p --all-databases > alldb_backup.sql The mysqldump command has also some other useful options: --add-drop-table: Tells MySQL to add a DROP TABLE statement before each CREATE TABLE in the dump. --no-data: Dumps only the database structure, not the contents. --add-locks: Adds the LOCK TABLES and UNLOCK TABLES statements you can see in the dump file. The mysqldump command has advantages and disadvantages. The advantages of using mysqldump are that it is simple to use and it takes care of table locking issues for you. The disadvantage is that the command locks tables. If the size of your tables is very big mysqldump can lock out users for a long period of time. Back up your MySQL Database with Compress If your mysql database is very big, you might want to compress the output of mysqldump. Just use the mysql backup command below and pipe the output to gzip, then you will get the output as gzip file. mysqldump - u [ uname ] - p [ pass ] [ dbname ] | gzip - 9 > [ backupfile.sql.gz ] If you want to extract the .gz file, use the command below: gunzip [backupfile.sql.gz] Restoring your MySQL Database Above we backup the Tutorials database into tut_backup.sql file. To re-create the Tutorials database you should follow two steps: Create an appropriately named database on the target machine Load the file using the mysql command: mysql - u [ uname ] - p [ pass ] [ db_to_restore ] < [ backupfile.sql ] Have a look how you can restore your tut_backup.sql file to the Tutorials database. mysql -u root -p Tutorials < tut_backup.sql To restore compressed backup files you can do the following: gunzip < [ backupfile.sql.gz ] | mysql - u [ uname ] - p [ pass ] [ dbname ] If you need to restore a database that already exists, you\u2019ll need to use mysqlimport command. The syntax for mysqlimport is as follows: mysqlimport - u [ uname ] - p [ pass ] [ dbname ] [ backupfile . sql ] Backing Up and Restoring using PHPMyAdmin It is assumed that you have phpMyAdmin installed since a lot of web service providers use it. To backup your MySQL database using PHPMyAdmin just follow a couple of steps: Open phpMyAdmin. Select your database by clicking the database name in the list on the left of the screen. Click the Export link. This should bring up a new screen that says View dump of database (or something similar). In the Export area, click the Select All link to choose all of the tables in your database. In the SQL options area, click the right options. Click on the Save as file option and the corresponding compression option and then click the \u2018Go\u2019 button. A dialog box should appear prompting you to save the file locally. Restoring your database is easy as well as backing it up. Make the following: Open phpMyAdmin. Create an appropriately named database and select it by clicking the database name in the list on the left of the screen. If you would like to rewrite the backup over an existing database then click on the database name, select all the check boxes next to the table names and select Drop to delete all existing tables in the database. Click the SQL link. This should bring up a new screen where you can either type in SQL commands, or upload your SQL file. Use the browse button to find the database file. Click Go button. This will upload the backup, execute the SQL commands and re-create your database. Script cr\u00e9ation multiple 1 2 3 4 5 6 7 8 9 10 11 12 13 #!/bin/bash while read line do DB = $( echo $line | awk '{print $1}' ) ; echo $DB USER = $( echo $line | awk '{print $2}' ) ; echo $USER PASS = $( echo $line | awk '{print $3}' ) ; echo $PASS mysql -e \"CREATE DATABASE ${ DB } ;\" mysql -e \"CREATE USER ${ USER } @localhost IDENTIFIED BY ' ${ PASS } ';\" mysql -e \"GRANT ALL PRIVILEGES ON ${ DB } .* TO ' ${ USER } '@'localhost';\" mysql -e \"FLUSH PRIVILEGES;\" done <listing3 Mettre \u00e0 jour le pass d\u2019un user ALTER USER 'userName'@'localhost' IDENTIFIED BY 'New-Password-Here'; SET PASSWORD FOR 'user-name-here'@'hostname' = PASSWORD('new-password'); Purger les logs binaires PURGE BINARY LOGS TO 'mysql-bin.010'; PURGE BINARY LOGS BEFORE '2008-04-02 22:46:26'; G\u00e9rer les processus mysql> SHOW processlist; Tuer un process KILL QUERY id Tuer plein de process select concat('KILL ',id,';') from information_schema.processlist where user='fronte' and command='Query' into outfile '/tmp/a.txt'; source /tmp/a.txt; Afficher les entetes des colonnes d\u2019une table select column_name from information_schema.columns where table_name='<table_name>'; Voir les moteurs de table SELECT TABLE_NAME, ENGINE FROM information_schema.TABLES WHERE TABLE_SCHEMA = \\\"$DB\\\" ;","title":"MySQL"},{"location":"SQL/MySQL/#mysql-and-mariadb-tricks","text":"","title":"MySQL and MariaDB tricks"},{"location":"SQL/MySQL/#dumper-une-table","text":"mysqldump $base $table","title":"Dumper une table"},{"location":"SQL/MySQL/#restaurer-le-dump-dune-table","text":"mysql -uroot -p DatabaseName < path\\TableName.sql","title":"Restaurer le dump d'une table"},{"location":"SQL/MySQL/#extraire-table-dun-dump","text":"grep - n \"Table structure\" [ MySQL_dump_filename ] . sql This will provide you with the starting line number in the MySQL dump file which defines each table. Using this, determine the starting and ending line numbers of the table you need (the ending line number will be the starting line number of the next table, minus one).","title":"Extraire table d'un dump"},{"location":"SQL/MySQL/#extract-the-table-from-the-mysql-database-dump-file","text":"sed - n '[starting_line_number],[ending_line_number] p' [ MySQL_dump_filename ] . sql > [ table_output_filename ] . sql","title":"extract the table from the MySQL database dump file"},{"location":"SQL/MySQL/#the-last-remaining-step-is-to-use-the-extracted-table","text":"mysql - u root - p [ some_database_name ] < [ table_output_filename ] . sql","title":"The last remaining step is to use the extracted table"},{"location":"SQL/MySQL/#gerer-les-bases-de-donnees","text":"show databases; Utiliser une base USE nom_base; Listes les tables dans cette base show tables; Pour cr\u00e9e une base de donn\u00e9es, saisir simplement CREATE DATABASE superbase; Pour la supprimer DROP DATABASE superbase;","title":"G\u00e9rer les bases de donn\u00e9es"},{"location":"SQL/MySQL/#gestion-des-utilisateurs","text":"Voir tous les utilisateurs select * from mysql.user;","title":"Gestion des utilisateurs"},{"location":"SQL/MySQL/#pour-creer-un-utilisateur","text":"Quelque soit l\u2019h\u00f4te CREATE USER 'utilisateur'@'%' IDENTIFIED BY 'motdepasse'; Que pour localhost CREATE USER 'utilisateur'@'localhost' IDENTIFIED BY 'motdepasse';","title":"Pour cr\u00e9er un utilisateur"},{"location":"SQL/MySQL/#attribuer-des-droits-aux-utilisateurs","text":"Pour attribuer tous les droits \u00e0 un utilisateur (en faire en quelque sortes un deuxi\u00e8me root) : GRANT ALL PRIVILEGES ON * . * TO 'utilisateur'@'localhost' IDENTIFIED BY 'motdepasse' WITH GRANT OPTION MAX_QUERIES_PER_HOUR 0 MAX_CONNECTIONS_PER_HOUR 0 MAX_UPDATES_PER_HOUR 0 MAX_USER_CONNECTIONS 0 ; Ou m\u00eame en lecture seule GRANT SELECT ON * . * TO 'utilisateur'@'localhost' IDENTIFIED BY 'motdepasse' ; Tous les droits sur une base GRANT ALL PRIVILEGES ON mydb.* TO 'myuser'@'localhost' WITH GRANT OPTION; On peut faire du 2 en 1. Voici un exemple pour la cr\u00e9ation d\u2019un utilisateur sans mot de passe avec des droits en lecture seule GRANT SELECT ON *.* TO 'ro'@'localhost'; De la m\u00eame fa\u00e7on, on peut supprimer ds droits avec REVOKE REVOKE ALL ON *.* FROM 'utilisateur'@'localhost';","title":"Attribuer des droits aux utilisateurs"},{"location":"SQL/MySQL/#changer-un-mot-de-passe-dutilisateur-de-mysql","text":"Cette commande fonctionne uniquement pour MySQL UPDATE mysql.USER SET password=PASSWORD(\"nouveau\") WHERE USER=\"utilisateur\"; Pour voir les utilisateurs cr\u00e9\u00e9s SELECT USER,host,password FROM mysql.USER; Pour un utilisateur donn\u00e9, on peut voir ses droits de la fa\u00e7on suivante SHOW GRANTS FOR \"utilisateur\"@\"localhost\" ; View permissions for individual databases SELECT user, host, db, select_priv, insert_priv, grant_priv FROM mysql.db; Voir les GRANTS sur une table: select user from mysql.db where db='DB_NAME';","title":"Changer un mot de passe d'utilisateur de MySQL"},{"location":"SQL/MySQL/#modifier-mot-de-passe-hasher-en-md5","text":"UPDATE users SET Password = (MD5('1cb826899b7')) WHERE User = 'sgennet';","title":"Modifier mot de passe hasher en MD5"},{"location":"SQL/MySQL/#tables","text":"","title":"Tables"},{"location":"SQL/MySQL/#pour-creer-une-table-simple-voici-un-exemple","text":"CREATE TABLE table1 ( id INT(10) NOT NULL AUTO_INCREMENT COMMENT 'id, autoincr\u00e9ment\u00e9', nom VARCHAR(20) NOT NULL, DATE DATE, message VARCHAR(255), PRIMARY KEY (id));","title":"Pour cr\u00e9er une table simple, voici un exemple"},{"location":"SQL/MySQL/#how-to-back-up-and-restore-a-mysql-database","text":"If you\u2019re storing anything in MySQL databases that you do not want to lose, it is very important to make regular backups of your data to protect it from loss. This tutorial will show you two easy ways to backup and restore the data in your MySQL database. You can also use this process to move your data to a new web server. Back up From the Command Line (using mysqldump) If you have shell or telnet access to your web server, you can backup your MySQL data by using the mysqldump command. This command connects to the MySQL server and creates an SQL dump file. The dump file contains the SQL statements necessary to re-create the database. Here is the proper syntax: mysqldump --opt -u [uname] -p[pass] [dbname] > [backupfile.sql] For example, to backup a database named \u2018Tutorials\u2019 with the username \u2018root\u2019 and with no password to a file tut_backup.sql, you should accomplish this command: mysqldump -u root -p Tutorials > tut_backup.sql This command will backup the \u2018Tutorials\u2019 database into a file called tut_backup.sql which will contain all the SQL statements needed to re-create the database. With mysqldump command you can specify certain tables of your database you want to backup. For example, to back up only php_tutorials and asp_tutorials tables from the \u2018Tutorials\u2019 database accomplish the command below. Each table name has to be separated by space. mysqldump -u root -p Tutorials php_tutorials asp_tutorials > tut_backup.sql Sometimes it is necessary to back up more that one database at once. In this case you can use the \u2013database option followed by the list of databases you would like to backup. Each database name has to be separated by space. mysqldump -u root -p --databases Tutorials Articles Comments > content_backup.sql If you want to back up all the databases in the server at one time you should use the \u2013all-databases option. It tells MySQL to dump all the databases it has in storage. mysqldump -u root -p --all-databases > alldb_backup.sql The mysqldump command has also some other useful options: --add-drop-table: Tells MySQL to add a DROP TABLE statement before each CREATE TABLE in the dump. --no-data: Dumps only the database structure, not the contents. --add-locks: Adds the LOCK TABLES and UNLOCK TABLES statements you can see in the dump file. The mysqldump command has advantages and disadvantages. The advantages of using mysqldump are that it is simple to use and it takes care of table locking issues for you. The disadvantage is that the command locks tables. If the size of your tables is very big mysqldump can lock out users for a long period of time.","title":"How to Back Up and Restore a MySQL Database"},{"location":"SQL/MySQL/#back-up-your-mysql-database-with-compress","text":"If your mysql database is very big, you might want to compress the output of mysqldump. Just use the mysql backup command below and pipe the output to gzip, then you will get the output as gzip file. mysqldump - u [ uname ] - p [ pass ] [ dbname ] | gzip - 9 > [ backupfile.sql.gz ] If you want to extract the .gz file, use the command below: gunzip [backupfile.sql.gz]","title":"Back up your MySQL Database with Compress"},{"location":"SQL/MySQL/#restoring-your-mysql-database","text":"Above we backup the Tutorials database into tut_backup.sql file. To re-create the Tutorials database you should follow two steps: Create an appropriately named database on the target machine Load the file using the mysql command: mysql - u [ uname ] - p [ pass ] [ db_to_restore ] < [ backupfile.sql ] Have a look how you can restore your tut_backup.sql file to the Tutorials database. mysql -u root -p Tutorials < tut_backup.sql To restore compressed backup files you can do the following: gunzip < [ backupfile.sql.gz ] | mysql - u [ uname ] - p [ pass ] [ dbname ] If you need to restore a database that already exists, you\u2019ll need to use mysqlimport command. The syntax for mysqlimport is as follows: mysqlimport - u [ uname ] - p [ pass ] [ dbname ] [ backupfile . sql ]","title":"Restoring your MySQL Database"},{"location":"SQL/MySQL/#backing-up-and-restoring-using-phpmyadmin","text":"It is assumed that you have phpMyAdmin installed since a lot of web service providers use it. To backup your MySQL database using PHPMyAdmin just follow a couple of steps: Open phpMyAdmin. Select your database by clicking the database name in the list on the left of the screen. Click the Export link. This should bring up a new screen that says View dump of database (or something similar). In the Export area, click the Select All link to choose all of the tables in your database. In the SQL options area, click the right options. Click on the Save as file option and the corresponding compression option and then click the \u2018Go\u2019 button. A dialog box should appear prompting you to save the file locally.","title":"Backing Up and Restoring using PHPMyAdmin"},{"location":"SQL/MySQL/#restoring-your-database-is-easy-as-well-as-backing-it-up-make-the-following","text":"Open phpMyAdmin. Create an appropriately named database and select it by clicking the database name in the list on the left of the screen. If you would like to rewrite the backup over an existing database then click on the database name, select all the check boxes next to the table names and select Drop to delete all existing tables in the database. Click the SQL link. This should bring up a new screen where you can either type in SQL commands, or upload your SQL file. Use the browse button to find the database file. Click Go button. This will upload the backup, execute the SQL commands and re-create your database.","title":"Restoring your database is easy as well as backing it up. Make the following:"},{"location":"SQL/MySQL/#script-creation-multiple","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 #!/bin/bash while read line do DB = $( echo $line | awk '{print $1}' ) ; echo $DB USER = $( echo $line | awk '{print $2}' ) ; echo $USER PASS = $( echo $line | awk '{print $3}' ) ; echo $PASS mysql -e \"CREATE DATABASE ${ DB } ;\" mysql -e \"CREATE USER ${ USER } @localhost IDENTIFIED BY ' ${ PASS } ';\" mysql -e \"GRANT ALL PRIVILEGES ON ${ DB } .* TO ' ${ USER } '@'localhost';\" mysql -e \"FLUSH PRIVILEGES;\" done <listing3","title":"Script cr\u00e9ation multiple"},{"location":"SQL/MySQL/#mettre-a-jour-le-pass-dun-user","text":"ALTER USER 'userName'@'localhost' IDENTIFIED BY 'New-Password-Here'; SET PASSWORD FOR 'user-name-here'@'hostname' = PASSWORD('new-password');","title":"Mettre \u00e0 jour le pass d'un user"},{"location":"SQL/MySQL/#purger-les-logs-binaires","text":"PURGE BINARY LOGS TO 'mysql-bin.010'; PURGE BINARY LOGS BEFORE '2008-04-02 22:46:26';","title":"Purger les logs binaires"},{"location":"SQL/MySQL/#gerer-les-processus","text":"mysql> SHOW processlist;","title":"G\u00e9rer les processus"},{"location":"SQL/MySQL/#tuer-un-process","text":"KILL QUERY id","title":"Tuer un process"},{"location":"SQL/MySQL/#tuer-plein-de-process","text":"select concat('KILL ',id,';') from information_schema.processlist where user='fronte' and command='Query' into outfile '/tmp/a.txt'; source /tmp/a.txt;","title":"Tuer plein de process"},{"location":"SQL/MySQL/#afficher-les-entetes-des-colonnes-dune-table","text":"select column_name from information_schema.columns where table_name='<table_name>';","title":"Afficher les entetes des colonnes d'une table"},{"location":"SQL/MySQL/#voir-les-moteurs-de-table","text":"SELECT TABLE_NAME, ENGINE FROM information_schema.TABLES WHERE TABLE_SCHEMA = \\\"$DB\\\" ;","title":"Voir les moteurs de table"},{"location":"SQL/PostgreSQL/","text":"PostgreSQL G\u00e9rer PostGreSQL Se connecter en user postgres: sudo -u postgres -i Se connecter \u00e0 postgresql psql Lister les bases (une fois connect\u00e9) : \\l Lister tous les sch\u00e9mas : \\dn Lister les tables d\u2019un sch\u00e9ma\u202f: \\dt nom_schema.* Exporter une base (sans \u00eatre connect\u00e9) pg_dump NOM_BASE > Fichier.sql Importer des donn\u00e9es dans une base existante psql - U USERNAME NOM_BASE < Fichier . sql CREATE USER davide WITH PASSWORD 'pass' ; GRANT ALL PRIVILEGES ON DATABASE kinds TO manuel ; ALTER DATABASE kanboard OWNER TO u_base ; Mettre \u00e0 jour un mot de passe CREATE EXTENSION pgcrypto; update users set password = crypt('pass', gen_salt('bf')) where username = 'admin'; Passer des commandes en cli sans auth interactive PGPASSFILE=<(echo localhost:5432:db_siao_extraction:u_siao_extraction:MON_PASS) psql -h localhost -p 5432 -d db_siao_extraction -U u_siao_extraction -c \"delete from urg_extraction_info;\" Reque\u00eate les requ\u00eates en cours ordonn\u00e9es par \u00e2ge en excluant la requ\u00eate de requ\u00eate SELECT pid, age(query_start, clock_timestamp()), usename, query FROM pg_stat_activity WHERE query != '<IDLE>' AND query NOT ILIKE '%pg_stat_activity%' AND query NOT ILIKE '%SET extra%' ORDER BY query_start asc; Tuer une requ\u00eate en cours select pg_terminate_backend (N\u00b0 requ\u00eate);","title":"PostgreSQL"},{"location":"SQL/PostgreSQL/#postgresql","text":"","title":"PostgreSQL"},{"location":"SQL/PostgreSQL/#gerer-postgresql","text":"Se connecter en user postgres: sudo -u postgres -i Se connecter \u00e0 postgresql psql Lister les bases (une fois connect\u00e9) : \\l Lister tous les sch\u00e9mas : \\dn Lister les tables d\u2019un sch\u00e9ma\u202f: \\dt nom_schema.*","title":"G\u00e9rer PostGreSQL"},{"location":"SQL/PostgreSQL/#exporter-une-base-sans-etre-connecte","text":"pg_dump NOM_BASE > Fichier.sql","title":"Exporter une base (sans \u00eatre connect\u00e9)"},{"location":"SQL/PostgreSQL/#importer-des-donnees-dans-une-base-existante","text":"psql - U USERNAME NOM_BASE < Fichier . sql CREATE USER davide WITH PASSWORD 'pass' ; GRANT ALL PRIVILEGES ON DATABASE kinds TO manuel ; ALTER DATABASE kanboard OWNER TO u_base ;","title":"Importer des donn\u00e9es dans une base existante"},{"location":"SQL/PostgreSQL/#mettre-a-jour-un-mot-de-passe","text":"CREATE EXTENSION pgcrypto; update users set password = crypt('pass', gen_salt('bf')) where username = 'admin';","title":"Mettre \u00e0 jour un mot de passe"},{"location":"SQL/PostgreSQL/#passer-des-commandes-en-cli-sans-auth-interactive","text":"PGPASSFILE=<(echo localhost:5432:db_siao_extraction:u_siao_extraction:MON_PASS) psql -h localhost -p 5432 -d db_siao_extraction -U u_siao_extraction -c \"delete from urg_extraction_info;\"","title":"Passer des commandes en cli sans auth interactive"},{"location":"SQL/PostgreSQL/#requeete-les-requetes-en-cours-ordonnees-par-age-en-excluant-la-requete-de-requete","text":"SELECT pid, age(query_start, clock_timestamp()), usename, query FROM pg_stat_activity WHERE query != '<IDLE>' AND query NOT ILIKE '%pg_stat_activity%' AND query NOT ILIKE '%SET extra%' ORDER BY query_start asc;","title":"Reque\u00eate les requ\u00eates en cours ordonn\u00e9es par \u00e2ge en excluant la requ\u00eate de requ\u00eate"},{"location":"SQL/PostgreSQL/#tuer-une-requete-en-cours","text":"select pg_terminate_backend (N\u00b0 requ\u00eate);","title":"Tuer une requ\u00eate en cours"},{"location":"Systemd/Mount_Shares/","text":"Mount via Systemd Units Si vous connectez des partages r\u00e9seau NFS ou CIFS (Samba) via une connexion Wi-Fi g\u00e9r\u00e9e par Network-Manager, vous avez peut-\u00eatre constat\u00e9 qu\u2019\u00e0 l\u2019extinction de votre machine, le d\u00e9montage du partage ne se fait pas correctement et bloque l\u2019arr\u00eat durant un certain laps de temps (g\u00e9n\u00e9ralement 1min30). Ceci est d\u00fb au fait que l\u2019acc\u00e8s au r\u00e9seau Wi-Fi est stopp\u00e9 avant le d\u00e9montage propre du partage. C\u2019est un probl\u00e8me qui m\u2019a emb\u00eat\u00e9 de temps en temps \u00e0 la maison, voici une parade simple. Elle consiste \u00e0 cr\u00e9er un service systemd qui ne fait rien en d\u00e9marrant mais force le d\u00e9montage des partages lorsque vous fermez votre session (d\u00e9connexion ou arr\u00eat/red\u00e9marrage de la machine). Il faut cr\u00e9er le fichier /etc/systemd/system/umount-shares.service avec le contenu suivant : # / etc / systemd / system / umount_nfs . servic e [ Unit ] Description = Force umount NFS shares Before = network . target graphical . target [ Service ] Type = oneshot RemainAfterExit = yes ExecStart =/ bin / true ExecStop =/ usr / bin / umount - a - f - t nfs , nfs4 , cifs [ Install ] WantedBy = default . target Puis l ' activer et le d\u00e9marrer : systemctl enable umount - shares . service systemctl start umount - shares . service Ainsi plus d\u2019arr\u00eat qui attend dans le vide un stop job for xxx parce qu\u2019un partage r\u00e9seau ne peut se d\u00e9monter.","title":"Mount via Systemd Units"},{"location":"Systemd/Mount_Shares/#mount-via-systemd-units","text":"Si vous connectez des partages r\u00e9seau NFS ou CIFS (Samba) via une connexion Wi-Fi g\u00e9r\u00e9e par Network-Manager, vous avez peut-\u00eatre constat\u00e9 qu\u2019\u00e0 l\u2019extinction de votre machine, le d\u00e9montage du partage ne se fait pas correctement et bloque l\u2019arr\u00eat durant un certain laps de temps (g\u00e9n\u00e9ralement 1min30). Ceci est d\u00fb au fait que l\u2019acc\u00e8s au r\u00e9seau Wi-Fi est stopp\u00e9 avant le d\u00e9montage propre du partage. C\u2019est un probl\u00e8me qui m\u2019a emb\u00eat\u00e9 de temps en temps \u00e0 la maison, voici une parade simple. Elle consiste \u00e0 cr\u00e9er un service systemd qui ne fait rien en d\u00e9marrant mais force le d\u00e9montage des partages lorsque vous fermez votre session (d\u00e9connexion ou arr\u00eat/red\u00e9marrage de la machine). Il faut cr\u00e9er le fichier /etc/systemd/system/umount-shares.service avec le contenu suivant : # / etc / systemd / system / umount_nfs . servic e [ Unit ] Description = Force umount NFS shares Before = network . target graphical . target [ Service ] Type = oneshot RemainAfterExit = yes ExecStart =/ bin / true ExecStop =/ usr / bin / umount - a - f - t nfs , nfs4 , cifs [ Install ] WantedBy = default . target Puis l ' activer et le d\u00e9marrer : systemctl enable umount - shares . service systemctl start umount - shares . service Ainsi plus d\u2019arr\u00eat qui attend dans le vide un stop job for xxx parce qu\u2019un partage r\u00e9seau ne peut se d\u00e9monter.","title":"Mount via Systemd Units"},{"location":"Virt/Libvirt_cert/","text":"How to generate and use a cert for Libvirt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 #!/bin/bash SERVER_KEY = server-key.pem # creating a key for our ca if [ ! -e ca-key.pem ] ; then openssl genrsa -des3 -out ca-key.pem 1024 fi # creating a ca if [ ! -e ca-cert.pem ] ; then openssl req -new -x509 -days 1095 -key ca-key.pem -out ca-cert.pem -subj \"/C=IL/L=Raanana/O=Red Hat/CN=my CA\" fi # create server key if [ ! -e $SERVER_KEY ] ; then openssl genrsa -out $SERVER_KEY 1024 fi # create a certificate signing request (csr) if [ ! -e server-key.csr ] ; then openssl req -new -key $SERVER_KEY -out server-key.csr -subj \"/C=IL/L=Raanana/O=Red Hat/CN=my server\" fi # signing our server certificate with this ca if [ ! -e server-cert.pem ] ; then openssl x509 -req -days 1095 -in server-key.csr -CA ca-cert.pem -CAkey ca-key.pem -set_serial 01 -out server-cert.pem fi # now create a key that doesn't require a passphrase openssl rsa -in $SERVER_KEY -out $SERVER_KEY .insecure mv $SERVER_KEY $SERVER_KEY .secure mv $SERVER_KEY .insecure $SERVER_KEY # show the results (no other effect) openssl rsa -noout -text -in $SERVER_KEY openssl rsa -noout -text -in ca-key.pem openssl req -noout -text -in server-key.csr openssl x509 -noout -text -in server-cert.pem openssl x509 -noout -text -in ca-cert.pem # copy *.pem file to /etc/pki/libvirt-spice if [[ ! -d \"/etc/pki/libvirt-spice\" ]] then mkdir -p /etc/pki/libvirt-spice fi cp ./*.pem /etc/pki/libvirt-spice # echo --host-subject echo \"your --host-subject is\" \\\" ` openssl x509 -noout -text -in server-cert.pem | grep Subject: | cut -f 10 - -d \" \" ` \\\" echo \"copy ca-cert.pem to %APPDATA%\\spicec\\spice_truststore.pem or ~/.spice/spice_truststore.pem in your clients\"","title":"How to generate and use a cert for Libvirt"},{"location":"Virt/Libvirt_cert/#how-to-generate-and-use-a-cert-for-libvirt","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 #!/bin/bash SERVER_KEY = server-key.pem # creating a key for our ca if [ ! -e ca-key.pem ] ; then openssl genrsa -des3 -out ca-key.pem 1024 fi # creating a ca if [ ! -e ca-cert.pem ] ; then openssl req -new -x509 -days 1095 -key ca-key.pem -out ca-cert.pem -subj \"/C=IL/L=Raanana/O=Red Hat/CN=my CA\" fi # create server key if [ ! -e $SERVER_KEY ] ; then openssl genrsa -out $SERVER_KEY 1024 fi # create a certificate signing request (csr) if [ ! -e server-key.csr ] ; then openssl req -new -key $SERVER_KEY -out server-key.csr -subj \"/C=IL/L=Raanana/O=Red Hat/CN=my server\" fi # signing our server certificate with this ca if [ ! -e server-cert.pem ] ; then openssl x509 -req -days 1095 -in server-key.csr -CA ca-cert.pem -CAkey ca-key.pem -set_serial 01 -out server-cert.pem fi # now create a key that doesn't require a passphrase openssl rsa -in $SERVER_KEY -out $SERVER_KEY .insecure mv $SERVER_KEY $SERVER_KEY .secure mv $SERVER_KEY .insecure $SERVER_KEY # show the results (no other effect) openssl rsa -noout -text -in $SERVER_KEY openssl rsa -noout -text -in ca-key.pem openssl req -noout -text -in server-key.csr openssl x509 -noout -text -in server-cert.pem openssl x509 -noout -text -in ca-cert.pem # copy *.pem file to /etc/pki/libvirt-spice if [[ ! -d \"/etc/pki/libvirt-spice\" ]] then mkdir -p /etc/pki/libvirt-spice fi cp ./*.pem /etc/pki/libvirt-spice # echo --host-subject echo \"your --host-subject is\" \\\" ` openssl x509 -noout -text -in server-cert.pem | grep Subject: | cut -f 10 - -d \" \" ` \\\" echo \"copy ca-cert.pem to %APPDATA%\\spicec\\spice_truststore.pem or ~/.spice/spice_truststore.pem in your clients\"","title":"How to generate and use a cert for Libvirt"},{"location":"Virt/Qemu_mount_image/","text":"Mounting a QEMU Image In order to mount a QUMU / KVM disk image you need to use qemu-nbd , which lets you use the NBD protocol to share the disk image on the network. First you need the module loaded: sudo modprobe nbd max_part=8 Then you can share the disk on the network and create the device entries: sudo qemu-nbd --connect=/dev/nbd0 file.qcow2 Then you mount it: sudo mount /dev/nbd0p1 /mnt/kvm When done, unmount and unshare it: sudo umount /mnt/kvm sudo nbd-client -d /dev/nbd0","title":"Mounting a QEMU Image"},{"location":"Virt/Qemu_mount_image/#mounting-a-qemu-image","text":"In order to mount a QUMU / KVM disk image you need to use qemu-nbd , which lets you use the NBD protocol to share the disk image on the network. First you need the module loaded: sudo modprobe nbd max_part=8 Then you can share the disk on the network and create the device entries: sudo qemu-nbd --connect=/dev/nbd0 file.qcow2 Then you mount it: sudo mount /dev/nbd0p1 /mnt/kvm When done, unmount and unshare it: sudo umount /mnt/kvm sudo nbd-client -d /dev/nbd0","title":"Mounting a QEMU Image"},{"location":"Virt/Sparse_file_copy/","text":"Copier un fichier sparse sur le r\u00e9seau Les fichiers dits \u00ab sparse \u00bb sont allou\u00e9s avec une taille sup\u00e9rieur \u00e0 la taille r\u00e9ellement occup\u00e9e sur le disque dur. Cela permet de n\u2019occuper l\u2019espace disque que si le fichier fait face \u00e0 un accroissement. On les rencontre couramment en virtualisation o\u00f9 l\u2019on parle aussi de \u00ab thin provisioning \u00bb. Le terme \u00ab sparse \u00bb se traduit par \u00ab clairsem\u00e9 \u00bb en fran\u00e7ais et \u00ab thin provisionning \u00bb par \u00ab provisionnement all\u00e9g\u00e9 \u00bb. Avec rsync et l\u2019option -S ou \u2013sparse permet de respecter le caract\u00e8re \u00ab sparse \u00bb du fichier qui ne prendra pas plus de place disque sur la source que sur la cible. Cependant l\u2019utilisation de cette option a un inconv\u00e9nient : la taille d\u2019allocation totale transite par le r\u00e9seau, ce qui est peu efficient. Pour \u00e9viter ce d\u00e9sagr\u00e9ment on peut faire appel \u00e0 une archive tar en mode sparse (-S). Le fichier obtenu peut ainsi \u00eatre transf\u00e9r\u00e9 via n\u2019importe quel protocole pour \u00eatre d\u00e9-tar\u00e9 sur place. tar Scvf image . qcow2 . tar image . qcow2 rsync image . qcow2 . tar serveur - cible : / chemin / tar Scvf image . qcow2 . tar image . qcow2 rsync image . qcow2 . tar serveur - cible : / chemin / Une variante consiste \u00e0 utiliser tar en mode flux avec un pipe comme indiqu\u00e9 sur cette page \u00ab How to copy sparse files faster\u00ab . tar cvzSpf \u2013 image . qcow2 | ssh user @serveur - distant \u2018 ( cd / tmp ; tar xzSpf - ) \u2019 tar cvzSpf \u2013 image . qcow2 | ssh user @serveur - distant \u2018 ( cd / tmp ; tar xzSpf - ) \u2019 L\u2019utilisation de tar conjointement avec SSH est une bonne id\u00e9e afin de b\u00e9n\u00e9ficier de l\u2019option sparse, mais aussi pour remplir les trames r\u00e9seau et acc\u00e9l\u00e9rer les \u00e9changes par rapport \u00e0 rsync, en particulier en cas de petits fichiers. Voir diff\u00e9rent exemples ici ou encore celui qui suit. tar -cS /dossier | ssh serveur-distant 'tar -xvf - -C /destination/' tar -cS /dossier | ssh serveur-distant 'tar -xvf - -C /destination/'","title":"Copier un fichier sparse sur le r\u00e9seau"},{"location":"Virt/Sparse_file_copy/#copier-un-fichier-sparse-sur-le-reseau","text":"Les fichiers dits \u00ab sparse \u00bb sont allou\u00e9s avec une taille sup\u00e9rieur \u00e0 la taille r\u00e9ellement occup\u00e9e sur le disque dur. Cela permet de n\u2019occuper l\u2019espace disque que si le fichier fait face \u00e0 un accroissement. On les rencontre couramment en virtualisation o\u00f9 l\u2019on parle aussi de \u00ab thin provisioning \u00bb. Le terme \u00ab sparse \u00bb se traduit par \u00ab clairsem\u00e9 \u00bb en fran\u00e7ais et \u00ab thin provisionning \u00bb par \u00ab provisionnement all\u00e9g\u00e9 \u00bb. Avec rsync et l\u2019option -S ou \u2013sparse permet de respecter le caract\u00e8re \u00ab sparse \u00bb du fichier qui ne prendra pas plus de place disque sur la source que sur la cible. Cependant l\u2019utilisation de cette option a un inconv\u00e9nient : la taille d\u2019allocation totale transite par le r\u00e9seau, ce qui est peu efficient. Pour \u00e9viter ce d\u00e9sagr\u00e9ment on peut faire appel \u00e0 une archive tar en mode sparse (-S). Le fichier obtenu peut ainsi \u00eatre transf\u00e9r\u00e9 via n\u2019importe quel protocole pour \u00eatre d\u00e9-tar\u00e9 sur place. tar Scvf image . qcow2 . tar image . qcow2 rsync image . qcow2 . tar serveur - cible : / chemin / tar Scvf image . qcow2 . tar image . qcow2 rsync image . qcow2 . tar serveur - cible : / chemin / Une variante consiste \u00e0 utiliser tar en mode flux avec un pipe comme indiqu\u00e9 sur cette page \u00ab How to copy sparse files faster\u00ab . tar cvzSpf \u2013 image . qcow2 | ssh user @serveur - distant \u2018 ( cd / tmp ; tar xzSpf - ) \u2019 tar cvzSpf \u2013 image . qcow2 | ssh user @serveur - distant \u2018 ( cd / tmp ; tar xzSpf - ) \u2019 L\u2019utilisation de tar conjointement avec SSH est une bonne id\u00e9e afin de b\u00e9n\u00e9ficier de l\u2019option sparse, mais aussi pour remplir les trames r\u00e9seau et acc\u00e9l\u00e9rer les \u00e9changes par rapport \u00e0 rsync, en particulier en cas de petits fichiers. Voir diff\u00e9rent exemples ici ou encore celui qui suit. tar -cS /dossier | ssh serveur-distant 'tar -xvf - -C /destination/' tar -cS /dossier | ssh serveur-distant 'tar -xvf - -C /destination/'","title":"Copier un fichier sparse sur le r\u00e9seau"},{"location":"Virt/Virt-Manager/","text":"Virt-Manager tricks USB Redirection ACL Error : chmod u+s /usr/bin/spice-client-glib-usb-acl-helper No root password asked : /usr/share/polkit-1/actions/org.spice-spice.lowlevelusbaccess.policy. Before changes I had follow <allow_any> auth_admin </allow_any> <allow_inactive> no </allow_inactive> <allow_active> auth_admin </allow_active> After I have <allow_any> yes </allow_any> <allow_inactive> no </allow_inactive> <allow_active> yes </allow_active>","title":"Virt Manager"},{"location":"Virt/Virt-Manager/#virt-manager-tricks","text":"","title":"Virt-Manager tricks"},{"location":"Virt/Virt-Manager/#usb-redirection","text":"ACL Error : chmod u+s /usr/bin/spice-client-glib-usb-acl-helper No root password asked : /usr/share/polkit-1/actions/org.spice-spice.lowlevelusbaccess.policy. Before changes I had follow <allow_any> auth_admin </allow_any> <allow_inactive> no </allow_inactive> <allow_active> auth_admin </allow_active> After I have <allow_any> yes </allow_any> <allow_inactive> no </allow_inactive> <allow_active> yes </allow_active>","title":"USB Redirection"},{"location":"Web/Nginx/","text":"List served sites grep server_name /etc/nginx/sites-enabled/* -RiI | column -t","title":"Nginx"},{"location":"Web/Nginx/#list-served-sites","text":"grep server_name /etc/nginx/sites-enabled/* -RiI | column -t","title":"List served sites"},{"location":"Web/Apache/Mapping/","text":"Create an redirection mapping httxt2dbm -i url-communication.txt -o url-communication.map","title":"Create an redirection mapping"},{"location":"Web/Apache/Mapping/#create-an-redirection-mapping","text":"httxt2dbm -i url-communication.txt -o url-communication.map","title":"Create an redirection mapping"},{"location":"Web/Apache/Redirection/","text":"Redirections Apache HTTP => HTTPS RewriteEngine On RewriteCond %{HTTPS} !=on RewriteRule ^/?(.*) https://%{SERVER_NAME}/$1 [R,L] Pour un domaine sp\u00e9cifique <If \"%{HTTP_HOST} = 'www.example.com'\" > Redirect \"/\" \"http://www.new-example.com/\" </If>","title":"Redirections Apache"},{"location":"Web/Apache/Redirection/#redirections-apache","text":"","title":"Redirections Apache"},{"location":"Web/Apache/Redirection/#http-https","text":"RewriteEngine On RewriteCond %{HTTPS} !=on RewriteRule ^/?(.*) https://%{SERVER_NAME}/$1 [R,L]","title":"HTTP =&gt; HTTPS"},{"location":"Web/Apache/Redirection/#pour-un-domaine-specifique","text":"<If \"%{HTTP_HOST} = 'www.example.com'\" > Redirect \"/\" \"http://www.new-example.com/\" </If>","title":"Pour un domaine sp\u00e9cifique"},{"location":"Web/HAProxy/Keepalived_VRRP_Config/","text":"Config for VRPP between 2 HAProxies with keepalived Assumptions This works on Ubuntu 14.04 haproxy-primary IP: 198.51.100.10 haproxy-secondary IP: 198.51.100.20 shared IP: 198.51.100.50 Any DNS rules should point to the shared IP (198.51.100.50) Steps Add a firewall rule for keepalived # 224.0.0.18 is the keepalived multicast address sudo ufw allow in from 198.51.100.20 to 224.0.0.18 # on 198.51.100.10 sudo ufw allow in from 198.51.100.10 to 224.0.0.18 # on 198.51.100.20 Allow access to a shared IP address edit /etc/sysctl.conf set net.ipv4.ip_nonlocal_bind=1 sudo sysctl -p # reload config change Install keepalived sudo apt-get install keepalived Configure keepalived on both servers Edit/create /etc/keepalived/keepalived.conf See example file below # the priority MUST be different on the primary and secondary servers! Restart keepalived sudo service keepalived restart Listen on the shared IP address Edit /etc/haproxy/haproxy.cfg bind 198.51.100.50:80 Restart haproxy (on both haproxy servers) sudo service haproxy restart Verify proper failover primary : sudo ip addr show | grep eth0 # should list the shared IP secondary : sudo ip addr show | grep eth0 # should NOT list the shared IP primary : sudo service haproxy stop primary : sudo ip addr show | grep eth0 # should NOT list the shared IP secondary : sudo ip addr show | grep eth0 # should list the shared IP primary : sudo service haproxy start primary : sudo ip addr show | grep eth0 # should list the shared IP secondary : sudo ip addr show | grep eth0 # should NOT list the shared IP / etc / keepalived / keepalived . conf vrrp_script chk_haproxy { # Requires keepalived - 1 . 1 . 13 script \"killall -0 haproxy\" # cheaper than pidof interval 2 # check every 2 seconds weight 2 # add 2 points of priority if OK } vrrp_instance VI_1 { interface eth0 state MASTER virtual_router_id 51 priority 101 # 101 on primary , 100 on secondary virtual_ipaddress { 198 . 51 . 100 . 50 } track_script { chk_haproxy } }","title":"Keepalived VRRP Config"},{"location":"Web/HAProxy/Keepalived_VRRP_Config/#config-for-vrpp-between-2-haproxies-with-keepalived","text":"","title":"Config for VRPP between 2 HAProxies with keepalived"},{"location":"Web/HAProxy/Keepalived_VRRP_Config/#assumptions","text":"This works on Ubuntu 14.04 haproxy-primary IP: 198.51.100.10 haproxy-secondary IP: 198.51.100.20 shared IP: 198.51.100.50 Any DNS rules should point to the shared IP (198.51.100.50)","title":"Assumptions"},{"location":"Web/HAProxy/Keepalived_VRRP_Config/#steps","text":"Add a firewall rule for keepalived # 224.0.0.18 is the keepalived multicast address sudo ufw allow in from 198.51.100.20 to 224.0.0.18 # on 198.51.100.10 sudo ufw allow in from 198.51.100.10 to 224.0.0.18 # on 198.51.100.20 Allow access to a shared IP address edit /etc/sysctl.conf set net.ipv4.ip_nonlocal_bind=1 sudo sysctl -p # reload config change Install keepalived sudo apt-get install keepalived Configure keepalived on both servers Edit/create /etc/keepalived/keepalived.conf See example file below # the priority MUST be different on the primary and secondary servers! Restart keepalived sudo service keepalived restart Listen on the shared IP address Edit /etc/haproxy/haproxy.cfg bind 198.51.100.50:80 Restart haproxy (on both haproxy servers) sudo service haproxy restart Verify proper failover primary : sudo ip addr show | grep eth0 # should list the shared IP secondary : sudo ip addr show | grep eth0 # should NOT list the shared IP primary : sudo service haproxy stop primary : sudo ip addr show | grep eth0 # should NOT list the shared IP secondary : sudo ip addr show | grep eth0 # should list the shared IP primary : sudo service haproxy start primary : sudo ip addr show | grep eth0 # should list the shared IP secondary : sudo ip addr show | grep eth0 # should NOT list the shared IP / etc / keepalived / keepalived . conf vrrp_script chk_haproxy { # Requires keepalived - 1 . 1 . 13 script \"killall -0 haproxy\" # cheaper than pidof interval 2 # check every 2 seconds weight 2 # add 2 points of priority if OK } vrrp_instance VI_1 { interface eth0 state MASTER virtual_router_id 51 priority 101 # 101 on primary , 100 on secondary virtual_ipaddress { 198 . 51 . 100 . 50 } track_script { chk_haproxy } }","title":"Steps"},{"location":"Web/HAProxy/LDAP_configuration/","text":"global log / dev / log local6 pidfile / var / run / haproxy . pid chroot / var / lib / haproxy maxconn 8192 user haproxy group haproxy daemon stats socket / var / lib / haproxy / stats . socket mode 660 level admin # Default SSL material locations ca - base / etc / ssl / certs crt - base / etc / ssl / private # Default ciphers to use on SSL - enabled listening sockets . # For more information , see ciphers ( 1 SSL ). This list is from : # https : // hynek . me / articles / hardening - your - web - servers - ssl - ciphers / ## ##ssl - default - bind - ciphers ECDH + AESGCM : DH + AESGCM : ECDH + AES256 : DH + AES256 : ECDH + AES128 : DH + AES : ECDH + 3 DES : DH + 3 DES : RSA + AESGCM : RSA + AES : RSA + 3 DES : ! aNULL : ! MD5 : ! DSS ## ##ssl - default - bind - options no - sslv3 ## ##tune . ssl . default - dh - param 2048 # LDAP and LDAP / STARTTLS frontend ldap_service_front mode tcp log global bind ldap . company . com : 389 description LDAP Service option tcplog option logasap option socket - stats option tcpka timeout client 5 s default_backend ldap_service_back backend ldap_service_back server ldap - 1 ad - dc01 . company . com : 389 check fall 1 rise 1 inter 2 s server ldap - 2 ad - dc02 . company . com : 389 check fall 1 rise 1 inter 2 s server ldap - 3 ad - dc03 . company . com : 389 check fall 1 rise 1 inter 2 s mode tcp balance leastconn timeout server 2 s timeout connect 1 s option tcpka # https : // www . mail - archive . com / haproxy @formilux . org / msg17371 . html option tcp - check tcp - check connect port 389 tcp - check send - binary 300 c0201 # LDAP bind request \"<ROOT>\" simple tcp - check send - binary 01 # message ID tcp - check send - binary 6007 # protocol Op tcp - check send - binary 0201 # bind request tcp - check send - binary 03 # LDAP v3 tcp - check send - binary 04008000 # name , simple authentication tcp - check expect binary 0 a0100 # bind response + result code : success tcp - check send - binary 30050201034200 # unbind request # LDAPS frontend ldapS_service_front mode tcp log global bind ldap . company . com : 636 ssl crt / etc / ssl / private / ldap_company_com . PEM description LDAPS Service option tcplog option logasap option socket - stats option tcpka timeout client 5 s default_backend ldaps_service_back backend ldaps_service_back server ldapS - 1 ad - dc01 . company . com : 636 check fall 1 rise 1 inter 2 s verify none check check - ssl server ldapS - 2 ad - dc02 . company . com : 636 check fall 1 rise 1 inter 2 s verify none check check - ssl server ldapS - 3 ad - dc03 . company . com : 636 check fall 1 rise 1 inter 2 s verify none check check - ssl mode tcp balance leastconn timeout server 2 s timeout connect 1 s option tcpka # option tcp - check tcp - check connect port 636 ssl tcp - check send - binary 300 c0201 # LDAP bind request \"<ROOT>\" simple tcp - check send - binary 01 # message ID tcp - check send - binary 6007 # protocol Op tcp - check send - binary 0201 # bind request tcp - check send - binary 03 # LDAP v3 tcp - check send - binary 04008000 # name , simple authentication tcp - check expect binary 0 a0100 # bind response + result code : success tcp - check send - binary 30050201034200 # unbind request","title":"LDAP configuration"}]}